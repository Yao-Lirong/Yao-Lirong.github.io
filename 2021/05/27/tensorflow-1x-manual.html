<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>海尔实习期间记录下的 TensorFlow 笔记</p> <p><span id="more"></span>&lt;h2 id="Basic-Notion"&gt;<a href="#Basic-Notion" class="headerlink" title="Basic Notion"></a>Basic Notion&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Graph: often refers to Computation Graph, which describes how to compute the output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Eager execution: evaluates operations immediately, without building graphs&lt;/p&gt;&lt;p&gt;Enabling eager execution changes how TensorFlow operations behave—now they immediately evaluate and return their values to Python. <code>tf.Tensor</code>objects reference concrete values instead of symbolic handles to nodes in a computational graph. Since there isn’t a computational graph to build and run later in a session, it’s easy to inspect results using <code>print()</code> or a debugger. Evaluating, printing, and checking tensor values does not break the flow for computing gradients.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Operation: 图中的节点, takes <code>Tensor</code> object as input, and produces <code>Tensor</code> objects as output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Tensor: multi-dimensional arrays with a uniform type (called <code>dtype</code>), 包含一个 n 维的数组或列表. 一个静态类型 rank, 和 一个 shape. &lt;/p&gt;&lt;p&gt;It does not hold the values of that operation’s output, but instead provides a means of computing those values. It is a symbolic handle of input/output of <code>Operation</code>. &lt;/p&gt;&lt;p&gt;图上操作间传递的数据都是 <code>Tensor</code>: A <code>Tensor</code> can be passed as an input to another <code>Operation</code>. This builds a dataflow connection between operations, which enables TensorFlow to execute an entire <code>Graph</code> that represents a large, multi-step computation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Session: launch the computation of a graph &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;InteractiveSession: a better graph runner that allows you to compute each operation step by step instead of only giving out the final result, as in <code>Session </code>&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># Build a dataflow graph.</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>]])</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a <code class="language-plaintext highlighter-rouge">Session</code> to execute the graph.</span></span><br><span class="line">sess = tf.compat.v1.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute the graph and store the value that <code class="language-plaintext highlighter-rouge">e</code> represents in <code class="language-plaintext highlighter-rouge">result</code>.</span></span><br><span class="line">result = sess.run(e)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;<code>a, b, c</code> are <code>Tensor</code> here. &lt;/p&gt;&lt;p&gt;<code>c = tf.matmul(a, b)</code> creates an <code>Operation</code> of type “MatMul” (Matrix Multiplication) that takes tensors <code>a</code> and <code>b</code> as input, and produces <code>c</code> as output. &lt;/p&gt;&lt;hr&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Variable: represent shared, persistent state your program manipulates (parameters of the model)&lt;/p&gt;&lt;p&gt;it is a <code>tf.Tensor</code> whose value can be changed by running ops on it&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Placeholder: a tensor whose value will later be fed.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="Operations-on-Tensors"&gt;<a href="#Operations-on-Tensors" class="headerlink" title="Operations on Tensors"></a>Operations on Tensors&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<code>tf.reduce_xxx(t, axis=i)</code>: If we have a tensor <code>t</code> of dimension $d_1 \times d_2 \times … \times d_n $, apply <code>r = reduce_xxx(t, axis = i)</code>, Each entry along axis <code>i</code> will be collapsed into a single entry, so r will have dimension $d_1 \times d_2 \times … d_{i-1} \times d_{i+1} … \times d_n $: &lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">a=np.random.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">2 arrays of dimension 3 X 4</span></span><br><span class="line"><span class="string">[[[8 5 7 1]</span></span><br><span class="line"><span class="string"> [9 7 2 2]</span></span><br><span class="line"><span class="string"> [7 7 4 6]]</span></span><br><span class="line"><span class="string"> [[7 7 8 4]</span></span><br><span class="line"><span class="string"> [7 4 3 6]</span></span><br><span class="line"><span class="string"> [5 3 2 8]]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default(): </span><br><span class="line"> r = (tf.reduce_sum(a, axis=<span class="number">1</span>)).<span class="built_in">eval</span>() <span class="comment"># reduce along axis of length 3</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[24 19 13 9]</span></span><br><span class="line"><span class="string"> [19 14 13 18]]</span></span><br><span class="line"><span class="string">'''</span> </span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>tf.reshape(t, list)</code>: Reorder all the elements in <code>t</code> so that we have a new dimension in r: $d_1’ = list[0], d_2’ = list[1], …$ If we have $d’_i = -1$ as one of the dimension, $d_i’ = \frac{d_1 \times d_2 \times … \times d_n}{list[0] \times list[1]\times…list[i-1]\times list[i+1] … \times list[n-1]} $, so &lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">r = tf.reshape(a, [-<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]).<span class="built_in">eval</span>() <span class="comment"># r will havee shape (6, 2, 2)</span></span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>tf.concat([t1, t2, ...], axis = i)</code>: pile all the arrays along axis <code>i</code>. These arrays must have the same length along the other axis. In the result, only the length along axis <code>i</code> will increase, the length of other axis remain the same. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>tf.tile(t, [m1,m2,...])</code>: multiple axis <code>i</code> with <code>mi</code>, so the result tensor dimension is $(d_1\times m_1, d_2\times m_2, …)$&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="Debug-with-Tensorboard"&gt;<a href="#Debug-with-Tensorboard" class="headerlink" title="Debug with Tensorboard"></a>Debug with Tensorboard&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;<code>tf.summary</code>: Follow <a href="https://github.com/tensorflow/tensorboard/blob/master/docs/r1/overview.md#summary-ops-how-tensorboard-gets-data-from-tensorflow" rel="external nofollow noopener" target="_blank">the official guide</a>&lt;/li&gt;&lt;li&gt;<code>tf.estimator</code>: Specify <code>model_dir</code> when initializing your <code>estimator</code>. Everything about the trained model will be stored in this directory, including <code>event</code> files logging training process. <a href="https://stackoverflow.com/a/53733417/12006199" rel="external nofollow noopener" target="_blank">Reference</a>&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="Utility"&gt;<a href="#Utility" class="headerlink" title="Utility"></a>Utility&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Sometimes we encounter <code>module 'tensorflow' has no attribute ...</code> because TensorFlow changed/refactored its function name. We can use <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade.py" rel="external nofollow noopener" target="_blank">this list</a> to manually update all changed names or directly use <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility" rel="external nofollow noopener" target="_blank">this script</a>. &lt;/li&gt;&lt;/ul&gt;</p> </body></html>