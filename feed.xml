<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yao-lirong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yao-lirong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-03T18:11:51+00:00</updated><id>https://yao-lirong.github.io/feed.xml</id><title type="html">Yao Lirong</title><subtitle>Yao Lirong&apos;s personal homepage </subtitle><entry><title type="html">Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search</title><link href="https://yao-lirong.github.io/2024/12/25/matryoshka-representation-learning-adaptive-retrieval-and-binary-vector-search.html" rel="alternate" type="text/html" title="Matryoshka Representation Learning, Adaptive Retrieval and Binary Vector Search"/><published>2024-12-25T05:00:00+00:00</published><updated>2024-12-25T05:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/12/25/matryoshka-representation-learning-adaptive-retrieval-and-binary-vector-search</id><content type="html" xml:base="https://yao-lirong.github.io/2024/12/25/matryoshka-representation-learning-adaptive-retrieval-and-binary-vector-search.html"><![CDATA[<p><strong>Intro to Matryoshka Representation Learning</strong></p> <p>In Matryoshka Representation Learning (MRL), we want to construct anencoding &lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em></sub>&lt;/span&gt; withdimension <span class="math inline"><em>d</em></span> such that itstruncations of different lengths (&lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em>/16</sub>, <em>e</em><sub><em>d</em>/8</sub>, <em>e</em><sub><em>d</em>/4</sub>, <em>e</em><sub><em>d</em>/2</sub>&lt;/span&gt;​)are each (somewhat) valid representations. Suppose you’re training on aclassification problem with the classic encoder + classifier headarchitecture. At train time:</p> <ul><li>classic setting: you just use the vector &lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em></sub>&lt;/span&gt; as input tothe classifier head</li><li>MRL: construct multiple classifier heads (in our case 5) and put oneon top of encoding of each length (&lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em>/16</sub>, …, <em>e</em><sub><em>d</em></sub>&lt;/span&gt;)and average the loss of each classifier head. So we build heads of size<code>[d, num_class], [d/2, num_class], ... [d/16, num_class]</code>Note these classifier heads share weights.</li></ul> <p><strong>Application: Adaptive Retrieval</strong></p> <p>Online retrieval is one of the tasks where latency matters the most.Given a user query <span class="math inline"><em>q</em></span>, it isslow to compute KNN from a dataset of size 1M (&lt;spanclass="math inline"&gt;10<sup>6</sup>&lt;/span&gt;) indexes if each index hasdimension 3072. With MRL, we can decompose the process into twostages:</p> <ol type="1"><li>Shortlist: First retrieve 2K indexes where the distance is computedusing only 1024-d vector (the first 1024 elements of the 3072vector)</li><li>Rerank: Find KNN among these 2K indexes where the distance iscomputed using the full length 3072 vector</li></ol> <p>The FLOP is therefore reduced from &lt;spanclass="math inline"&gt;3072 × 10<sup>6</sup>&lt;/span&gt; to &lt;spanclass="math inline"&gt;1024 × 10<sup>6</sup> + 3072 × 2<em>K</em>&lt;/span&gt;.Ce Gao tested full length 3072-dim vector vs adaptive retrieval usingMatryoshka 1024-dim. The accuracy dropped from 99% to 89% with RequestsPer Second (RPS) raises from 300 to 1000.</p> <p>Find more details of Matryoshka Representation Learning and itsapplications in this wonderful &lt;ahref="https://aniketrege.github.io/blog/2024/mrl/#what-is-mrl-really-this-time"&gt;blogpost. Read from section <em>What is MRL? (Really this Time)</em>&lt;/a&gt;</p> <p><strong>Binary Vector Search</strong></p> <p>&lt;ahref="https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors"&gt;CeGao suggested&lt;/a&gt; another way to reduce memory and FLOP use. He proposesto turn the length <span class="math inline"><em>d</em></span> FP32vector into a length <span class="math inline"><em>d</em></span> binaryvector, where original positive value is set to 1 and original negativevalue is set to 0.</p> <p>Without using adaptive retrieval, the accuracy dropped from 99% to83%, but the latency (RPS = 3000) and memory has a significantimprovement because previously one single vector / encoding consists of<span class="math inline"><em>d</em></span> 32-bit number, whereas nowit only consists of <span class="math inline"><em>d</em></span> 1-bitnumber.</p> <p>If you adapt the Adaptive Retrieval setup mentioned earlier:</p> <ol type="1"><li>Shortlist: retrieve 2K indexes using full-length but binaryvector</li><li>Rerank: find KNN among 2K indexes using full-length, FP32vector</li></ol> <p>you get a precision drop from 99% only to 96% with an RPS of1700.</p> <p>P.S. I discovered this method on &lt;ahref="https://simonwillison.net/2024/Mar/26/binary-vector-search/"&gt;SimonWillison’s blog&lt;/a&gt;.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Intro to Matryoshka Representation Learning In Matryoshka Representation Learning (MRL), we want to construct an]]></summary></entry><entry><title type="html">YouTube Recommendation Algorithms (2016)</title><link href="https://yao-lirong.github.io/2024/10/15/youtube-recommendation-algorithms-2016.html" rel="alternate" type="text/html" title="YouTube Recommendation Algorithms (2016)"/><published>2024-10-15T04:00:00+00:00</published><updated>2024-10-15T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/10/15/youtube-recommendation-algorithms-2016</id><content type="html" xml:base="https://yao-lirong.github.io/2024/10/15/youtube-recommendation-algorithms-2016.html"><![CDATA[<p>This is a detailed reading of Google’s paper &lt;ahref="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf"&gt;DeepNeural Networks for YouTube Recommendations&lt;/a&gt;</p> <p><span id="more"></span>&lt;h2 id="candidate-generation"&gt;Candidate Generation&lt;/h2&gt;&lt;h3 id="problem-setup"&gt;Problem Setup&lt;/h3&gt;&lt;p&gt;We pose recommendation as an extreme multi-class classificationproblem where we <strong>predict which video will be watchednext</strong>. Specifically, we classify a specific video watch &lt;spanclass=”math inline”&gt;<em>w</em><sub><em>t</em></sub>&lt;/span&gt; at time &lt;spanclass=”math inline”&gt;<em>t</em>&lt;/span&gt; among millions of videos &lt;spanclass=”math inline”&gt;<em>i</em>&lt;/span&gt; (classes) from a video corpus<span class="math inline"><em>V</em></span> based on a user &lt;spanclass=”math inline”&gt;<em>U</em>&lt;/span&gt; and context &lt;spanclass=”math inline”&gt;<em>C</em>&lt;/span&gt;. And <span class="math inline">$u\in \R^d$</span> represents a high-dimensional “embedding” of theuser-context pair and the <span class="math inline">$v_j \in\R^d$</span> represent embeddings of each candidate video. &lt;spanclass=”math display”&gt;<em>P</em>(<em>w</em><sub><em>t</em></sub> = <em>i</em> ∣ <em>U</em>, <em>C</em>) = <code>Softmax</code>(<em>V</em>, <em>u</em>)<sub><em>i</em></sub>&lt;/span&gt;&lt;/p&gt;&lt;h3 id="input"&gt;Input&lt;/h3&gt;&lt;p&gt;what we didn’t use: explicit feedback (thumbs up/down, in-productsurveys) because there’s too few of them in the tail of videos.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<strong>embedded video watches</strong>:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<strong>representation of each video</strong>: in myunderstanding, YouTube didn’t extract information from their videos, andfed the extracted info into the embedder. Instead, they directly fed thevideo ids into the embedder. To my understanding, “Inspired bycontinuous bag of words language models” means they fed the video idsinto the embedder, just like NLP feeds BoW representation into embedder.It doesn’t mean YouTube decomposes a video into word count like BoW.&lt;/p&gt;&lt;p&gt;I reached this conclusion from the last sentence of 4.1 FeatureRepresentation - Embedding Categorical Features, where<code>1000000*32 / (2048*2048) = 7</code>&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The overwhelming majority of model parameters are in thesehigh-cardinality embedding spaces - for example, one million IDsembedded in a 32 dimensional space have 7 times more parameters thanfully connected layers 2048 units wide.&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>representation of watch history</strong>: watch historyis a variable-length list of videos. YouTube used average-pooling totransform them into a fixed-length vector.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>why order-independent pooling?</strong>&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>embedded search tokens</strong>: each query is tokenizedinto unigrams and bigrams. each token is embedded. All these embeddedtokens from all queries are then pooled and fed into the model as asummarized dense search history.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>geographic embeddings</strong>: The user’s geographicregion and device are embedded and concatenated.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>Simple binary and continuous features</strong>: such asthe user’s gender, logged-in state and age are input directly into thenetwork as real values normalized to [0, 1].&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>example (sample) age</strong>: ML model often has biastowards past samples because there’s more data about them. It’s commonto promote video freshness during re-rank phase, but YouTube also try toreduce this bias as early as in candidate generation phase. We definethe example age to be the time between training and obtaining thissample. e.g. <code>t</code> days earlier, after having watched video<code>v</code>, user searched word <code>w</code> and clicked on anothervideo <code>y</code>. We use this sample in training, so its sample ageis <code>t</code>. By introducing this feature, our model no longerreflects the average watch likelihood in the training window of severalweeks, but the likelihood at a specific time step. At serving time, thisfeature is set to zero (or slightly negative) to reflect that the modelis making predictions at the very end of the training window.&lt;/p&gt;&lt;p&gt;总结某&lt;ahref=”https://zhuanlan.zhihu.com/p/52504407”&gt;知乎讨论下的内容&lt;/a&gt;:example age 和消除第一种 ad position bias 做法类似&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;纯feed应用中前置内容点击率被高估：新闻客户端，position靠前的是虚高的，这部分叫positionbias，输入给模型的时候也要输入它们的位置，在线上预估时置0&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;搜索偏向应用中前置内容点击率被低估：手百，手淘，美团等，都会在首页默认展示feed，但很多目的明确的用户压根不会用这些推荐功能，导致这部分展示的内容点击率是被低估了。实际操作中大家可能只针对有feed互动的用户进行采样，抛弃了完全过路型用户的行为，也算是修正bias了&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="data-gathering-details"&gt;Data Gathering Details&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<strong>Class Balance</strong>: generate a fixed number oftraining examples per user, effectively weighting our users equally inthe loss function&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>Permutation-Invariant Pooling</strong>: in pooling,YouTube chose average pooling among sum, max, etc, because averageperformed the best. The important thing is, they decided to abandonsequence information whatsoever. Their explanation is below and I don’tquite buy it because they’re definitely better way to solve this problemthan discarding the info altogether. In addition, they did publish &lt;ahref=”https://research.google/pubs/latent-cross-making-use-of-context-in-recurrent-recommender-systems/”&gt;anotherpaper on sequence-based recommendation system&lt;/a&gt; later. I think at thispaper’s publishing time, they either didn’t want to publicize it or havenot tested that in detail.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Consider the user has just issued a search query for “taylor swift”.Since our problem is posed as predicting the next watched video, aclassifier given this information will predict that the most likelyvideos to be watched are those which appear on the corresponding searchresults page for “taylor swift”. Unsurprisingly, reproducing the user’slast search page as homepage recommendations performs very poorly. Bydiscarding sequence information and representing search queries with anunordered bag of tokens, the classifier is no longer directly aware ofthe origin of the label.&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>Next-Watch Heldout</strong>: at the time, manycollaborative filtering systems implicitly choose the labels and contextby holding out a random item and predicting it from other items in theuser’s history. They decided to always hold out and predict user’s nextwatch and achieved much better results. This is now already thestandard. In fact it appeared in my college class CS4780 byKillian.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="training"&gt;Training:&lt;/h3&gt;&lt;p&gt;In these experiments, a vocabulary of 1M videos and 1M search tokenswere embedded with 256 floats each in a maximum bag size of 50 recentwatches and 50 recent searches. The softmax layer outputs a multinomialdistribution over the same 1M video classes with a dimension of 256(which can be thought of as a separate output video embedding).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<strong>negative sampling</strong>: for the same user, inaddition to his watched videos, we also sample some unwatched videos togeneralize the model.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>Importance Weighting</strong>: we do not take softmaxover all the 1M videos. Instead, we sample ~5000 of them, compute theirprobability, re-weight them based on importance (<strong>watch time,click rate?</strong>) and only compute loss over these samples.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>loss</strong>: this is a multi-class classificationproblem, so we use cross-entropy loss naturally&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="inference-serving"&gt;Inference / Serving:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<strong>kNN</strong>: In serving, we need an algorithm sublinearto number of classes (videos to recommend). Say the last layer of thenetwork has hidden dimension <span class="math inline"><em>d</em></span>and we have <span class="math inline"><em>N</em></span> videos topredict. Decoding hidden dimension back into per-video logits and take asoftmax takes &lt;spanclass=”math inline”&gt;<em>O</em>(<em>d</em><em>N</em>)&lt;/span&gt;. Sortingtakes &lt;spanclass=”math inline”&gt;<em>O</em>(<em>N</em>log <em>N</em>)&lt;/span&gt;. Thetotal time is &lt;spanclass=”math inline”&gt;<em>O</em>(<em>d</em><em>N</em> + <em>N</em>log <em>N</em>)&lt;/span&gt;.On the other hand, naive kNN takes &lt;spanclass=”math inline”&gt;<em>O</em>(<em>d</em><em>N</em>)&lt;/span&gt; time intotal and some heuristic version like Ball tree can take &lt;spanclass=”math inline”&gt;<em>O</em>(<em>d</em>log <em>N</em>)&lt;/span&gt;​​.&lt;/p&gt;&lt;p&gt;distance is based on dot product&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>how do we get video embedding? (where isdecoder?)</strong>&lt;/p&gt;<figure>&lt;img src=”/images/youtube-candidate-generation-architecture.png”alt=”youtube-candidate-generation-architecture” /&gt;&lt;figcaptionaria-hidden=”true”&gt;youtube-candidate-generation-architecture&lt;/figcaption&gt;</figure>&lt;p&gt;All classification models have to include a decoder (FC layer) at theend of the network but before the softmax layer to decode the hiddenvector back to per-video logits in video ID space to make prediction. Ifwe have 1M videos and hidden vector is of dimension 256, the decoder isa matrix of size [256, 1M]. However, the graph presented in the paper isvery confusing because the authors omit drawing the decoder and made itan implicit part of the softmax layer.&lt;/p&gt;&lt;p&gt;Anyway, we know we do have that decoder, so it’s natural to use thevectors in the decoder as our video embedding. The i-th video’sembedding is simply <code>decoder[:, i]</code>.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>weight sharing / weight tying</strong>: this is a conceptI encountered in nanoGPT and has become clear here. At the beginning ofthe network, we have a video encoder from video ID space to hiddenspace; at the end of the network, we have a video decoder from hiddenspace back to video ID space. It is possible to share weights (use thesame weights) in encoder and decoder to save space (recall this partcosts the most parameter). This is just mentioned by people in &lt;ahref=”https://zhuanlan.zhihu.com/p/52504407”&gt;comment section&lt;/a&gt; and isnot implemented by Google.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="ranking"&gt;Ranking&lt;/h2&gt;&lt;h3 id="problem-setup-1"&gt;Problem Setup&lt;/h3&gt;&lt;p&gt;We pose ranking as <strong>predicting the expected watch time of avideo</strong>. Ranking by click-through rate often promotes deceptivevideos that the user does not complete (“clickbait”) whereas watch timebetter captures engagement&lt;/p&gt;&lt;h3 id="input-feature-engineering"&gt;Input (Feature Engineering)&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;user’s previous interaction with the item itself and other similaritems: e.g. user’s past history with the channel that uploaded the videobeing scored (how many videos has the user watched from this channel?When was the last time the user watched a video on this topic?)&lt;/li&gt;&lt;li&gt;propagate information from candidate generation into ranking in theform of features: e.g. which sources nominated this video candidate?What scores did they assign?&lt;/li&gt;&lt;li&gt;frequency of past video impressions: If a user was recentlyrecommended a video but did not watch it then the model will naturallydemote this impression on the next page load.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="input-details"&gt;Input Details&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;embedding space should increase approximately proportional to the<strong>logarithm</strong> of the number of unique values of dataspace&lt;/li&gt;&lt;li&gt;very large space (video id &amp; query token) is <strong>truncatedby click-through-rate</strong>. So we only recommend videos above acertain CTR. Note these filtered out videos can still be searchedout.&lt;/li&gt;&lt;li&gt;Out-of-vocabulary values (new / truncated videos) are mapped to the<strong>zero embedding</strong>.&lt;/li&gt;&lt;li&gt;Continuous features are always <strong>normalized</strong> to [0,1)&lt;/li&gt;&lt;li&gt;In addition to the raw normalized feature &lt;spanclass=”math inline”&gt;<em>x</em>&lt;/span&gt;, we also input &lt;spanclass=”math inline”&gt;$\sqrt x$&lt;/span&gt; and &lt;spanclass=”math inline”&gt;<em>x</em><sup>2</sup>&lt;/span&gt;​, giving the networkmore expressive power by allowing it to easily form <strong>super- andsub-linear</strong> functions of the feature. Feeding powers ofcontinuous features was found to improve offline accuracy.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="training-1"&gt;Training&lt;/h3&gt;&lt;p&gt;We use Logistic Regression to predict expected watch time (EWT) of avideo, but LR only predicts 0 or 1. How can we use it to predict EWT? Weuse weighted logistic regression, where the positive (clicked)impressions are weighted by the observed watch time on the video.Negative (unclicked) impressions all receive unit weight.&lt;/p&gt;&lt;p&gt;In training, we use the weighted cross entropy loss: &lt;spanclass=”math display”&gt;<code>WeightedCrossEntropy</code> = −∑<sub><em>i</em></sub>[<em>T</em><sub><em>i</em></sub><em>y</em><sub><em>i</em></sub>log <em>p</em><sub><em>i</em></sub> + (1 − <em>y</em><sub><em>i</em></sub>)log (1 − <em>p</em><sub><em>i</em></sub>)]&lt;/span&gt;### Serving&lt;/p&gt;&lt;p&gt;In serving, we directly output &lt;spanclass=”math inline”&gt;<em>e</em><sup><em>θ</em><em>x</em></sup>&lt;/span&gt; asthe predicted watch time. Why is this the watch time? Recall in LogisticRegression, we have a binary classification problem, so we can define&lt;spanclass=”math inline”&gt;<em>P</em>(<em>Y</em><sub><em>i</em></sub> = 1|<em>X</em><sub><em>i</em></sub>) = <em>p</em>, <em>P</em>(<em>Y</em><sub><em>i</em></sub> = 0|<em>X</em><sub><em>i</em></sub>) = 1 − <em>p</em>&lt;/span&gt;and <span class="math display">\(p = \frac{1}{1 + e^{-\theta^T x}}\)</span> In statistics, we define odds as: &lt;spanclass=”math display”&gt;\(\texttt{odds} = \frac{p}{1-p} = \frac{1}{e^{-\theta^Tx}} = e^{\theta^Tx}\)&lt;/span&gt; If we take a log at both sides, we have the log odds, or<strong>logits</strong> <span class="math display">\(\ln(\texttt{odds}) = \ln(\frac{p}{1-p}) = \theta^Tx\)</span> Now let’s look at the our weighted logistic regressionproblem: Positive impressions are weighted by watch time &lt;spanclass=”math inline”&gt;<em>T</em><sub><em>i</em></sub>&lt;/span&gt;. Negativeimpressions receive unit weight. We have a total &lt;spanclass=”math inline”&gt;<em>N</em>&lt;/span&gt; videos, and &lt;spanclass=”math inline”&gt;<em>k</em>&lt;/span&gt; of them are positive (clicked). Wehave <span class="math inline"><em>k</em> = <em>p</em><em>N</em></span>and the expected watch time of all videos is &lt;spanclass=”math inline”&gt;$\mathbb E[T] = \frac{\sum_i^k T_i}{N}$&lt;/span&gt;. Nowlook at our weighted odds: <span class="math display">\(\begin{align}\texttt{wghted odds} &amp;amp;= \frac{\text{weighted posprob}}{\text{weighted neg prob}}\\&amp;amp;= \frac{\sum_i^k T_i}{N-k} = \frac{\sum_i^k T_i}{N-pN} =\frac{\sum_i^k T_i}{N(1-p)} = \frac{\sum_i^k T_i}{N}\frac{1}{1-p}\\&amp;amp; = \mathbb E[T](1+p) \approx \mathbb E[T] \hspace{20px}\text{($p$is small)}\end{align}\)</span> Therefore, at serving time, we can directly output &lt;spanclass=”math inline”&gt;<em>e</em><sup><em>θ</em><em>x</em></sup>&lt;/span&gt;because it is the expected watch time.&lt;/p&gt;&lt;h3 id="evaluation-metric"&gt;Evaluation Metric&lt;/h3&gt;&lt;p&gt;Since we’re essentially predicting a video’s watch time, we“wrongly-predicted video’s watch time” as our evaluation metric. In thepaper, author called it “weighted, per-user loss”. Specifically, foreach user, we feed the model both positive (clicked) and negative(unclicked) impressions shown to him on a single page. We first predicttheir respective watch time with our model. “mispredicted watch time” isthe positive video’s watch time when the negative impression receives alonger predicted watch time than the positive impression. This user’sloss is total mispredicted watch time / total watch time of ground-truth(positive) impressions.&lt;/p&gt;&lt;h2 id="takeaway"&gt;Takeaway&lt;/h2&gt;&lt;p&gt;example age&lt;/p&gt;&lt;p&gt;expected watch time&lt;/p&gt;&lt;p&gt;KNN - quick serving&lt;/p&gt;&lt;p&gt;feature engineering&lt;/p&gt;&lt;p&gt;what is <strong>surrogate problem</strong>?&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=WK_Nr4tUtl8&lt;/p&gt;&lt;h2 id="comment"&gt;Comment&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;RNN这个方法在17年已经由AlexBeutel做上线了，其实在16年初就想做，只是效果还没有完全出来，后来Alex发现了原先做法的一些弱点，很快改进之后就上线了，作为重要的candidatesgeneration来源；排序目标只写了EWT，一是Google的技术保密要求，毕竟还是要做到HR-safe的，论文只能点到即止，二是相对有效并且能够比分开预测ctr和staytime能节省servinglatenc&lt;/p&gt;&lt;p&gt;– &lt;ahref=”https://www.zhihu.com/people/dcb32aa26dd3a456bd79d2c2cdffa433”&gt;严林&lt;/a&gt;under &lt;ahref=”https://zhuanlan.zhihu.com/p/52169807”&gt;重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This is a detailed reading of Google’s paper Deep Neural Networks for YouTube Recommendations]]></summary></entry><entry><title type="html">Running MobileBert on Android with TensorFlow Lite</title><link href="https://yao-lirong.github.io/2024/09/22/running-mobilebert-on-android-with-tensorflow-lite.html" rel="alternate" type="text/html" title="Running MobileBert on Android with TensorFlow Lite"/><published>2024-09-22T04:00:00+00:00</published><updated>2024-09-22T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/09/22/running-mobilebert-on-android-with-tensorflow-lite</id><content type="html" xml:base="https://yao-lirong.github.io/2024/09/22/running-mobilebert-on-android-with-tensorflow-lite.html"><![CDATA[<p>So Google, fxxk you.</p> <p><span id="more"></span>&lt;h2 id="prerequsities"&gt;Prerequsities&lt;/h2&gt;&lt;p&gt;This picture very well explains how TFLite works and also whyTensorFlow 2 has both a <code>tf</code> and a <code>keras</code>.&lt;/p&gt;<figure>&lt;imgsrc=”https://web.archive.org/web/20220216170621if_/https://www.tensorflow.org/lite/images/convert/workflow.svg”alt=”TFLite Workflow” /&gt;&lt;figcaption aria-hidden="true"&gt;TFLite Workflow&lt;/figcaption&gt;</figure>&lt;h2 id="detours"&gt;Detours&lt;/h2&gt;&lt;p&gt;This section is mostly rant, but it is meaningful in preventing youfrom taking any of the wrong path. Skip to the next section for atutorial on what to do.&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;&lt;p&gt;We first found the Google’s official release &lt;ahref=”https://github.com/google-research/google-research/tree/master/mobilebert”&gt;http://google-research/mobilebert/&lt;/a&gt;,but&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the tutorial was unclear: Why do I need <code>data_dir</code> and<code>output_dir</code> to export TFLite? How do I even read in thepre-trained weights?&lt;/li&gt;&lt;li&gt;the code itself was pretty messy: why did they have export functionand training function all at this same file <code>run_squad.py</code>and the only way to tell the program whether to train/export is checkingwhether <code>export_dir is None</code> rather than passing a flag?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In figuring out what each part does in this code, I looked upTensorFlow 1’s doc and good lord they were broken. Google doesn’t evenhost it anywhere: you have to go to &lt;ahref=”https://github.com/tensorflow/docs/tree/master/site/en/r1”&gt;aGitHub repo&lt;/a&gt; to read them in <code>.md</code> format. At this momentI decided I will not touch anything written by TensorFlow 1’s API. (Iactually went through this pain back at my first ML intern in Haier, butnot again)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Sidenote before this: I didn’t know you can release model’s onKaggle (thought everyone releases on Hugging Face) and Google &lt;ahref=”https://www.kaggle.com/discussions/product-feedback/448425”&gt;movedtheir own TensorFlow Hub to Kaggle&lt;/a&gt;&lt;/p&gt;&lt;p&gt;So my supervisor found me &lt;ahref=”https://www.kaggle.com/models/google/mobilebert/tensorFlow1”&gt;amore readable Google release on Kaggle&lt;/a&gt; with some high-level API anddoesn’t require you to read the painful source code. The above link has<a href="https://www.kaggle.com/models/tensorflow/mobilebert">a redirectto TensorFlow 2 implementation</a> with an official TFLite release. Howneat.&lt;/p&gt;&lt;p&gt;However, the official TFLite release&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;doesn’t have &lt;ahref=”https://www.tensorflow.org/lite/guide/signatures”&gt;signature&lt;/a&gt; -TensorFlow’s specification of input and output (remember when you passinputs to a model you need to give name to theme.g. <code>token_ids = ..., mask = ...</code>) which is required forXiaomi Service Framework to run a TFLite. P.S. Yes signature is notrequired to specify when exporting, but for god’s sake all your tutorialteaches people to use it and your own released ditched it? WTFGoogle.&lt;/li&gt;&lt;li&gt;is broken (as expected?). &lt;ahref=”forgot%20where%20the%20guide%20was”&gt;When I tried to run it on myPC&lt;/a&gt;, I got the following error<code>indices_has_only_positive_elements was not true.gather index out of boundsNode number 2 (GATHER) failed to invoke.gather index out of boundsNode number 2 (GATHER) failed to invoke</code>.Someone encountered &lt;ahref=”https://github.com/tensorflow/tensorflow/issues/59730”&gt;a similarbug&lt;/a&gt; while running the example code provided by TensorFlow and theGoogle SWE found a bug in their example. At this moment I decided not totrust this TFLite file anymore and just convert it on my own.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;So let’s use this official TensorFlow 2 implementation and &lt;ahref=”forgot%20where%20the%20guide%20was”&gt;convert it to TFLite&lt;/a&gt;. Itwas all good and running on my PC, but&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Its output format was really weird&lt;ul&gt;&lt;li&gt;It output consists of<code>'mobile_bert_encoder', 'mobile_bert_encoder_1', 'mobile_bert_encoder_2', ..., 'mobile_bert_encoder_51'</code>&lt;/li&gt;&lt;li&gt;Each of these has shape <code>(1, 4, 128, 128)</code> for a<code>seq_length = 128, hidden_dim = 512</code> model. I figured 4 beingthe number of heads and the other 128 is <code>hidden_dim</code> foreach head.&lt;/li&gt;&lt;li&gt;They output attention scores, not the final encoded vector: my inputwas 5 tokens and they output is<code>output[0, 0, 0, :] = array([0.198, 0.138, 0.244, 0.148, 0.270, 0. , 0. , ...</code>.They sum to 1 and any other positions at <code>output</code> are 0 , soattention score was my best guess.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;It doesn’t run on Android phone:<code>tflite engine load failed due to java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 153. Are you using old TFLite binary with newer model?</code>A &lt;ahref=”https://stackoverflow.com/questions/67883156/tflite-runtime-op-builtin-code-out-of-range-131-are-you-using-old-tflite-bi”&gt;StackOverflow answer&lt;/a&gt; suggests the TensorFlow used to export TFLiterunning on my PC doesn’t match the version of TFLite run time on thisAndroid phone. It can also be caused by me messing up with the wholeenvironment while installing &lt;ahref=”https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model”&gt;Optimum&lt;/a&gt;to export TFLite last night, but I didn’t bother to look because Ifinally found the solution&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;And comes the savior, the king, the go-to solution in MLOps -Huggingface. Reminded by a discussion I read by chance, I came torealize <code>TFMobileBertModel.from_pretrained</code> actually returnsthe Keras model (and the without <code>TF</code> version returns aPyTorch model). That means I can just use Hugging Face API to read itin, then use the native TensorFlow 2 API to export to TFLite. Andeverything works like a charm now. The final output signature is justHugging Face’s familiar<code>['last_hidden_state', 'pooler_output']</code>&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2 id="converting-tensorflow-model-to-tflite"&gt;Converting TensorFlowModel to TFLite&lt;/h2&gt;&lt;p&gt;Conversion is pretty straight forward. You can just follow thisofficial guide: &lt;ahref=”https://www.tensorflow.org/lite/models/convert/convert_models”&gt;ForMobile &amp; Edge: Convert TensorFlow models&lt;/a&gt;. Though I actuallyfollowed my predecessor’s note (which actually comes from &lt;ahref=”https://www.tensorflow.org/lite/guide/signatures”&gt;another TFtutorial&lt;/a&gt;). He also told me to caution that calling<code>tf.disable_eager_execution()</code> can lead to absence ofsignature, so do not call <code>tf.disable_eager_execution()</code> todisable eager mode.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertTokenizerFast, TFMobileBertModel</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># Convert Model</span></span><br/><span class="line"><span class="keyword">if</span> be_sane:</span><br/><span class="line"> bert_model = TFMobileBertModel.from_pretrained(kerasH5_model_path) <span class="keyword">if</span> keras_file <span class="keyword">else</span> &lt;/span&gt;<br/><span class="line"> TFMobileBertModel.from_pretrained(pytorch_model_path, from_pt = <span class="literal">True</span>)</span><br/><span class="line"> converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br/><span class="line"><span class="keyword">else</span>: <span class="comment"># be crazy or already knows the messy TensorFlow.SavedModel format</span></span><br/><span class="line"> converter = tf.lite.TFLiteConverter.from_saved_model(model_path)</span><br/><span class="line">tflite_model = converter.convert()</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># Save Model</span></span><br/><span class="line">tflite_output_path = <span class="string">'/model.tflite'</span></span><br/><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(tflite_output_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br/><span class="line"> f.write(tflite_model)</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># Check Signature</span></span><br/><span class="line"><span class="comment"># Empty signature means error in the export process and the file cannot be used by Xiaomi Service Framework</span></span><br/><span class="line">interpreter = tf.lite.Interpreter(model_path=tflite_output_path)</span><br/><span class="line">interpreter = tf.lite.Interpreter(model_content=tflite_model)</span><br/><span class="line">interpreter.allocate_tensors()</span><br/><span class="line">signatures = interpreter.get_signature_list()</span><br/><span class="line"><span class="built_in">print</span>(<span class="string">"tflite model signatures:"</span>, signatures)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;blockquote&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">{<span class="string">'serving_default'</span>: {<span class="string">'inputs'</span>: [<span class="string">'attention_mask'</span>,</span><br/><span class="line"><span class="string">'input_ids'</span>,</span><br/><span class="line"><span class="string">'token_type_ids'</span>],</span><br/><span class="line"><span class="string">'outputs'</span>: [<span class="string">'last_hidden_state'</span>, <span class="string">'pooler_output'</span>]}}</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/blockquote&gt;&lt;p&gt;In addition, summarizing from the detours I took,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Do not use Hugging Face’s Optimum for (at least vanilla) conversionbecause it just calls the above command (see &lt;ahref=”https://github.com/huggingface/optimum/blob/e0f58121140ce4baa01919ad70a6c13e936f7605/optimum/exporters/tflite/convert.py#L363-L371”&gt;code&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;Do not even bother to look at &lt;ahref=”https://github.com/google-research/google-research/tree/master/mobilebert#export-mobilebert-to-tf-lite-format”&gt;Google’soriginal code&lt;/a&gt; converting MobileBert to TFLite because nobody knowswhat they’re writing.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="running-tflite-on-pc"&gt;Running TFLite (on PC)&lt;/h2&gt;&lt;p&gt;Running TFLite on Android phone is the other department’s task. Ijust want to run the TFLite file on PC to test everything’s good. To dothat, I strictly followed TensorFlow’s official guide: &lt;ahref=”https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python”&gt;<strong>TensorFlowLite inference: Load and run a model in Python</strong>&lt;/a&gt;.Ourconverted models have the signatures, you can just follow the “with adefined SignatureDef” guide.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">tokenizer = MobileBertTokenizerFast(<span class="string">f"<span class="subst">{model_path}</span>/vocab.txt"</span>)</span><br/><span class="line">t_output = tokenizer(<span class="string">"越过长城，走向世界"</span>, return_tensors=<span class="string">"tf"</span>)</span><br/><span class="line">ii, tt, am = t_output[<span class="string">'input_ids'</span>], t_output[<span class="string">'token_type_ids'</span>], t_output[<span class="string">'attention_mask'</span>]</span><br/><span class="line"><span class="comment"># <code class="language-plaintext highlighter-rouge">get_signature_runner()</code> with empty input gives the "serving_default" runner</span></span><br/><span class="line"><span class="comment"># <code class="language-plaintext highlighter-rouge">runner</code> input parameter is specified by <code class="language-plaintext highlighter-rouge">serving_default[&amp;#x27;inputs&amp;#x27;]</code></span></span><br/><span class="line">runner = interpreter.get_signature_runner() </span><br/><span class="line">output = runner(input_ids = ii, token_type_ids = tt, attention_mask = am)</span><br/><span class="line"><span class="keyword">assert</span> output.keys == [<span class="string">'last_hidden_state'</span>, <span class="string">'pooler_output'</span>]</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;On the other hand, for a model without signatures, you need to usethe more primitive API <code>input_details</code> and<code>output_details</code>. They specify the following properties,where <code>index</code> is (probably) the index of this tensor in thecompute graph. To pass input values and get output values, you need toaccess them by this index.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">interpreter.allocate_tensors()</span><br/><span class="line">input_details = interpreter.get_input_details()</span><br/><span class="line">output_details = interpreter.get_output_details()</span><br/><span class="line"></span><br/><span class="line">interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">'index'</span>], input_data)</span><br/><span class="line">interpreter.invoke()</span><br/><span class="line">output_data = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">'index'</span>])</span><br/><span class="line"><span class="built_in">print</span>(output_data)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The following is the <code>input_details</code> of the non-signatureGoogle packed MobileBert.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">interpreter.get_input_details()</span><br/><span class="line">[{<span class="string">'name'</span>: <span class="string">'model_attention_mask:0'</span>,</span><br/><span class="line"> <span class="string">'index'</span>: <span class="number">0</span>,</span><br/><span class="line"> <span class="string">'shape'</span>: array([ <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br/><span class="line"> <span class="string">'shape_signature'</span>: array([ <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br/><span class="line"> <span class="string">'dtype'</span>: numpy.int64,</span><br/><span class="line"> <span class="string">'quantization'</span>: (<span class="number">0.0</span>, <span class="number">0</span>),</span><br/><span class="line"> <span class="string">'quantization_parameters'</span>: {<span class="string">'scales'</span>: array([], dtype=float32),</span><br/><span class="line"> <span class="string">'zero_points'</span>: array([], dtype=int32),</span><br/><span class="line"> <span class="string">'quantized_dimension'</span>: <span class="number">0</span>},</span><br/><span class="line"> <span class="string">'sparsity_parameters'</span>: {}},</span><br/><span class="line"> {…}]</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="numerical-accuracy"&gt;Numerical Accuracy&lt;/h2&gt;&lt;p&gt;Our original torch/TensorFlow encoder and the converted TFLiteencoder, when both running on PC using Python, has a 1.2% difference intheir output (<code>last_hidden_state</code> or<code>pooled_output</code>). We <strong>do not know</strong> where thisdiscrepancy comes from.&lt;/p&gt;&lt;h2 id="converting-tokenizer-to-tflite"&gt;Converting Tokenizer toTFLite&lt;/h2&gt;&lt;p&gt;We exported and ran the <em>encoder</em>, but that’s not enough. Wecan’t ask the user to type in <code>token_ids</code> every time.Therefore, we need to integrate the preprocessor (tokenizer) into ourTFLite file. To do that, we first tried integrating &lt;ahref=”https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/multi-cased-preprocess/3”&gt;Google’sofficial Keras tokenizer implementation&lt;/a&gt; into our BERT model andconvert them together into a TFLite (yeah I didn’t learn the lesson).This failed in the converting step for reasons that would become clearlater. And we switched gears to follow some other guide and first try toconvert a standalone tokenizer to TFLite.&lt;/p&gt;&lt;p&gt;Tokenizer is a part of the TensorFlow Text library. I followed the &lt;ahref=”https://www.tensorflow.org/text/guide/text_tf_lite”&gt;<strong>officialguide: Converting TensorFlow Text operators to TensorFlowLite</strong>&lt;/a&gt; with <code>text.FastBertTokenizer</code>. Note whenyou follow it, do it carefully and closely. I encountered a few problemsalong the way:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;&lt;p&gt;When you change the <code>text.WhitespaceTokenizer</code> inguide to our <code>text.FastBertTokenizer</code>, remember to specify a<code>text.FastBertTokenizer(vocab=vocab_lst)</code>. We need not thepath to the vocab but the actual liste.g. <code>[ "[PAD]", "[unused0]", "[unused1]", ...]</code> describesthe vocab where <code>[PAD]</code> maps to token id 0,<code>[unused0]</code> to token id 1, and so on.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>text.FastBertTokenizer</code> (or the standard version)does not add <code>[CLS]</code> token for you. Google says this is tomake sure “you are able to manipulate the tokens and determine how toconstruct your segments separately” (&lt;ahref=”https://github.com/tensorflow/text/issues/146”&gt;GitHub issue&lt;/a&gt;).How considerate you are, dear Google. I spent one and a half dayfiguring out how to add these tokens when the model’s input length needsto be fixed, otherwise it triggers TensorFlow’s compute graph to throw“can’t get variable-length input” error. I finally found a solution in&lt;ahref=”https://github.com/google-ai-edge/mediapipe/blob/a91256a42bbe49f8ebdb9e2ec7643c5c69dbec6f/mediapipe/model_maker/python/text/text_classifier/bert_tokenizer.py#L58-L71”&gt;Google’smediapipe’s implementation&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>Could not translate MLIR to FlatBuffer</code> when running<code>tflite_model = converter.convert()</code>: as mentioned, you mustfollow the guide very carefully. The guide specifies a TensorFlow Textversion. If not this version, the conversion would fail&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">pip install -U <span class="string">"tensorflow-text==2.11.*"</span></span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>Encountered unresolved custom op: FastBertNormalize</code>when running converted interpreter / signature: as stated in the &lt;ahref=”https://www.tensorflow.org/text/guide/text_tf_lite#inference”&gt;Inferencesection of the guide&lt;/a&gt;, tokenizers are custom operations and need tobe specified when running inference. (I can’t find doc for<code>InterpreterWithCustomOps</code> anywhere but it does have anargument <code>model_path</code>)&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">interp = interpreter.InterpreterWithCustomOps(</span><br/><span class="line"> model_content=tflite_model,<span class="comment"># or model_path=TFLITE_FILE_PATH</span></span><br/><span class="line"> custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;TensorFlow Text custom ops are not found on Android: the aboveinference guide writes&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;while the example below shows inference in Python, the steps aresimilar in other languages with some minor API translations&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;which is a total lie. Android does not support these operations asthe &lt;ahref=”https://www.tensorflow.org/lite/guide/op_select_allowlist#tensorflow_text_and_sentencepiece_operators”&gt;customtext op list&lt;/a&gt; only mentions python support.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;At the end, I did manage to 1 merge the above tokenizer andHuggingFace model, 2 export a TFLite model that reads in a text andoutputs the last hidden state. However, I seem to have lost that pieceof the code. Don’t worry though. Because thanks to Google’s shittyframework, it only works with very few tokenizer implementations anyway.The work-for-all solution is to build your own tokenizer in Java.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;P.S. While debugging the FlatBuffer error, I came across the &lt;ahref=”https://www.tensorflow.org/lite/guide/authoring”&gt;TensorFlowauthoring tool&lt;/a&gt; that can explicitly specify a function’s input outputformat and detect op unsupported by TFLite. However, the tools is prettybroken for me. Debugging this tool would probably take longer thanfinding the problem yourself online / ask on a forum.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="writing-your-own-tokenizer"&gt;Writing Your Own Tokenizer&lt;/h2&gt;&lt;p&gt;What’s weird is TensorFlow does have an official BERT on Androidexample. Reading it again, I found their tokenizer is actuallyimplemented by C++ (&lt;ahref=”https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier#key_features_of_the_bertnlclassifier_api”&gt;seethis example&lt;/a&gt;). The repo containing the tokenizer code is called &lt;ahref=”https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/text/tokenizers/bert_tokenizer.h”&gt;tflite-support&lt;/a&gt;.Finding &lt;ahref=”https://www.tensorflow.org/lite/inference_with_metadata/lite_support#current_use-case_coverage”&gt;thislibrary’s doc&lt;/a&gt;, it becomes clear that the text-related operations arecurrently not supported.&lt;/p&gt;<figure>&lt;img src=”/images/tflite-support.png”alt=”TFLite-Support Current use-case coverage” /&gt;&lt;figcaption aria-hidden="true"&gt;TFLite-Support Current use-casecoverage&lt;/figcaption&gt;</figure>&lt;p&gt;Google seems to have used JNI to call the C++ implementation oftokenizer (&lt;ahref=”https://github.com/tensorflow/tflite-support/blob/8ed4a7b70df385a253aad7ed7df782439f42da6c/tensorflow_lite_support/java/src/java/org/tensorflow/lite/task/text/nlclassifier/BertNLClassifier.java#L39-L53”&gt;seecode&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;Therefore, we’d better write our own tokenizer. Luckily Hugging Facealso has a Bert on Android example - &lt;ahref=”https://github.com/huggingface/tflite-android-transformers/tree/master/bert”&gt;tflite-android-transformers&lt;/a&gt;and writes more accessible code. We directly copied &lt;ahref=”https://github.com/huggingface/tflite-android-transformers/tree/master/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization”&gt;theirtokenizer implementation&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;However, when switching to Chinese vocabulary, the tokenizer goesglitchy. See the following example where we tokenize thesentence「越过长城 ，走向世界」&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># Our Java tokenizer gives the following tokens, which detokenizes to the following string</span></span><br/><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">19871</span>, <span class="number">20327</span>, <span class="number">14871</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">14460</span>, <span class="number">13743</span>, <span class="number">17575</span>, <span class="number">102</span>])</span><br/><span class="line"><span class="string">'[CLS] 越过长城 ， 走向世界 [SEP]'</span></span><br/><span class="line"></span><br/><span class="line"><span class="comment"># On the other hand, official Hugging Face python BertTokenizer gives</span></span><br/><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">6814</span>, <span class="number">7270</span>, <span class="number">1814</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">1403</span>, <span class="number">686</span>, <span class="number">4518</span>, <span class="number">102</span>])</span><br/><span class="line"><span class="string">'[CLS] 越 过 长 城 ， 走 向 世 界 [SEP]'</span></span><br/><span class="line"></span><br/><span class="line"><span class="comment"># Inspecting the first difference, our Java tokenizer seems to have used sentencepiece </span></span><br/><span class="line">tokenizer.decode([<span class="number">19871</span>])</span><br/><span class="line"><span class="string">'##过'</span></span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;It turns out &lt;ahref=”https://github.com/google-research/bert/blob/master/multilingual.md#tokenization”&gt;BERTin its original implementation&lt;/a&gt; (&lt;ahref=”https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/tokenization.py#L207”&gt;code&lt;/a&gt;)does not use sentence-piece tokenizer on Chinese characters. Instead, ituses character level tokenizer. Therefore, we need to first insert awhitespace to every character to ensure sentence-piece isn’t applied.Note Hugging Face tokenizer follows BERT original python code veryclosely so you can &lt;ahref=”https://github.com/huggingface/tflite-android-transformers/blob/dcd6da1bfb28e3cd6bc83b58a112cdcd3d6cc2fe/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization/BasicTokenizer.java#L34”&gt;easilyfind where to insert&lt;/a&gt; that piece of code.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Bert original implementation in Python, with Chinese logic&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, text</span>):</span><br/><span class="line"> <span class="string">"""Tokenizes a piece of text."""</span></span><br/><span class="line"> text = convert_to_unicode(text)</span><br/><span class="line"> text = <span class="variable language_">self</span>._clean_text(text)</span><br/><span class="line"> <span class="comment"># Chinese Logic</span></span><br/><span class="line"> text = <span class="variable language_">self</span>._tokenize_chinese_chars(text)</span><br/><span class="line"> orig_tokens = whitespace_tokenize(text)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging Face tokenizer in Java, without Chinese logic&lt;/p&gt;<figure class="highlight java">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">BasicTokenizer</span> {</span><br/><span class="line"> <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">tokenize</span><span class="params">(String text)</span> {</span><br/><span class="line"> <span class="type">String</span> <span class="variable">cleanedText</span> <span class="operator">=</span> cleanText(text);</span><br/><span class="line"> <span class="comment">// Insert Here</span></span><br/><span class="line"> List&lt;String&gt; origTokens = whitespaceTokenize(cleanedText);</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="building-a-classifier"&gt;Building a Classifier&lt;/h2&gt;&lt;p&gt;The final task is actually to build a classifier of 28 online storecommodity classes. As I mentioned in the <a href="#Detours">Detourssection</a>, I do not know and don’t wanna bother to know how to defineor change a signature. Therefore, I again turn to Hugging Face for its<code>MobileBertForSequenceClassification</code>.&lt;/p&gt;&lt;p&gt;The default classification head only has 1 layer, I changed itsstructure to give it more expressive power.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">model = MobileBertForSequenceClassification.from_pretrained(</span><br/><span class="line"> model_path, num_labels=<span class="built_in">len</span>(labels), problem_type=<span class="string">"multi_label_classification"</span>,</span><br/><span class="line"> id2label=id2label, label2id=label2id)</span><br/><span class="line">model.classifier = nn.Sequential(OrderedDict([</span><br/><span class="line"> (<span class="string">'fc1'</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br/><span class="line"> (<span class="string">'relu1'</span>, nn.LeakyReLU()),</span><br/><span class="line"> (<span class="string">'fc2'</span>, nn.Linear(<span class="number">1024</span>, num_labels))</span><br/><span class="line">]))</span><br/><span class="line"><span class="comment"># Fine-tune …</span></span><br/><span class="line">torch.save(model.state_dict(), model_path)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;However, this throws error when you try to read such a fine-tunedmodel back in. <code>MobileBertForSequenceClassification</code> is setto have one-layer classification head, so it cannot read in yourself-defined classifier’s weights.&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br/><span class="line"> model_path, problem_type=<span class="string">"multi_label_classification"</span>,</span><br/><span class="line"> num_labels=len(labels), id2label=id2label, label2id=label2id)</span><br/><span class="line"></span><br/><span class="line">&gt; Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at ./ckpts/ and are newly initialized: [<span class="string">'classifier.bias'</span>, <span class="string">'classifier.weight'</span>]</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;To solve this, you can&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Save encoder weight and classifier weight separately, then load themseparately&lt;/li&gt;&lt;li&gt;Create a custom class corresponding to your weights and initializean instance of that class instead&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2 is clearly &lt;ahref=”https://github.com/huggingface/transformers/issues/1001#issuecomment-520162877”&gt;themore sensible way&lt;/a&gt;. You should read the very clearly written<code>MobileBertForSequenceClassification</code> to understand whatexactly needs to be changed. It turns out all we have to do is to extendthe original class and change its <code>__init__</code> part, so it hasa 2-layer classification head.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertForSequenceClassification, TFMobileBertForSequenceClassification</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">class</span> <span class="title class_">CustomMobileBertForSequenceClassification</span>(<span class="title class_ inherited__">MobileBertForSequenceClassification</span>):</span><br/><span class="line"> <span class="keyword">def</span> <span class="title function_"><strong>init</strong></span>(<span class="params">self, config</span>):</span><br/><span class="line"> <span class="built_in">super</span>().<strong>init</strong>(config)</span><br/><span class="line"> <span class="variable language_">self</span>.classifier = nn.Sequential(OrderedDict([</span><br/><span class="line"> (<span class="string">'fc1'</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br/><span class="line"> (<span class="string">'relu1'</span>, nn.LeakyReLU()),</span><br/><span class="line"> (<span class="string">'fc2'</span>, nn.Linear(<span class="number">1024</span>, <span class="number">28</span>))</span><br/><span class="line"> ]))</span><br/><span class="line"> <span class="variable language_">self</span>.post_init()</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">class</span> <span class="title class_">TFCustomMobileBertForSequenceClassification</span>(<span class="title class_ inherited__">TFMobileBertForSequenceClassification</span>):</span><br/><span class="line"> <span class="keyword">def</span> <span class="title function_"><strong>init</strong></span>(<span class="params">self, config, *inputs, **kwargs</span>):</span><br/><span class="line"> <span class="built_in">super</span>().<strong>init</strong>(config, <em>inputs, **kwargs)&lt;/span&gt;<br/><span class="line"> <span class="variable language_">self</span>.classifier = keras.Sequential([</span><br/><span class="line"> keras.layers.Dense(<span class="number">1024</span>, input_dim=<span class="number">768</span>, name=<span class="string">'fc1'</span>),</span><br/><span class="line"> keras.layers.LeakyReLU(alpha=<span class="number">0.01</span>, name = <span class="string">'relu1'</span>), <span class="comment"># Keras defaults alpha to 0.3</span></span><br/><span class="line"> keras.layers.Dense(<span class="number">28</span>, name=<span class="string">'fc2'</span>)</span><br/><span class="line"> ])</span><br/><span class="line"></span><br/><span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br/><span class="line"> model_path, problem_type=<span class="string">"multi_label_classification"</span>,</span><br/><span class="line"> num_labels=<span class="built_in">len</span>(labels), id2label=id2label, label2id=label2id)</span><br/><span class="line">tf_model = TFCustomMobileBertForSequenceClassification.from_pretrained(</span><br/><span class="line"> …, from_pt=<span class="literal">True</span>)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;However, you may find these two models output different values on thesame input. A closer look at weights unveil that <strong>Hugging Facedidn’t convert classifier’s weights from our Torch model to TensorFlowmodel correctly</strong>. We have to set them manually instead.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">tf_model.classifier.get_layer(<span class="string">"fc1"</span>).set_weights([torch_model.classifier.fc1.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc1.bias.detach()])</span><br/><span class="line">tf_model.classifier.get_layer(<span class="string">"fc2"</span>).set_weights([torch_model.classifier.fc2.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc2.bias.detach()])</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;And now we are finally ready to go.&lt;/p&gt;&lt;h2 id="quantization"&gt;Quantization&lt;/h2&gt;&lt;p&gt;I followed this official doc: &lt;ahref=”https://ai.google.dev/edge/litert/models/post_training_quantization”&gt;Post-trainingquantization&lt;/a&gt;. Because of time limit, I didn’t try Quantization AwareTraining (QAT).&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">vanilla_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br/><span class="line">tflite_model = vanilla_converter.convert()</span><br/><span class="line"></span><br/><span class="line">quant8_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br/><span class="line">quant8_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br/><span class="line">tflite_quant8_model = quant8_converter.convert()</span><br/><span class="line"></span><br/><span class="line">quant16_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br/><span class="line">quant16_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br/><span class="line">quant16_converter.target_spec.supported_types = [tf.float16]</span><br/><span class="line">tflite_quant16_model = quant16_converter.convert()</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;Below I report several key metrics for this Chinese-MobileBERT + a2-layer classification head of [768</em>1024, 1024*<code>class_num</code>].This was tested on a Xiaomi 12X with snapdragon 870. The baseline modelis my colleague’s BERT-Large implementation with accuracy 88.50% andsize 1230MB. My model’s accuracy was bad at first: 75.01% withhyper-parameter <code>weight_decay = 0.01, learning_rate = 1e-4</code>,but we searched out a good hyper-parameter of<code>weight_decay = 2e-4,learning_rate = 2e-5</code> giving 86.01%. Wehad 28 classes, 38000 training data in total, and trained for 5 epochswhere the validation accuracy roughly flattens.&lt;/p&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col style="width: 12%" /&gt;&lt;col style="width: 11%" /&gt;&lt;col style="width: 5%" /&gt;&lt;col style="width: 24%" /&gt;&lt;col style="width: 10%" /&gt;&lt;col style="width: 12%" /&gt;&lt;col style="width: 10%" /&gt;&lt;col style="width: 4%" /&gt;&lt;col style="width: 7%" /&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Quantization&lt;/th&gt;&lt;th&gt;Logit Difference&lt;/th&gt;&lt;th&gt;Accuracy&lt;/th&gt;&lt;th&gt;Accuracy (after hyper-param search)&lt;/th&gt;&lt;th&gt;Model Size (MB)&lt;/th&gt;&lt;th&gt;Inference Time(ms)&lt;/th&gt;&lt;th&gt;Power Usage(ma)&lt;/th&gt;&lt;th&gt;CPU(%)&lt;/th&gt;&lt;th&gt;Memory(MB)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;float32 (No quant)&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;75.01%&lt;/td&gt;&lt;td&gt;86.094%&lt;/td&gt;&lt;td&gt;101.4&lt;/td&gt;&lt;td&gt;1003.3&lt;/td&gt;&lt;td&gt;89.98&lt;/td&gt;&lt;td&gt;108.02&lt;/td&gt;&lt;td&gt;267.11&lt;/td&gt;&lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;float16&lt;/td&gt;&lt;td&gt;0.015%&lt;/td&gt;&lt;td&gt;75.01%&lt;/td&gt;&lt;td&gt;86.073%&lt;/td&gt;&lt;td&gt;51&lt;/td&gt;&lt;td&gt;838&lt;/td&gt;&lt;td&gt;64.15&lt;/td&gt;&lt;td&gt;108.77&lt;/td&gt;&lt;td&gt;377.11&lt;/td&gt;&lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;int8&lt;/td&gt;&lt;td&gt;4.251%&lt;/td&gt;&lt;td&gt;63.49%&lt;/td&gt;&lt;td&gt;85.947%&lt;/td&gt;&lt;td&gt;25.9&lt;/td&gt;&lt;td&gt;573.8&lt;/td&gt;&lt;td&gt;60.09&lt;/td&gt;&lt;td&gt;110.83&lt;/td&gt;&lt;td&gt;233.19&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;If look at the not fine-tuned, vanilla transformer encoder only, the<code>last_hidden_state</code> has a difference:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Quantization&lt;/th&gt;&lt;th&gt;Logit Difference&lt;/th&gt;&lt;th&gt;Model Size (MB)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;float32 (No quant)&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;97&lt;/td&gt;&lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;float16&lt;/td&gt;&lt;td&gt;0.1%&lt;/td&gt;&lt;td&gt;48.1&lt;/td&gt;&lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;int8&lt;/td&gt;&lt;td&gt;19.8%&lt;/td&gt;&lt;td&gt;24.9&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id="small-language-models"&gt;Small Language Models&lt;/h2&gt;&lt;p&gt;BERT is the go-to option for classification task. But when it comesto small BERT, we had several options:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;mobileBERT&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;distilledBERT&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;tinyBERT&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As the post is about, we used mobileBERT at last because it’s byGoogle Brain and Google probably knows their thing best.&lt;/p&gt;&lt;p&gt;On the other hand, if you’re looking for small generative model,which people mostly call SLM (Small Language Model) as opposed to LLM, Ifound these options but didn’t try them myself.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;openELM: Apple, 1.1B&lt;/li&gt;&lt;li&gt;Phi-2: Microsoft, 2.7B&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="post-script"&gt;Post Script&lt;/h2&gt;&lt;p&gt;If you want to build an app utilizing edge transformer, I wouldrecommend to read the source code of &lt;ahref=”https://github.com/huggingface/tflite-android-transformers”&gt;HuggingFace’s toy app&lt;/a&gt;. It doesn’t have a README or tutorial, nor have Igone through it personally, but everything from TensorFlow sucks(including MediaPipe unfortunately)&lt;/p&gt;&lt;p&gt;When checking back on this tutorial at date 2024/12/28, I foundGoogle released &lt;ahref=”https://github.com/google-ai-edge/ai-edge-torch”&gt;AI EdgeTorch&lt;/a&gt;, the official tool converting PyTorch models into a .tfliteformat. So you may probably want to try this first, but again, don’ttrust anything from TensorFlow team.&lt;/p&gt;</span></figure></span></figure></p>]]></content><author><name></name></author><summary type="html"><![CDATA[So Google, fxxk you.]]></summary></entry><entry><title type="html">Variational Inference</title><link href="https://yao-lirong.github.io/2024/09/09/variational-inference.html" rel="alternate" type="text/html" title="Variational Inference"/><published>2024-09-09T04:00:00+00:00</published><updated>2024-09-09T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/09/09/variational-inference</id><content type="html" xml:base="https://yao-lirong.github.io/2024/09/09/variational-inference.html"><![CDATA[<h2 id="probabilistic-latent-variable-models">Probabilistic LatentVariable Models</h2> <p>The two general forms of probabilistic models are:</p> <ul><li><span class="math inline"><em>p</em>(<em>x</em>)</span>: a typicalprobabilistic distribution. In this model, we call &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; the query.</li><li>&lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em> ∣ <em>x</em>)&lt;/span&gt;: aconditional probabilistic distribution. In this model, we cal &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; the evidence and &lt;spanclass="math inline"&gt;<em>y</em>&lt;/span&gt; the query.</li></ul> <p>Latent variable models are models that have variables other than thequery and the evidence.</p> <ul><li><p>&lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>) = ∑<sub><em>z</em></sub><em>p</em>(<em>x</em> ∣ <em>z</em>) <em>p</em>(<em>z</em>)&lt;/span&gt;​</p><p>A classic latent variable model of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)&lt;/span&gt; is the mixture model,where <span class="math inline"><em>p</em>(<em>x</em>)</span> isactually a mixture of several different probabilistic model. Forexample, in the following graph, &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; is a discrete variablerepresenting which class a datapoint belongs to and is represented bydifferent colors here. &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em> ∣ <em>z</em>)&lt;/span&gt; is eachclass’s probability distribution, where in this case can each be modeledby a Gaussian. And &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)&lt;/span&gt;​ when we observe it, isjust a bunch of uncolored datapoints and is hard to fit a distributionon it. However, we can see it’s roughly spread in 3 clusters so weintroduce the latent variable representing class and we can now well fita Gaussian mixture model on it (a mixture of 3 Gaussians)</p><figure>&lt;img src="/images/Mixture-Gaussian-Distribution.png"alt="Gaussian Mixture Model" /&gt;<figcaption aria-hidden="true">Gaussian Mixture Model</figcaption></figure></li><li><p>&lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em> ∣ <em>x</em>) = ∑<sub><em>z</em></sub><em>p</em>(<em>y</em> ∣ <em>x</em>, <em>z</em>) <em>p</em>(<em>z</em>)&lt;/span&gt;or &lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em> ∣ <em>x</em>) = ∑<sub><em>z</em></sub><em>p</em>(<em>y</em> ∣ <em>z</em>) <em>p</em>(<em>z</em> ∣ <em>x</em>)&lt;/span&gt;:the conditional probability is a bit more free. You can decompose andmodel it using <span class="math inline"><em>z</em></span>​ as youlike.</p><p>An example of latent conditional model is the mixture densitynetwork, which we use in RL’s imitation learning to deal withmulti-modal situations each requiring a different distribution.</p></li></ul> <h3 id="latent-variable-models-in-general">Latent Variable Models inGeneral</h3> <p>When we use latent variable models, it means we want to<strong>decompose a complicated distribution into several simple / easydistributions</strong>. By <strong>complicated</strong>, we mean it’snot possible to write it in a well-defined distribution. By<strong>simple / easy</strong>, we mean we can write it as awell-defined parametrized distribution, where the parameters can becomplex, but the distribution itself is easy to write (a Gaussian ofjust mean and sigma, or as a Bernoulli with just one variable, etc.)&lt;spanclass="math display"&gt;<em>p</em>(<em>x</em>) = ∫<em>p</em>(<em>x</em> ∣ <em>z</em>)<em>p</em>(<em>z</em>)<em>d</em><em>z</em>&lt;/span&gt;</p> <ul><li><span class="math inline"><em>p</em>(<em>z</em>)</span> is an “easy”prior we choose. For example a Gaussian, a categorical distribution,etc.</li><li><span class="math inline"><em>p</em>(<em>x</em> ∣ <em>z</em>)</span>should also be an easy distribution, like a Gaussian: $ p(x z) =(<em>{nn}(z), </em>{nn}(z))$ even though the mapping from &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; to the actual parameters ofGaussian can be complex, where in this case we have to model the mappingthrough a neural network and this mapping is the learnable part.</li><li><span class="math inline"><em>p</em>(<em>x</em>)</span> iscomplicated, not possible to write out as any well-defined distribution.Therefore, we decompose it into the two parts above that are<strong>easy to parametrize as a probability distribution and learn theparameters inside the distribution</strong>.</li></ul> <p>Generative models are not equal to latent variable models. We usuallymodel generative models as latent variable ones because generativemodels are usually complex probability distributions and we can make iteasier by introducing one or more latent variable.</p> <h3 id="how-to-train-a-latent-variable-model">How to Train a LatentVariable Model</h3> <p>Given dataset &lt;spanclass="math inline"&gt;𝒟 = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>N</em></sub>}&lt;/span&gt;,to fit a typical probabilistic model &lt;spanclass="math inline"&gt;<em>p</em><sub><em>θ</em></sub>(<em>x</em>)&lt;/span&gt;,we use the maximum likelihood estimation: <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N \sum_i \logp_\theta(x_i)$$</span> In the latent variable model set up, we can substitute thedefinition in and an MLE would look like <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N\sum_i \log \left( \int p_\theta(x_i \mid z) p(z) dz \right)$$</span> &lt;spanclass="math inline"&gt;<em>p</em><sub><em>θ</em></sub>(<em>x</em> ∣ <em>z</em>)&lt;/span&gt;and <span class="math inline"><em>p</em>(<em>z</em>)</span> aredistributions of our choices, but this integral is still intractablewhen <span class="math inline"><em>z</em></span> is continuous. So nowit’s time to do some math tricks.</p> <h2 id="variational-inference">Variational Inference</h2> <h3 id="variational-approximation">Variational Approximation</h3> <p>First, we construct an easy / simple probability distribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to approximate &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;- the posterior distribution specific to datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;. By easy weagain mean it is easy to parametrize (a Gaussian, a Bernoulli, etc.)</p> <p>We will show that by introducing this &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;,we can actually construct a lower bound of &lt;spanclass="math inline"&gt;log <em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.What’s good with this lower bound? Later on, we will also prove thisbound is sufficiently tight, so as we push up the value of this lowerbound, we push up the value of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;which is exactly what we want.</p> <p><span class="math display">$$\begin{align}\log p(x_{i})&amp;= \log\int_{z}p(x_{i}|z)p(z)\\&amp;= \log\int_{z}p(x_{i}|z)p(z) \frac{q_i(z)}{q_i(z)} \\&amp;= \log \mathbb E_{z\sim q_{i}(z)} \left[\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] \\&amp;\geq \mathbb E_{z\sim q_{i}(z)}\left[\log\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] &amp;\text{\# Jensen'sInequality} \\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] - \mathbb E_{z\sim q_{i}(z)} \left[ \log {q_{i}(z)}\right]\\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)= \mathcal L_i(p, q_i)\end{align}$$</span> Recall <span class="math inline"><em>p</em>(<em>x</em>)</span>is a difficult probability distribution, so we decompose it into twoeasy distributions &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, and use an easydistribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to approximate the posterior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.Now the good thing is: everything here is tractable: for the first term,we can fix a &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;of our choice (recall &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; is adistribution we constructed), sample some &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;, and evaluate the expression. Forthe second term, we notice it is just the entropy of a distribution andfor simple distributions (we constructed &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to besimple), it has a closed form (even if it doesn’t, you can simply sampleand evaluate)</p> <p>We call the final lower bound we derived here the <strong>variancelower bound</strong> or <strong>evidence lower bound (ELBO)</strong>.<span class="math display">$$\begin{align}\log p(x_{i})&amp;\geq \mathcal L_i(p, q_i) \\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)\end{align}$$</span> ### Effect of Pushing Up ELBO (Intuitively)</p> <p>Assume our <span class="math inline"><em>p</em>(⋅)</span>​ is a fixedvalue, what does pushing up ELBO mean? Here, we give out an intuitiveexplanation. First, we look at <strong>the first term</strong> with thetwo log combined. <span class="math display">$$\begin{align} &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] \\= &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i},z) \right]\end{align}$$</span> To maximize this value, we just have to find a distribution of<span class="math inline"><em>z</em></span>, inside which we have thelargest value of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)&lt;/span&gt;.Therefore, we want <span class="math inline"><em>z</em></span> todistribute mostly under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)&lt;/span&gt;,Since &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;is the distribution we currently have for z, we want &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to sit mostly under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)&lt;/span&gt;.In the following graph, the y-axis is &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)&lt;/span&gt;,the distribution we try to maximize, and the x-axis is our latentvariable z. There is also a hidden axis - the probability mass(distribution) of <span class="math inline"><em>z</em></span>. Weproject this hidden axis to the y-axis in this graph. To maximize thisfirst term, we spread <span class="math inline"><em>z</em></span>’s massas much under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)&lt;/span&gt;as possible, which makes the green part of this graph.</p> <figure><img src="/images/p(xz)-with-entropy.png" alt="maximize ELBO"/><figcaption aria-hidden="true">maximize ELBO</figcaption></figure> <p>Now we take <strong>the second term entropy</strong> intoconsideration. &lt;spanclass="math display"&gt;ℒ<sub><em>i</em></sub>(<em>p</em>, <em>q</em><sub><em>i</em></sub>) = 𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[log <em>p</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)] + ℋ<sub><em>z</em> ∼ <em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>(<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;From our <a href="link!"><strong><em>entropy post</em></strong></a>, weknow entropy measures the expected code length of communicating theevent described by a random variable. So the more random this variableis, the more code words it’s required to communicate it. Therefore, themore spread out / uniform the distribution is, the higher the entropy.If we’re maxing the entropy, we don’t want the distribution to beskinny. See the following graph.</p> <figure><img src="/images/entropy-example.png" alt="entropy-example"/><figcaption aria-hidden="true">entropy-example</figcaption></figure> <p>When we consider both entropy and the first term, we should achievethis probability distribution depicted in brown. If we don’t have theentropy, <span class="math inline"><em>z</em></span> will want to situnder the most likely point, but since we added entropy, &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; now tries to cover it. Inconclusion, (equal sign “=” reads “in effect”) maximize evidence lowerbound = cover most of the &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;distribution = maximize approximation between &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;.</p> <figure><img src="/images/p(xz)-with-entropy.png" alt="maximize ELBO"/><figcaption aria-hidden="true">maximize ELBO</figcaption></figure> <h3 id="effect-of-pushing-up-elbo-analytically">Effect of Pushing UpELBO (Analytically)</h3> <p>Can we measure how good our approximation is? That is, can we measurethe distance between &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;and <span class="math inline"><em>q</em><sub><em>i</em></sub></span>? Infact, we have a nice, analytical way to look at it using <strong>KLdivergence</strong>. For two arbitrary distribution &lt;spanclass="math inline"&gt;<em>p</em>, <em>q</em>&lt;/span&gt; of &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt;, the KL divergence of &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; from &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt; (the distance from &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; to &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;, note KL divergence is notsymmetric) is</p> <p><span class="math display">$$\begin{align}D_{\mathrm{KL}}(q|p)&amp;=E_{x\sim q(x)}\left[\log{\frac{q(x)}{p(x)}}\right]\\&amp;=E_{x \sim q(x)}[\log q(x)]-E_{x \sim q(x)}[\log p(x)]\\&amp;=-E_{x \sim q(x)}[\log p(x)]-H(q)\end{align}$$</span> Doesn’t this look similar to our evidence lower bound?Borrowing that explanation, minimizing KL divergence = cover most of the<span class="math inline"><em>p</em>(<em>z</em>)</span> distribution =maximize approximation between &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;.</p> <figure><img src="/images/KL-divergence.png" alt="KL-divergence"/><figcaption aria-hidden="true">KL-divergence</figcaption></figure> <p>Having understood the definition of KL divergence, let’s use it tomeasure the distance between &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;- the distribution we want &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; toapproximate: <span class="math display">$$\begin{align}D_{KL}(q_{i}(z)\|p(z \mid x_{i}))&amp;= E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)}{p(z|x_{i})}}\right]\\&amp;= E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)p(x_{i})}{p(x_{i},z)}}\right]\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +E_{z\sim q_{i}(z)}\log q_i(z) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +\mathcal H(q_i) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -\mathcal L(p, q_i) + \log p(x_i)\\\log p(x_i) &amp;= \mathcal L(p, q_i) + D_{KL}(q_{i}(x_{i})\|p(z \midx_{i}))\end{align}$$</span> Therefore, having a good approximation of &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;= driving KL divergence, which is always a non-negative number, to 0 =the evidence lower bound is a tight bound or even equal to &lt;spanclass="math inline"&gt;log <em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;​- the ultimate thing we want to optimize.</p> <p>Looking at our optimization objective &lt;spanclass="math inline"&gt;ℒ&lt;/span&gt; here: &lt;spanclass="math display"&gt;ℒ(<em>p</em>, <em>q</em><sub><em>i</em></sub>) = log <em>p</em>(<em>x</em><sub><em>i</em></sub>) − <em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub>)∥<em>p</em>(<em>z</em> ∣ <em>x</em><sub><em>i</em></sub>))&lt;/span&gt;</p> <ul><li><p>When we optimize w.r.t. &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt;: note the first term &lt;spanclass="math inline"&gt;log <em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;is independent of <span class="math inline"><em>q</em></span>, so itsvalue stays the same. We are in effect optimizing against the KLdivergence only, making the distance between our approximation &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;smaller. The best / extreme case is we have &lt;spanclass="math inline"&gt;<em>D</em><sub><em>K</em><em>L</em></sub> = 0&lt;/span&gt;,so &lt;spanclass="math inline"&gt;ℒ = log <em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.</p></li><li><p>When we optimize w.r.t. &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;: Recall our ultimate goal is tomake &lt;spanclass="math inline"&gt;log <em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;bigger, so we make a better model in theory. Only in theory because wedon’t know whether the bound is tight or not.</p></li></ul> <h3 id="the-learning-algorithm">The Learning Algorithm?</h3> <p>Therefore, when we optimize &lt;spanclass="math inline"&gt;ℒ(<em>p</em>, <em>q</em><sub><em>i</em></sub>)&lt;/span&gt;​w.r.t. <span class="math inline"><em>q</em></span>​, we make the boundtighter (make <span class="math inline">ℒ</span>​ a better approximationof &lt;spanclass="math inline"&gt;log <em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;​); when we optimize &lt;spanclass="math inline"&gt;ℒ(<em>p</em>, <em>q</em><sub><em>i</em></sub>)&lt;/span&gt;​w.r.t. <span class="math inline"><em>p</em></span>​, we make a bettermodel in general.</p> <p>By alternating these two steps, we have <strong>the actual learningalgorithm</strong>. Let’s review: which parts are learnable in these twodistributions?</p> <ul><li><p>In our <a href="#Latent-Variable-Models-in-General">latentvariable model setup</a>, we decompose the complicated distribution<span class="math inline"><em>p</em>(<em>x</em>)</span> into two easydistributions &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, where the mappingfrom <span class="math inline"><em>z</em></span> to actual parameters ofthis <span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>distribution needs to be modeled by a complex network. Therefore, theonly distribution in the <span class="math inline"><em>p</em></span>part with learnable parameters is &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt;. We denoteit with &lt;spanclass="math inline"&gt;<em>p</em><sub><em>θ</em></sub>(<em>x</em>|<em>z</em>)&lt;/span&gt;.</p></li><li><p>In our <a href="#Variational-Approximation">ELBO setup</a>, wealso introduced a simple &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;for each datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt; toapproximate the posterior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.To optimize w.r.t. <span class="math inline"><em>q</em></span>, weoptimize the parameters of each distribution. If &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>) = 𝒩(<em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)&lt;/span&gt;,we optimize each &lt;spanclass="math inline"&gt;<em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>&lt;/span&gt;.(<em>we can optimize the entropy value for sure, but I’m not entirelysure how you would take gradient of the expectation term &lt;spanclass="math inline"&gt;<em>E</em><sub><em>z</em> ∼ <em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[log <em>p</em>(<em>z</em>)]&lt;/span&gt;</em>)</p></li></ul> <p>Therefore, we have our learning algorithm: &lt;spanclass="math display"&gt;$$\begin{align}&amp;\text{for each $x_i$ in $\{x_1, \dots, x_N\}$: }\\&amp;\hspace{4mm} \text{sample $z \sim q_i(z)$}\\&amp;\hspace{4mm} \text{optimize against $p$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_\theta \mathcal L (p_\theta, q_i)= \nabla_\theta \log p_\theta(x_i|z) \\&amp;\hspace{4mm} \hspace{4mm} \theta \leftarrow \theta + \alpha\nabla_\theta \mathcal L (p, q_i) \\&amp;\hspace{4mm} \text{optimize against $q$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_{\mu_i, \sigma_i} \mathcal L(p_\theta, q_i) = \nabla_{\mu_i, \sigma_i} \left[\mathbb E_{z\simq_{i}(z)} \left[\log p(x_{i}|z)+\log p(z) \right] + \mathcal H_{z\simq_{i}(z)} (q_i) \right] \\&amp;\hspace{4mm} \hspace{4mm} (\mu_i, \sigma_i) \leftarrow (\mu_i,\sigma_i) + \alpha \nabla_{\mu_i, \sigma_i} \mathcal L (p, q_i) \\\end{align}$$&lt;/span&gt;</p> <p>There’s a problem with optimizing &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; though. Notewe have a separate <span class="math inline"><em>q</em></span> for eachdata point <span class="math inline"><em>i</em></span>, which means ifwe have <span class="math inline"><em>N</em></span> data points, we willhave to store &lt;spanclass="math inline"&gt;<em>N</em> × (|<em>μ</em><sub><em>i</em></sub>| + |<em>σ</em><sub><em>i</em></sub>|)&lt;/span&gt;parameters assuming we chose &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to beGaussian. In machine learning, the number of data points &lt;spanclass="math inline"&gt;<em>N</em>&lt;/span&gt; is usually in millions, makingthis model unwieldily big. It’s true in inference time we do not use<span class="math inline"><em>q</em></span> at all (we’ll see why thisis true in the last chapter about VAE), but in training time, we stillneed them so it’s necessary to keep all these parameters.</p> <p>Therefore, instead of having a separate &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(⋅)&lt;/span&gt; toapproximate each data point’s &lt;spanclass="math inline"&gt;<em>P</em>(⋅|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;specifically, we use a learnable model &lt;spanclass="math inline"&gt;<em>q</em><sub><em>ϕ</em></sub>(⋅|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;to approximate &lt;spanclass="math inline"&gt;<em>p</em>(⋅|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;This learnable network will take in a datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;, predicts thecorresponding &lt;spanclass="math inline"&gt;<em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>&lt;/span&gt;.We can then sample <span class="math inline"><em>z</em></span>​ from thispredicted network.</p> <h2 id="amortized">Amortized</h2> <p>By adapting <span class="math inline"><em>q</em></span> to be alearnable network &lt;spanclass="math inline"&gt;<em>q</em><sub><em>ϕ</em></sub>&lt;/span&gt;​ instead,model size does not depend on the number of datapoints anymore.Therefore, it is <strong>amortized</strong>.</p> <p>The variational lower bound becomes: &lt;spanclass="math display"&gt;ℒ(<em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>), <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)) = 𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>) + log <em>p</em>(<em>z</em>)] + ℋ(<em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>))&lt;/span&gt;The learning algorithm naturally becomes: $$ $$</p> <h3 id="gradient-over-expectation-policy-gradient">Gradient OverExpectation (Policy Gradient)</h3> <p>The question now boils down to how do we calculate this gradient&lt;spanclass="math inline"&gt;∇<sub><em>ϕ</em></sub>ℒ(<em>p</em><sub><em>θ</em></sub>, <em>q</em><sub><em>ϕ</em></sub>)&lt;/span&gt;.</p> <p>The second term entropy is easy. We purposefully chose &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; to be a simple distribution, sothere is usually a close form of its entropy and we just have to look itup.</p> <p>The meat is in the first part. How do we take gradient w.r.t.parameter <span class="math inline"><em>ϕ</em></span> in the expectationterm’s distribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>ϕ</em></sub>&lt;/span&gt; ? Note theterm inside expectation is independent of &lt;spanclass="math inline"&gt;<em>ϕ</em>&lt;/span&gt;, so we can rewrite it as &lt;spanclass="math inline"&gt;<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>) = log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>) + log <em>p</em>(<em>z</em>)&lt;/span&gt;and call the whole thing &lt;spanclass="math inline"&gt;<em>J</em>&lt;/span&gt;.<br/>&lt;spanclass="math display"&gt;<em>J</em>(<em>ϕ</em>) = ∇<sub><em>ϕ</em></sub>𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)]&lt;/span&gt;We chose these namings purposefully because we encountered somethingsimilar back in the &lt;ahref="https://slides.com/sarahdean-2/sp24-cs-4789-lec-16?token=KNeurk-c#/11/0/4"&gt;policygradient part of reinforcement learning <strong>LINK???</strong>&lt;/a&gt;.Say we have a trajectory <span class="math inline"><em>τ</em></span>,sampled from the state transition function with learnable policy &lt;spanclass="math inline"&gt;<em>π</em><sub><em>θ</em></sub>&lt;/span&gt;, the finalexpected value we can get from starting state &lt;spanclass="math inline"&gt;<em>s</em><sub>0</sub>&lt;/span&gt; can be written as thefollowing, where <span class="math inline"><em>R</em>(<em>τ</em>)</span>is a reward function returning the reward of this trajectory. &lt;spanclass="math display"&gt;<em>J</em>(<em>θ</em>) = <em>V</em><sup><em>π</em><sub><em>θ</em></sub></sup>(<em>s</em><sub>0</sub>) = 𝔼<sub><em>τ</em> ∼ <em>P</em><sub><em>s</em><sub>0</sub></sub><sup><em>π</em><sub><em>θ</em></sub></sup></sub>[<em>R</em>(<em>τ</em>)]&lt;/span&gt;We can take the gradient of this value function &lt;spanclass="math inline"&gt;<em>V</em>&lt;/span&gt; w.r.t our policy &lt;spanclass="math inline"&gt;<em>π</em><sub><em>θ</em></sub>&lt;/span&gt;, so this iscalled policy gradient. If you’re unfamiliar with RL setup, you justhave to know we can derive the following gradient and we can approximateit by sampling <span class="math inline"><em>M</em></span> trajectories.$$ <span class="math display">$$Pugging in our $q$ and $\phi$,$$</span> $$</p> <h3 id="reparametrization-trick">Reparametrization Trick</h3> <p>We have our full learning algorithm and it’s ready to go now.However, there is a tiny improvement we can do.</p> <p>We defined our &lt;spanclass="math inline"&gt;<em>q</em><sub><em>ϕ</em></sub>&lt;/span&gt; to be anormal distribution &lt;spanclass="math inline"&gt;𝒩(<em>μ</em><sub><em>ϕ</em></sub>, <em>σ</em><sub><em>ϕ</em></sub>)&lt;/span&gt;Observe that all normal distributions can be written as a function ofthe unit normal distribution. Therefore, a sample &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; is in effect: &lt;spanclass="math display"&gt;<em>z</em> ∼ 𝒩(<em>μ</em><sub><em>ϕ</em></sub>, <em>σ</em><sub><em>ϕ</em></sub>) ⇔ <em>z</em> = <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>, <em>ϵ</em> ∼ 𝒩(0, 1)&lt;/span&gt;Let’s rewrite our expectation term to now sample an &lt;spanclass="math inline"&gt;<em>ϵ</em>&lt;/span&gt; from the unit normal distributioninstead. By decomposing <span class="math inline"><em>z</em></span> intothese two parts, we separate the stochastic part and changed &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; from a sample of some stochasticdistribution into a deterministic function &lt;spanclass="math inline"&gt;<em>z</em>(<em>ϕ</em>, <em>ϵ</em>)&lt;/span&gt;parametrized by <span class="math inline"><em>ϕ</em></span> and randomvariable <span class="math inline"><em>ϵ</em></span> that is independentof <span class="math inline"><em>ϕ</em></span>. &lt;spanclass="math inline"&gt;<em>ϵ</em>&lt;/span&gt; takes the stochastic part alone.Our learnable parameter <span class="math inline"><em>ϕ</em></span> nowonly parametrizes deterministic quantity. &lt;spanclass="math display"&gt;∇<sub><em>ϕ</em></sub><em>J</em>(<em>ϕ</em>) = ∇<sub><em>ϕ</em></sub>𝔼<sub><em>ϵ</em> ∼ 𝒩(0, 1)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>)]&lt;/span&gt;Aside from these theoretical benefits, mathematically, we do not have totake gradient w.r.t an expectation of parametrized distribution anymore.Instead, the gradient can go straight into the expectation term now likehow we usually interchange gradient and expectation (think aboutdiscrete case, expectation is just a big sum so we can do it). &lt;spanclass="math display"&gt;∇<sub><em>ϕ</em></sub><em>J</em>(<em>ϕ</em>) = 𝔼<sub><em>ϵ</em> ∼ 𝒩(0, 1)</sub>[∇<sub><em>ϕ</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>)]&lt;/span&gt;Further, to approximate this expectation, we just sample some &lt;spanclass="math inline"&gt;<em>ϵ</em>&lt;/span&gt; from this normal distribution.<span class="math display">$$\nabla_\phi J(\phi)\approx \frac 1 M \sum_j^M \nabla_\phi R(x_i, \mu_\phi + \epsilon_j\sigma_\phi)$$</span></p> <p>With reparametrization, we achieve a lower variance than policygradient because we are using the derivative of R. (<em>Unfortunatelythe lecturer didn’t provide a quantitative analysis on this and I don’tknow how to prove it</em>) On the other hand, previously, we only tookderivative w.r.t. the probability distribution. Why didn’t we usederivative of R back in RL with policy gradient? It’s not we don’t wantto but we can’t: we can’t use reparametrization in RL because in RL weusually cannot take derivative w.r.t. reward R.</p> <table><colgroup><col style="width: 6%"/><col style="width: 23%"/><col style="width: 23%"/><col style="width: 23%"/><col style="width: 23%"/></colgroup><thead><tr class="header"><th>Method</th><th>Formula</th><th>Approximation</th><th>Benefit</th><th>Deficit</th></tr></thead><tbody><tr class="odd"><td>Policy Gradient</td><td>&lt;spanclass="math inline"&gt;∇<sub><em>ϕ</em></sub>𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em> ∣ <em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>z</em>)]&lt;/span&gt;</td><td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi[\logq_\phi(z_j \mid x_i)] R(x_i,z_j)$</span></td><td>works with both discrete and continuous latent variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;</td><td>High variance, requires multiple samples &amp; small learningrates</td></tr><tr class="even"><td>Reparametrization</td><td>&lt;spanclass="math inline"&gt;𝔼<sub><em>ϵ</em> ∼ 𝒩(0, 1)</sub>[∇<sub><em>ϕ</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>, <em>μ</em><sub><em>ϕ</em></sub> + <em>ϵ</em><em>σ</em><sub><em>ϕ</em></sub>)]&lt;/span&gt;</td><td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi R(x_i,\mu_\phi + \epsilon_j \sigma_\phi)$</span></td><td>low variance, simple to implement (we’ll see soon)</td><td>only works with continuous variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; and have to model it with aGaussian</td></tr></tbody></table> <p>In fact, you can forget about the policy gradient method and simplytake it for granted that you cannot back propagate a sampled value &lt;spanclass="math inline"&gt;∇<sub><em>ϕ</em></sub>𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>&lt;/span&gt;,so you have to find some way to make our &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;​ deterministic, which is what we’redoing here with our reparametrization trick.</p> <figure>&lt;img src="/images/reparametrization-trick.png"alt="reparametrization-trick" /&gt;<figcaption aria-hidden="true">reparametrization-trick</figcaption></figure> <p>Left is without the “reparameterization trick”, and right is with it.Red shows sampling operations that are non-differentiable. Blue showsloss layers. We forward the network by going up and back propagate it bygoing down. The forward behavior of these networks is identical, butback propagation can be applied only to the right network. Figure copiedfrom <a href="https://arxiv.org/abs/1606.05908">Carl Doersch: Tutorialon Variational Autoencoders</a></p> <h3 id="looking-at-mathcal-l-directly">Looking at &lt;spanclass="math inline"&gt;ℒ&lt;/span&gt; Directly</h3> <p><span class="math display">$$\begin{align}\mathcal L_i = \mathcal L \left( p_\theta(x_i | z), q_\phi(z | x_i)\right)&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)+\log p(z) \right] + \mathcal H (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p_\theta(x_{i}|z)\right] + \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p(z) \right] + \mathcalH (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)\right] - D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;= \mathbb E_{\epsilon \sim \mathcal N(0,1)} \left[\logp_\theta(x_{i}| \mu_\phi + \epsilon \sigma_\phi)\right] -D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;\approx \frac 1 M \sum_j^M \log p_\theta(x_{i}| \mu_\phi +\epsilon_j \sigma_\phi) - D_{KL}(q_\phi(z | x_i)\|p(z)) \\\end{align}$$</span></p> <p>For the first term, we can just evaluate it. For the second KL term,since we chose both distributions to be easy (in this case Gaussian),there often is a nice analytical form for it.</p> <p>Therefore, we can go ahead to maximize the variational lower bound<span class="math inline">ℒ</span>​. We can also draw out the followingcomputational graph for the log term and conclude we can back propagatethis graph without any problem. On the other hand, if we didn’t do thereparametrization trick, we will get stuck at &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;: you cannot back propagate &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; - a sampled value instead of avariable. And we will have to seek help from policy gradient. Withreparametrization, we decompose &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; into two variables &lt;spanclass="math inline"&gt;<em>μ</em><sub><em>ϕ</em></sub>, <em>σ</em><sub><em>ϕ</em></sub>&lt;/span&gt;we can back propagate through and one stochastic value &lt;spanclass="math inline"&gt;<em>ϵ</em>&lt;/span&gt; we do not care about.</p> <figure><img src="/images/computational-graph.png" alt="computational-graph"/><figcaption aria-hidden="true">computational-graph</figcaption></figure> <h2 id="variational-autoencoder">Variational Autoencoder</h2> <h3 id="setup-and-interpretation">Setup and Interpretation</h3> <p>What we have gone though constitutes the full pipeline of avariational autoencoder.</p> <p>In a variation autoencoder, we have observed variable &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; and latent variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;</p> <ul><li>encoder &lt;spanclass="math inline"&gt;<em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em>) = 𝒩(<em>μ</em><sub><em>ϕ</em></sub>(<em>x</em>), <em>σ</em><sub><em>ϕ</em></sub>(<em>x</em>))&lt;/span&gt;</li><li>decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>θ</em></sub>(<em>x</em>|<em>z</em>) = 𝒩(<em>μ</em><sub><em>θ</em></sub>(<em>z</em>), <em>σ</em><sub><em>θ</em></sub>(<em>z</em>))&lt;/span&gt;</li></ul> <p>In training, given an observed sample &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;, we encode itto latent variable &lt;spanclass="math inline"&gt;<em>z</em><sub><em>i</em></sub>&lt;/span&gt; using &lt;spanclass="math inline"&gt;<em>q</em><sub><em>ϕ</em></sub>&lt;/span&gt;, then triesto decode it back with decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>θ</em></sub>&lt;/span&gt;. We maximizethe variational lower bound during the process. For all &lt;spanclass="math inline"&gt;<em>N</em>&lt;/span&gt; samples, the training objectivelooks like: (where the <span class="math inline"><em>ϵ</em></span> is asampled value) <span class="math display">$$\max_{\phi,\theta} \frac 1 N \sum_i^N \log p_\theta\left(x_{i}|\mu_\phi(x_i) + \epsilon \sigma_\phi(x_i)\right) - D_{KL}(q_\phi(z |x_i)\|p(z)) \\$$</span> In inference (generation), we sample a &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; from our prior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, then decode it using<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>: &lt;spanclass="math inline"&gt;<em>z</em> ∼ <em>p</em>(<em>z</em>), <em>x</em> ∼ <em>p</em><sub><em>θ</em></sub>(<em>x</em>|<em>z</em>)&lt;/span&gt;</p> <p>Why does the variational autoencoder work? We talked about manybenefits of maximizing this variational lower bound in &lt;ahref="#Effect-of-Pushing-Up-ELBO-(Analytically)"&gt;previous chapter&lt;/a&gt;.Let’s look at it again in this decoder-encoder setup,. &lt;spanclass="math display"&gt;ℒ<sub><em>i</em></sub> = 𝔼<sub><em>z</em> ∼ <em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)] − <em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>ϕ</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)∥<em>p</em>(<em>z</em>))&lt;/span&gt;</p> <ul><li>The first &lt;spanclass="math inline"&gt;log <em>p</em><sub><em>θ</em></sub>&lt;/span&gt; termmaximizes the probability of our observed image &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; given a sample &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;, so the model makes decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>θ</em></sub>&lt;/span&gt; toreconstruct image <span class="math inline"><em>x</em></span>​ asaccurate as possible.</li><li>The second KL term restricts the encoding of an image to be close tothe actual prior, which makes sure at inference / generate time, we candirectly sample from the prior.</li></ul> <h3 id="comparison-with-auto-encoder">Comparison with Auto-Encoder</h3> <figure><img src="/images/vae-and-ae.png" alt="vae-and-ae"/><figcaption aria-hidden="true">vae-and-ae</figcaption></figure> <p>The VAE’s decoder is trained to convert random points in theembedding space (generated by perturbing the input encodings) tosensible outputs. By contrast, the decoder for the deterministicautoencoder only ever gets as inputs the exact encodings of the trainingset, so it does not know what to do with random inputs that are outsidewhat it was trained on. So a standard autoencoder cannot create newsamples.</p> <p>The reason the VAE is better at sample is that it embeds images intoGaussians in latent space, whereas the AE embeds images into points,which are like delta functions. The advantage of using a latentdistribution is that it encourages local smoothness, since a given imagemay map to multiple nearby places, depending on the stochastic sampling.By contrast, in an AE, the latent space is typically not smooth, soimages from different classes often end up next to each other. Figurecopied from &lt;ahref="https://probml.github.io/pml-book/book1.html"&gt;ProbabilisticMachine Learning: An Introduction - Figure 20.26&lt;/a&gt;</p> <p>We can leverage the smoothness of the latent space to perform imageinterpolation in latent space.</p> <h2 id="reference">Reference</h2> <p>Most content of this blog post comes from &lt;ahref="https://www.youtube.com/watch?v=UTMpM4orS30"&gt;Berkeley CS 285(Sergey Levine): Lecture 18, Variational Inference&lt;/a&gt;, which I thinkorganized his lecture based on &lt;ahref="https://arxiv.org/abs/1906.02691"&gt;An Introduction to VariationalAutoencoders&lt;/a&gt; (2.1-2.7, and 2.9.1), or more in-depth on the author’sPhD thesis <a href="http://dpkingma.com/#phdthesis">VariationalInference and Deep Learning: A New Synthesis</a> I found this wonderfultutorial in &lt;ahref="https://probml.github.io/pml-book/book2.html"&gt;ProbabilisticMachine Learning: Advanced Topics&lt;/a&gt;</p> <p>Some graph come from &lt;ahref="https://probml.github.io/pml-book/book1.html"&gt;ProbabilisticMachine Learning: An Introduction&lt;/a&gt; itself and &lt;ahref="https://arxiv.org/abs/1606.05908"&gt;Carl Doersch: Tutorial onVariational Autoencoders&lt;/a&gt;, which is referenced in the previousbook.</p> <p>Note though the <em>Probabilistic Machine Learning</em> book itselfis a horrible book with extremely confusing explanations.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Probabilistic Latent Variable Models The two general forms of probabilistic models are]]></summary></entry><entry><title type="html">Hyper-Parameter Tuning with Optuna</title><link href="https://yao-lirong.github.io/2024/08/23/hyper-parameter-tuning-with-optuna.html" rel="alternate" type="text/html" title="Hyper-Parameter Tuning with Optuna"/><published>2024-08-23T04:00:00+00:00</published><updated>2024-08-23T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/08/23/hyper-parameter-tuning-with-optuna</id><content type="html" xml:base="https://yao-lirong.github.io/2024/08/23/hyper-parameter-tuning-with-optuna.html"><![CDATA[<p>After self-implementing a grid-search but having a horrible timewriting pyplot visualizing the result, I finally decided to find anexisting tool to do the HP tuning for me.</p> <p><span id="more"></span>&lt;p&gt;There are two popular HP tuning framework&lt;/p&gt;&lt;ul&gt;&lt;li&gt;<a href="https://docs.ray.io/en/latest/tune/index.html">RayTune</a>:almost industry standard&lt;/li&gt;&lt;li&gt;<a href="https://optuna.org/">Optuna</a>: user friendly, requiresminimal modification to original code&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There’s also &lt;ahref=”https://github.com/skorch-dev/skorch”&gt;skorch&lt;/a&gt; integratingscikit-learn and pytorch, so you can use &lt;ahref=”https://skorch.readthedocs.io/en/v1.0.0/user/quickstart.html#grid-search”&gt;sklearn<code>GridSearchCV</code>&lt;/a&gt;. For our simple task, we will go with<code>Optuna</code>.&lt;/p&gt;&lt;h2 id="getting-started"&gt;Getting Started&lt;/h2&gt;&lt;p&gt;To get Optuna running, you just need to add 4 lines in your traininglogic and a few more lines to start its search. In training logic:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">image_datasets, lr, weight_decay, num_epochs, trial : optuna.trial.Trial=<span class="literal">None</span></span>):</span><br/><span class="line"> optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)</span><br/><span class="line"> <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br/><span class="line"> model.train()</span><br/><span class="line"> <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">"train"</span>]:</span><br/><span class="line"><span class="comment"># Training Logic</span></span><br/><span class="line"> model.<span class="built_in">eval</span>()</span><br/><span class="line"> <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">"val"</span>]:</span><br/><span class="line"> running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br/><span class="line"> <span class="comment"># Eval Logic</span></span><br/><span class="line"> epoch_loss = running_loss / dataset_sizes[<span class="string">"val"</span>]</span><br/><span class="line"> <span class="keyword">if</span> epoch_acc &gt; best_acc <span class="keyword">or</span> (epoch_acc == best_acc <span class="keyword">and</span> epoch_loss &lt; best_loss):</span><br/><span class="line"> best_acc, best_loss = epoch_acc, epoch_loss</span><br/><span class="line"> <span class="string">""" OPTUNA CODE GOES HERE:</span></span><br/><span class="line"><span class="string"> For each epoch, you should report value of a user-defined factor. </span></span><br/><span class="line"><span class="string"> Optuna uses this factor alone to determine whether to prune out </span></span><br/><span class="line"><span class="string"> this trial at current epoch step. Your objective value returned </span></span><br/><span class="line"><span class="string"> has nothing to do with pruning.</span></span><br/><span class="line"><span class="string"> Read for more at: https://optuna.readthedocs.io/en/v3.6.1/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.report</span></span><br/><span class="line"><span class="string"> """</span></span><br/><span class="line"> <span class="keyword">if</span> trial <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br/><span class="line"> trial.report(epoch_loss, epoch)</span><br/><span class="line"> <span class="keyword">if</span> trial.should_prune():</span><br/><span class="line"> <span class="keyword">raise</span> optuna.exceptions.TrialPruned()</span><br/><span class="line"> <span class="keyword">return</span> best_loss</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The following code shows how to set the search space and start thesearch.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">optuna_objective</span>(<span class="params">trial : optuna.trial.Trial</span>):</span><br/><span class="line"> <span class="string">""" Define a custom objective function we want to optimize on. </span></span><br/><span class="line"><span class="string"> This function returns value of the criteria you want to finally evaluate your model on. </span></span><br/><span class="line"><span class="string"> i.e. how you compare different models. The best model should have the best value of this objective.</span></span><br/><span class="line"><span class="string"> If you say the best model should have highest training accuracy at the last epoch, then return training accuracy at the last epoch here. In our example, we think the best model should have the best <code class="language-plaintext highlighter-rouge">best_loss</code>, where a model's <code class="language-plaintext highlighter-rouge">best_loss</code> is this model's lowest validation loss across all epochs.</span></span><br/><span class="line"><span class="string"> """</span></span><br/><span class="line"> image_datasets = prepare_data()</span><br/><span class="line"> lr = trial.suggest_float(<span class="string">"lr"</span>, <span class="number">1e-6</span>, <span class="number">1e-1</span>, log=<span class="literal">True</span>)</span><br/><span class="line"> weight_decay = trial.suggest_float(<span class="string">"weight_decay"</span>, <span class="number">1e-6</span>, <span class="number">1e-1</span>, log=<span class="literal">True</span>)</span><br/><span class="line"> loss = train_model(image_datasets, lr, weight_decay, <span class="number">15</span>, trial)</span><br/><span class="line"> <span class="keyword">return</span> loss</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">if</span> <strong>name</strong> == <span class="string">"<strong>main</strong>"</span>:</span><br/><span class="line"> <span class="string">"""</span></span><br/><span class="line"><span class="string"> Create a study called <code class="language-plaintext highlighter-rouge">plant_144</code> where we minimize the objective passed in.</span></span><br/><span class="line"><span class="string"> Start the search. The search ends when we finish 10 trials or spend 3 hours. </span></span><br/><span class="line"><span class="string"> """</span></span><br/><span class="line"> study = optuna.create_study(</span><br/><span class="line"> direction=<span class="string">"minimize"</span>,</span><br/><span class="line"> study_name=<span class="string">"plant_144"</span>)</span><br/><span class="line"> study.optimize(optuna_objective, n_trials=<span class="number">10</span>, timeout=<span class="number">3</span><em><span class="number">60</span></em><span class="number">60</span>)</span><br/><span class="line"> <span class="built_in">print</span>(<span class="string">" Objective Value: "</span>, study.best_trial.value)</span><br/><span class="line"> <span class="built_in">print</span>(<span class="string">" Params: "</span>)</span><br/><span class="line"> <span class="keyword">for</span> key, value <span class="keyword">in</span> study.best_trial.params.items():</span><br/><span class="line"> <span class="built_in">print</span>(<span class="string">f" <span class="subst">{key}</span>: <span class="subst">{value}</span>"</span>)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The above example was adapted from &lt;ahref=”https://github.com/optuna/optuna-examples/blob/ecc3e4282161f3cece1dc26d95f4186e3905e497/pytorch/pytorch_simple.py”&gt;Optuna’sPyTorch starting example&lt;/a&gt;. For more reporting printout statements,check the original example.&lt;/p&gt;&lt;h2 id="saving-study-and-board-visualization"&gt;Saving Study and BoardVisualization&lt;/h2&gt;&lt;p&gt;In addition to printing out all the info to the console and losingthem from memory after this python script finishes, we can save them inthe form of an RDB (Relational Database, or just database as mostdatabases are RDB). To do this, we pass a <em>database URL</em> to the<code>storage</code> argument&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">study = optuna.create_study(</span><br/><span class="line"> direction=<span class="string">"minimize"</span>,</span><br/><span class="line"> study_name=<span class="string">"plant_144"</span>,</span><br/><span class="line"> storage=<span class="string">"sqlite:///db.sqlite3"</span>)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;You can now Ctrl+C stop this search at anytime and resume it byrunning the same code again.&lt;/p&gt;&lt;p&gt;Database exposes itself as a server in machines. Therefore, to accessit (even in local machine), we use Database URL. Just like to access awebpage online, we use an HTTPS url. In our example here, the historywill be stored in a file called <code>db.sqlite3</code> under currentdirectory.&lt;/p&gt;&lt;p&gt;This file is a general database and can store study other than theone called <code>plant_144</code>. You can store another study insideit.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">study = optuna.create_study(</span><br/><span class="line"> direction=<span class="string">"maximize"</span>,</span><br/><span class="line"> study_name=<span class="string">"plant_8"</span>,</span><br/><span class="line"> storage=<span class="string">"sqlite:///db.sqlite3"</span>)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;For me this code just worked without having to install SQLite DB.This is probably because it comes with my Ubuntu but I have no idea.Check official tutorial &lt;ahref=”https://optuna.readthedocs.io/en/v3.6.1/tutorial/20_recipes/001_rdb.html”&gt;Saving/ResumingStudy&lt;/a&gt; for more on saving and loading.&lt;/p&gt;&lt;p&gt;You can now visualize the search history, each parameter’simportance, etc. with &lt;ahref=”https://github.com/optuna/optuna-dashboard”&gt;optuna-dashboard&lt;/a&gt;&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">optuna-dashboard sqlite:///db.sqlite3</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure><figure><img src="/images/optuna-dashboard.png" alt="optuna-dashboard"/>&lt;figcaption aria-hidden="true"&gt;optuna-dashboard&lt;/figcaption&gt;</figure>&lt;h2 id="multi-gpu-parallelism-support"&gt;Multi-GPU ParallelismSupport&lt;/h2&gt;&lt;p&gt;<a href="https://stackoverflow.com/a/62564488">roman’s Stack Overflowanswer</a> provides a very simple way to do multi-GPU tuning byutilizing Optuna’s resume feature. To do so, create a study by followingthe previous code. Then modify your code now to resume instead ofstarting a new study.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> <strong>name</strong> == <span class="string">'<strong>main</strong>'</span>:</span><br/><span class="line"> study = optuna.load_study(study_name=<span class="string">'plant_144'</span>, storage=<span class="string">'sqlite:///db.sqlite3'</span>)</span><br/><span class="line"> study.optimize(objective, n_trials=<span class="number">100</span>)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;and simply start “resume” this study on different available GPUs&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">CUDA_VISIBLE_DEVICES=3 <span class="built_in">nohup</span> python optuna.py &gt; log3.txt 2&gt;&amp;1 &amp;</span><br/><span class="line">CUDA_VISIBLE_DEVICES=5 <span class="built_in">nohup</span> python optuna.py &gt; log5.txt 2&gt;&amp;1 &amp;</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The history from both processes will be stored under the study called<code>plant_144</code> in file <code>db.sqlite3</code>.&lt;/p&gt;&lt;p&gt;For more information on parallelizing on multiple gpu, check officialguide: &lt;ahref=”https://optuna.readthedocs.io/en/v3.6.1/tutorial/10_key_features/004_distributed.html”&gt;EasyParallelization&lt;/a&gt;&lt;/p&gt;&lt;h2 id="some-complaints"&gt;Some Complaints&lt;/h2&gt;&lt;p&gt;In its visualization, Optuna doesn’t provide an option to filter outthe “bad” trial runs, making the scale of all graph ridiculous andusually of no information.&lt;/p&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[After self-implementing a grid-search but having a horrible time writing pyplot visualizing the result, I finally decided to find an existing tool to do the HP tuning for me.]]></summary></entry><entry><title type="html">KV Cache</title><link href="https://yao-lirong.github.io/2024/07/02/kv-cache.html" rel="alternate" type="text/html" title="KV Cache"/><published>2024-07-02T04:00:00+00:00</published><updated>2024-07-02T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/07/02/kv-cache</id><content type="html" xml:base="https://yao-lirong.github.io/2024/07/02/kv-cache.html"><![CDATA[<p>Before this, see &lt;ahref="#2024/06/17-Conducting-Multi-Round-Conversation-with-Transformers"&gt;2024/06/17Conducting Multi-Round Conversation with Transformers&lt;/a&gt; for why weneed cache. But we have query, key, value three matrices. Why do youonly cache past keys and values? How about past queries?</p> <p><span id="more"></span>&lt;h2 id="attention-mechanism-in-detail"&gt;Attention Mechanism inDetail&lt;/h2&gt;Recall the attention process in transformer can be written in thefollowing matrix form: <span class="math display">\(Z = \text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\)</span> If we look a particular output at position &lt;spanclass=”math inline”&gt;<em>i</em>&lt;/span&gt;, it can be written as: \(z_i =({}&lt;p&gt;)&lt;/p&gt;&lt;pre&gt;&lt;code&gt;\begin&amp;#123;bmatrix&amp;#125;v_1 \\v_2 \\\vdots \\v_n\end&amp;#123;bmatrix&amp;#125;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;\) A simple example can be found in the famous &lt;ahref=”https://jalammar.github.io/illustrated-transformer/”&gt;IllustratedTransformer&lt;/a&gt;&lt;/p&gt;<figure>&lt;img src=”https://jalammar.github.io/images/t/self-attention-output.png”alt=”self attention output” /&gt;&lt;figcaption aria-hidden="true"&gt;self attention output&lt;/figcaption&gt;</figure>&lt;p&gt;From the formula and the example, we can see that key and values arealways a pair in calculation. In fact, this is aligned with the veryconcept of soft dictionary behind attention: we get a query fromsomewhere and look at all the keys in the dictionaries to find, for eachkey, how much it relates to this query and output the weighted averageof each key’s value based on the relatedness.&lt;/p&gt;&lt;h2 id="generative-transformer-decoder-based"&gt;Generative Transformer(Decoder Based)&lt;/h2&gt;<figure>&lt;imgsrc=”https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png”alt=”Autoregressive Decoder” /&gt;&lt;figcaption aria-hidden="true"&gt;Autoregressive Decoder&lt;/figcaption&gt;</figure>&lt;p&gt;Let’s consider a causal language model, aka a transformer’sautoregressive generative decoder. At inference time, <strong>we onlycare about the output at the last position</strong> because the model isautoregressive and the outputs at all the previous positions are exactlythe same as our input. (See the above graph from blogpost &lt;ahref=”https://huggingface.co/blog/encoder-decoder”&gt;Transformers-basedEncoder-Decoder Models&lt;/a&gt;) Therefore, if the current sequence haslength <span class="math inline"><em>s</em></span>, we only care about<span class="math inline"><em>z</em><sub><em>s</em></sub></span>. Allthe other outputs &lt;spanclass=”math inline”&gt;<em>z</em><sub>1…<em>s</em> − 1</sub>&lt;/span&gt; areuseless.&lt;/p&gt;&lt;p&gt;&lt;ahref=”https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L188-L191”&gt;Inferencecode in Karpathy’s nanoGPT&lt;/a&gt; corroborated this in its inference timeimplementation:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br/><span class="line"> <span class="comment"># inference-time mini-optimization: only forward the lm_head on the very last position</span></span><br/><span class="line"> logits = <span class="variable language_">self</span>.lm_head(x[:, [-<span class="number">1</span>], :]) <span class="comment"># note: using list [-1] to preserve the time dim</span></span><br/><span class="line"> loss = <span class="literal">None</span></span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>Now revisit the formula to calculate the output &lt;spanclass=”math inline”&gt;<em>z</em><sub><em>s</em></sub>&lt;/span&gt;: \(z_s =( {}&lt;p&gt;)&lt;/p&gt;&lt;pre&gt;&lt;code&gt;\begin&amp;#123;bmatrix&amp;#125;v_1 \\v_2 \\\vdots \\v_s\end&amp;#123;bmatrix&amp;#125;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;\) It should be clear that to save computation, we only need to cachethe <code>kv</code> values in the previous positions and it’s useless tocache the <code>q</code> value.&lt;/p&gt;&lt;p&gt;To give a more detailed example, let’s consider the whole process togenerate a sequence of tokens with length &lt;spanclass=”math inline”&gt;<em>n</em>&lt;/span&gt;: &lt;spanclass=”math inline”&gt;<em>t</em><sub>1</sub>, …, <em>t</em><sub><em>n</em></sub>&lt;/span&gt;.We can see the previous queries are never used in the computation.<br/>\(\)&lt;/p&gt;&lt;h2 id="time-complexity-boost"&gt;Time Complexity Boost&lt;/h2&gt;&lt;p&gt;People complain about the slow inference time of generativetransformer model, where it has a quadratic sequence length term &lt;spanclass=”math inline”&gt;<em>O</em>(<em>s</em><sup>2</sup>)&lt;/span&gt;. Thisquadratic term is caused by &lt;spanclass=”math inline”&gt;<em>Q</em><em>K</em><sup><em>T</em></sup>&lt;/span&gt;matrix multiplication in attention where both matrices have shape &lt;spanclass=”math inline”&gt;<em>s</em> × <em>d</em>&lt;/span&gt;. Recall running timeof matmul <span class="math inline"><em>A</em><em>B</em></span> where<span class="math inline">$A \in \R^{m \times p}, B \in \R^{p \timesn}$</span> is &lt;spanclass=”math inline”&gt;<em>O</em>(<em>m</em><em>p</em><em>n</em>)&lt;/span&gt;,so this matmul of query and key matrix has time complexity &lt;spanclass=”math inline”&gt;<em>O</em>(<em>s</em><sup>2</sup><em>d</em>)&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;However, by observing that we only need the output at the very lastposition in generative model and utilizing KV-cache, we reduce ourmatrix <span class="math inline">$Q \in \R^{s \times d}$</span> to asingle vector of <span class="math inline">$q \in \R^{1 \timesd}$</span> and effectively reduce the time complexity of this operationto <span class="math inline"><em>O</em>(<em>s</em><em>d</em>)</span>.Therefore, we can eliminate the quadratic term from our inference timeand only need linear time <span class="math inline"><em>s</em></span>instead.&lt;/p&gt;&lt;h2 id="what-about-encoder-based-transformer-model"&gt;What about EncoderBased Transformer Model?&lt;/h2&gt;&lt;p&gt;Encoder Based transformer models do not have the issue of repeatedlycomputing the same past tokens’ attention scores so do not need aKV-cache.&lt;/p&gt;&lt;h2 id="code-implementation"&gt;Code Implementation&lt;/h2&gt;&lt;p&gt;Facebook’s &lt;ahref=”https://github.com/facebookresearch/XLM”&gt;cross-lingual languagemodel (XLM)&lt;/a&gt; gives a fantastic example of how to implement KV-Cache(or transformers in general, it provides abundant comments of tensorshape at each step).&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;&lt;p&gt;At inference time, do not recompute elements (where<code>slen</code> or a more descriptive naming can be<code>cached_sequence_length</code> is how many previous positions havebeen cached): &lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L373-L380”&gt;link&lt;/a&gt;&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br/><span class="line"> _slen = slen - cache[<span class="string">'slen'</span>]</span><br/><span class="line"> x = x[:, -_slen:]</span><br/><span class="line"> positions = positions[:, -_slen:]</span><br/><span class="line"> <span class="keyword">if</span> langs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br/><span class="line"> langs = langs[:, -_slen:]</span><br/><span class="line"> mask = mask[:, -_slen:]</span><br/><span class="line"> attn_mask = attn_mask[:, -_slen:]</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;Retrieve, use and update cache: &lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L199-L207”&gt;link1&lt;/a&gt;&lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L423”&gt;link2&lt;/a&gt;&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.layer_id <span class="keyword">in</span> cache:</span><br/><span class="line"> k_, v_ = cache[<span class="variable language_">self</span>.layer_id]</span><br/><span class="line"> k = torch.cat([k_, k], dim=<span class="number">2</span>) <span class="comment"># (bs, n_heads, klen, dim_per_head)</span></span><br/><span class="line"> v = torch.cat([v_, v], dim=<span class="number">2</span>) <span class="comment"># (bs, n_heads, klen, dim_per_head)</span></span><br/><span class="line">cache[<span class="variable language_">self</span>.layer_id] = (k, v)</span><br/><span class="line">…</span><br/><span class="line">cache[<span class="string">'slen'</span>] += tensor.size(<span class="number">1</span>)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;XLM can serve multiple purposes including as a generative causallanguage model, masked language model, or a translation language model.We use KV-Cache only with causal language model in<code>generate()</code> function, see &lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L482-L498”&gt;code&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;XLM has a <code>Memory</code> module that implements &lt;ahref=”https://github.com/facebookresearch/XLM#v-product-key-memory-layers-pkm”&gt;Product-KeyMemory Layers&lt;/a&gt; whose mechanism rings very familiar to me but I can’trecall where I’ve encountered something similar before. Anyway, you canignore those <code>Memory</code> implementations and focus on theattention part if use it as a source to learn cache or the basics ofattention.&lt;/p&gt;&lt;h2 id="more-code-examples"&gt;More Code Examples&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;This Medium post &lt;ahref=”https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8”&gt;KVcaching explained&lt;/a&gt; leads way to where to find Hugging Face’simplementation in general, which can be too modular and abstractnowadays. It’s hidden in the <code>forward</code> function in<code>XXXForCausalLM</code>. Take &lt;ahref=”https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/llama2#transformers.LlamaForCausalLM”&gt;<code>LlamaForCausalLM</code>&lt;/a&gt;as an example, in its <code>forward</code> function, we still need to godown the abstraction to <code>LlamaModel</code> -&gt;<code>LlamaDecoderLayer</code> -&gt; &lt;ahref=”https://github.com/huggingface/transformers/blob/6c1d0b069de22d7ed8aa83f733c25045eea0585d/src/transformers/models/llama/modeling_llama.py#L337-L340”&gt;<code>LlamaAttention</code>&lt;/a&gt;and we can see the <code>past_key_value</code> there implementing the<code>Cache</code> class. I didn’t read into how Hugging Face didit.&lt;/li&gt;&lt;li&gt;<a href="https://zhuanlan.zhihu.com/p/630832593">This Zhihu postexplaining KV-Cache</a> leads way to &lt;ahref=”https://github.com/huggingface/transformers/blob/d1a1bcf56aeb8593b9cc613b21422e6311875599/src/transformers/models/gpt2/modeling_gpt2.py#L318-L321”&gt;HuggingFace’s GPT-2&lt;/a&gt;. The &lt;ahref=”https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L105-L108”&gt;originalGPT-2 code&lt;/a&gt; is in fact more straightforward, but you’d better justread XLM. It simply has more comments and the naming is moreself-explanatory.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="ps"&gt;PS&lt;/h2&gt;&lt;p&gt;I initially didn’t find where Hugging Face implemented KV-Cache incurrent version (<code>transformer 4.40</code>) but only this &lt;ahref=”https://github.com/huggingface/transformers/blob/aec1ca3a588bc6c65f7886e3d3eaa74901a6356f/src/transformers/cache_utils.py#L293”&gt;<code>Cache</code>class&lt;/a&gt; and failed to find where it’s used. So I followed therecommendation under &lt;ahref=”https://zhuanlan.zhihu.com/p/601044938”&gt;this Zhihu post&lt;/a&gt; to goto transformer 2.5.0 instead. A quick search like “kv” or “cache” led meto &lt;ahref=”https://github.com/huggingface/transformers/blob/v2.5.0/src/transformers/modeling_xlm.py”&gt;<code>modeling_xlm.py</code>&lt;/a&gt;.I was surprised to find early Hugging Face model code was more of arename of original implementation instead of a refactor they do now.&lt;/p&gt;&lt;p&gt;I then read this &lt;ahref=”https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8”&gt;KVcaching explained&lt;/a&gt; post. Its graph isn’t super straightforward but itintroduces how KV-cache reduces time complexity and where to findHugging Face’s implementation.&lt;/p&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Before this, see 2024/06/17 Conducting Multi-Round Conversation with Transformers for why we need cache. But we have query, key, value three matrices. Why do you only cache past keys and values? How about past queries?]]></summary></entry><entry><title type="html">Conducting Multi-Round Conversation with Transformers</title><link href="https://yao-lirong.github.io/2024/06/17/conducting-multi-round-conversation-with-transformers.html" rel="alternate" type="text/html" title="Conducting Multi-Round Conversation with Transformers"/><published>2024-06-17T04:00:00+00:00</published><updated>2024-06-17T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/06/17/conducting-multi-round-conversation-with-transformers</id><content type="html" xml:base="https://yao-lirong.github.io/2024/06/17/conducting-multi-round-conversation-with-transformers.html"><![CDATA[<p>I was using LLaVA to query in an image how many characters there are.For higher accuracy, I decided to employ Chain of Thought, but struggledto implement it. CoT is conducted through a multiple round conversation.It is easily done in a graphical chat interface but how is it doneinternally with code?</p> <p><span id="more"></span>&lt;h2 id="token-level"&gt;Token Level&lt;/h2&gt;&lt;p&gt;Before diving into instruct / chat model, let’s go to the lowestlevel and think how transformers do generation. Transformer is anautoregressive model: it uses its own output as input for the nextround. Looking at &lt;ahref=”https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L328”&gt;nanoGPT’s<code>generate</code> function&lt;/a&gt;:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br/><span class="line"> <span class="string">"""</span></span><br/><span class="line"><span class="string"> Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span><br/><span class="line"><span class="string"> the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span><br/><span class="line"><span class="string"> """</span></span><br/><span class="line"> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br/><span class="line"> idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= <span class="variable language_">self</span>.config.block_size <span class="keyword">else</span> idx[:, -<span class="variable language_">self</span>.config.block_size:]</span><br/><span class="line"> logits, _ = <span class="variable language_">self</span>(idx_cond)</span><br/><span class="line"> logits = logits[:, -<span class="number">1</span>, :] / temperature</span><br/><span class="line"> probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br/><span class="line"> idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br/><span class="line"> idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br/><span class="line"> <span class="keyword">return</span> idx</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;If we ignore the details, this for loop is effectively doing:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">token0 = tokenizer(text)</span><br/><span class="line">output1 = model(token0)</span><br/><span class="line"></span><br/><span class="line">token1 = get_resposne(output1) </span><br/><span class="line">output2 = model(token0 + token1)</span><br/><span class="line"></span><br/><span class="line">token2 = get_resposne(output2)</span><br/><span class="line">output3 = model(token0 + token1 + token2)</span><br/><span class="line">…</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;By writing it out like this, it’s clear that each turn of generation,we feed the previous step input into the model as something new, thoughexactly the same. Therefore, when we call<code>model(token0 + token1)</code>, <strong>we forgot about all theattention we calculated in <code>model(token0)</code></strong> eventhough the attention for <code>token0</code> part is actually completelythe same. This is why people complain transformer inference is slow andthis is where the inference speed-up techniques like KV-cache comesin.&lt;/p&gt;&lt;p&gt;This also reveals that the very popular graph demonstrating thetheory behind transformer’s inference lied (at least to me). Whencalculate &lt;spanclass=”math inline”&gt;<em>y</em><sub><em>i</em> + 1</sub>&lt;/span&gt;, we donot re-use &lt;spanclass=”math inline”&gt;<em>y</em><sub>0</sub>…<em>y</em><sub><em>i</em></sub>&lt;/span&gt;or the attention or the activations in the middle. We just re-feed themback into the model as something completely new.&lt;/p&gt;<figure>&lt;imgsrc=”https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png”alt=”Autoregressive Decoder” /&gt;&lt;figcaption aria-hidden="true"&gt;Autoregressive Decoder&lt;/figcaption&gt;</figure>&lt;h2 id="conversation-level"&gt;Conversation Level&lt;/h2&gt;&lt;p&gt;Chat model is also just a text continuation model except it follows achat template distinguishing which texts are inputted by the user andwhich are generated by the assistant. In the lowest abstraction level -the token level, for each turn, the model outputs one token and usesthat as part of the input in next turn’s generation. One abstractionlevel higher to this conversation level, to do multiple-roundconversation, a chat model similarly outputs one response to one user’sinput and uses that response as a part of the input for next turn’sgeneration. Therefore, to conduct conversation with a chat model, wejust append the model’s response at each turn to its correspondinginput.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">input1 = tokenizer(text1)</span><br/><span class="line">output1 = model(input1)</span><br/><span class="line"><span class="comment"># output1 contains input1 and model's response 1</span></span><br/><span class="line">response1 = get_resposne(output1) </span><br/><span class="line">input2 = tokenizer(text2)</span><br/><span class="line">output2 = model(input1 + response1 + input2)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;And yes, this means to get <code>output2</code>, we feed<code>input1 + response1</code> both as new to the model, but thisshouldn’t be a concern anymore since we feed each token as newanyway.&lt;/p&gt;&lt;h2 id="get_response"&gt;<code>get_response</code>&lt;/h2&gt;&lt;p&gt;The question now comes to how we should implement<code>get_response</code> to extract the assistant’s response from thetext-continuation model’s output.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Find the indicator (prefix) of the start of assistant’s message:Note when the model doesn’t follow the instruction and failed togenerate such a prefix, this method fails.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">prefix = <span class="string">"[/INST]"</span> <span class="comment"># escape special characters for regex</span></span><br/><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br/><span class="line"> output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br/><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br/><span class="line">answer_idx = [m.end() <span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(prefix, detoked_output)][-<span class="number">1</span>]</span><br/><span class="line">answer = detoked_output[answer_idx:]</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>recommended</strong> - Get the substring that is afterthe input (prompt): Hugging Face uses this approach in their &lt;ahref=”https://github.com/huggingface/transformers/blob/1c1aec2ef1d6822fae3ffbb973b4c941f65f4ddf/src/transformers/pipelines/text_generation.py#L369-L387”&gt;TextGenerationPipeline&lt;/a&gt;.There’s a <code>clean_up_tokenization_spaces</code> variable in<code>decode</code> function which defaults to <code>False</code>. (Forwhat it does, see &lt;ahref=”https://discuss.huggingface.co/t/what-does-the-parameter-clean-up-tokenization-spaces-do-in-the-tokenizer-decode-function/17399”&gt;thisdiscussion&lt;/a&gt;) Hugging Face set it to <code>True</code> in both call,but I tried set both to <code>False</code> or one to <code>True</code>the other to <code>False</code>, either can give correct results. Thatsaid, it’s still best to follow what Hugging Face wrote. After all theyknow their codes best.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">with</span> torch.no_grad():</span><br/><span class="line"> output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br/><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>, </span><br/><span class="line"> clean_up_tokenization_spaces=<span class="literal">True</span>)</span><br/><span class="line">cutoff = <span class="built_in">len</span>(text_processor.decode(</span><br/><span class="line"> inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>],</span><br/><span class="line"> skip_special_tokens=<span class="literal">True</span>,</span><br/><span class="line"> clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br/><span class="line"> ))</span><br/><span class="line">answer = detoked_output[cutoff:]</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="detours-when-taking-the-recommended-approach"&gt;Detours whenTaking the Recommended Approach&lt;/h2&gt;&lt;p&gt;I had some trouble with this recommended approach at first:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">chat = [</span><br/><span class="line"> {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"&lt;image&gt;\nHow many animated characters are there in this image?"</span>}</span><br/><span class="line">]</span><br/><span class="line">prompt = text_processor.apply_chat_template(chat, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br/><span class="line">inputs = processor(prompt, image, return_tensors=<span class="string">"pt"</span>).to(device)</span><br/><span class="line">…</span><br/><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br/><span class="line">cutoff = <span class="built_in">len</span>(prompt)</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;And <code>cutoff</code> is actually many indexes after the realstarting point of assistant’s response. That is because when we<code>apply_chat_template</code>, we added some special tokens<code>&lt;s&gt; &lt;\s&gt;</code> to indicate the start and end of oneturn of conversation with the assistant, but when we detokenize theoutput, we <code>skip_special_tokens</code> to get the response only andcaused this discrepancy.&lt;/p&gt;&lt;p&gt;I thought at first that this discrepancy comes from LLaVA replaced<code>&lt;image&gt;</code> token with the image embeddings (or<code>pixel_values</code> as Hugging Face calls it) because<code>&lt;image&gt;</code> also disappeared in the<code>detoked_output</code>. However, after reading LLaVA’s paper: &lt;ahref=”https://arxiv.org/abs/2304.08485”&gt;Visual Instruction Tuning&lt;/a&gt;Figure 1: LLaVA network architecture, I realized LLaVA actually puts theimage in front of the text input instead of inserting it in themiddle.&lt;/p&gt;<figure>&lt;img src=”https://arxiv.org/html/2304.08485v2/x1.png”alt=”LLaVA architecture” /&gt;&lt;figcaption aria-hidden="true"&gt;LLaVA architecture&lt;/figcaption&gt;</figure>&lt;p&gt;And <code>&lt;image&gt;</code> disappeared because it’s also aspecial token. However it was not inside the<code>tokenizer.all_special_tokens</code>. Reading the source code oftokenizer, I’m actually not sure how it was added as a special token sowas not able to debug why it’s not in <code>all_special_tokens</code>.For this specific behavior, I submitted &lt;ahref=”https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192”&gt;anissue on Hugging Face forum&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;You can find chat template definition in<code>tokenizer_config.json -&gt; "chat_template"</code>. Also in thisfile, <code>"added_tokens_decoder"</code> attribute defines<code>&lt;image&gt;</code> as a special token.&lt;/p&gt;&lt;h2 id="the-complete-code"&gt;The Complete Code&lt;/h2&gt;&lt;p&gt;I referenced Hugging Face conversation pipeline for &lt;ahref=”https://huggingface.co/docs/transformers/main/conversations#what-happens-inside-the-pipeline”&gt;thegeneral structure&lt;/a&gt; and &lt;ahref=”https://github.com/huggingface/transformers/blob/1c1aec2ef1d6822fae3ffbb973b4c941f65f4ddf/src/transformers/pipelines/text_generation.py#L369-L387”&gt;theresponse extractor&lt;/a&gt;&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">queries = [</span><br/><span class="line"> <span class="string">"&lt;image&gt;\nHow many animated characters are there in this image?"</span>,</span><br/><span class="line"> <span class="string">"Answer with a single number in decimal format. Give no explanations."</span></span><br/><span class="line">]</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">def</span> <span class="title function_">generate_response</span>(<span class="params">image</span>):</span><br/><span class="line"> chat = []</span><br/><span class="line"> <span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br/><span class="line"> chat.append({<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: query})</span><br/><span class="line"> prompt = text_processor.apply_chat_template(chat, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br/><span class="line"> inputs = processor(prompt, image, return_tensors=<span class="string">"pt"</span>).to(device)</span><br/><span class="line"></span><br/><span class="line"> <span class="keyword">with</span> torch.no_grad():</span><br/><span class="line"> output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br/><span class="line"> output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br/><span class="line"> </span><br/><span class="line"> input_ids = inputs[<span class="string">"input_ids"</span>]</span><br/><span class="line"> cutoff = <span class="built_in">len</span>(text_processor.decode(</span><br/><span class="line"> input_ids[<span class="number">0</span>],</span><br/><span class="line"> skip_special_tokens=<span class="literal">True</span>,</span><br/><span class="line"> clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br/><span class="line"> ))</span><br/><span class="line"> answer = output[cutoff:]</span><br/><span class="line"> chat.append({<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: answer})</span><br/><span class="line"> <span class="keyword">return</span> answer</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="ps"&gt;PS&lt;/h2&gt;&lt;p&gt;As written at the start of this blogpost, it all began from me tryingto do multi-round conversation with a transformer. A web search took meto these discussions (&lt;ahref=”https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/19”&gt;link1&lt;/a&gt;, <a href="https://github.com/salesforce/LAVIS/issues/357">link2</a>). It’s obvious <a href="#Conversation-Level">this acceptedapproach</a> of appending output to previous message causes great wasteof computing resources, which made me realize how transform works &lt;ahref=”#Token-Level”&gt;internally at the lowest level&lt;/a&gt; is itself a wasteof resources.&lt;/p&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I was using LLaVA to query in an image how many characters there are. For higher accuracy, I decided to employ Chain of Thought, but struggled to implement it. CoT is conducted through a multiple round conversation. It is easily done in a graphical chat interface but how is it done internally with code?]]></summary></entry><entry><title type="html">GPT-4o Release</title><link href="https://yao-lirong.github.io/2024/05/14/gpt-4o-release.html" rel="alternate" type="text/html" title="GPT-4o Release"/><published>2024-05-14T04:00:00+00:00</published><updated>2024-05-14T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/05/14/gpt-4o-release</id><content type="html" xml:base="https://yao-lirong.github.io/2024/05/14/gpt-4o-release.html"><![CDATA[<p>One day before Google I/O, OpenAI made a &lt;ahref="https://openai.com/index/spring-update/"&gt;Spring UpdateRelease&lt;/a&gt;, introducing multi-modal end-to-end model &lt;ahref="https://openai.com/index/hello-gpt-4o/"&gt;GPT4-o&lt;/a&gt;</p> <p><span id="more"></span>&lt;h2 id="capabilities-and-features"&gt;Capabilities and Features&lt;/h2&gt;&lt;p&gt;In their &lt;ahref=”https://www.youtube.com/watch?v=DQacCB9tDaw”&gt;release live&lt;/a&gt;, wesee&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Real-time responsiveness in audio mode, ability to beinterruptted&lt;/li&gt;&lt;li&gt;Detect tone and mood in your speech, including how hard youbreath&lt;/li&gt;&lt;li&gt;Real-time responsiveness in vision mode: no need to take a picture,just hold your phone’s camera there and it can screenshot(?) foryou&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Right after the live, OpenAI &lt;ahref=”https://openai.com/index/hello-gpt-4o/”&gt;updated their blog&lt;/a&gt;,showing more demos:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;<a href="https://vimeo.com/945587746">Two GPT-4os harmonizing</a>:on the same device same session. They can sing and harmonize. They canfollow user’s instruction to sing faster, sing slower, or sing in ahigher voice.&lt;/li&gt;&lt;li&gt;<a href="https://vimeo.com/945587944">Lullaby</a>: user can giveinstruction by speech to tell GPT-4o to go lighter, louder, …&lt;/li&gt;&lt;li&gt;<a href="https://vimeo.com/945587927">Taking faster</a>: user cangive instruction by speech to tell GPT-4o to speak faster, slower&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;<a href="https://vimeo.com/945591584">Failure cases</a>: itsometimes&lt;/p&gt;&lt;ul&gt;&lt;li&gt;go wild and speak in another language&lt;/li&gt;&lt;li&gt;fail in translation tasks&lt;/li&gt;&lt;li&gt;fail in teaching intonation in Chinese&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="technicality"&gt;Technicality&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;GPT-4o is a single new model <strong>end-to-end across text, vision,and audio</strong>, meaning that all inputs and outputs are processed bythe same neural network. Previously, ChatGPT Voice Mode is a pipeline ofthree separate models: audio-text transcription, GPT-3.5/GPT-4 textmodel, text-to-audio conversion model&lt;/li&gt;&lt;li&gt;Designed a new tokenizer with greater compression: Hindi has 2.9xfewer tokens, Russian 1.7x, Korean 1.7x, Chinese 1.4x, Japanese 1.4x,and European languages, including English, has 1.1x fewer tokens. Newtokenizer means fully new pre-trained model (brought up in &lt;ahref=”https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/comment/l3ww1y7/”&gt;thisreddit thread&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;It is super fast, responding to audio inputs with an average of 320milliseconds, while original ChatGPT Voice Mode has latencies of 2.8seconds (GPT-3.5) and 5.4 seconds (GPT-4) on average. At the same time,GPT-4o “achieves GPT-4 Turbo-level performance on text, reasoning, andcoding intelligence.” What did they do to speed up inference? Is itQuantization, MoE or something else? (brought up in &lt;ahref=”https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/comment/l3wqmit/”&gt;thisreddit thread&lt;/a&gt;) What’s the model size? Nothing is reported.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="inspecting-the-new-tokenizer"&gt;Inspecting the New Tokenizer&lt;/h2&gt;&lt;p&gt;When I used reddit on the day GPT-4o released, &lt;ahref=”https://www.reddit.com/r/real_China_irl/comments/1crvv4m/openai%E7%AE%80%E5%8D%95%E8%BE%B1%E4%B8%AA%E5%8D%8E/”&gt;thispost&lt;/a&gt; came to me suggesting Chinese tokens in OpenAI’s new tokenizerare greatly contaminated.&lt;/p&gt;&lt;p&gt;The new tokenizer <code>o200k_base</code> is actually twice as largeas the last <code>cl100k_base</code> and has already been loaded toGitHub in &lt;ahref=”https://github.com/openai/tiktoken/commit/9d01e5670ff50eb74cdb96406c7f3d9add0ae2f8”&gt;thiscommit&lt;/a&gt;.&lt;/p&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[One day before Google I/O, OpenAI made a Spring Update Release, introducing multi-modal end-to-end model GPT4-o]]></summary></entry><entry><title type="html">CLIP</title><link href="https://yao-lirong.github.io/2024/04/22/clip.html" rel="alternate" type="text/html" title="CLIP"/><published>2024-04-22T04:00:00+00:00</published><updated>2024-04-22T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/04/22/clip</id><content type="html" xml:base="https://yao-lirong.github.io/2024/04/22/clip.html"><![CDATA[<p>CLIP investigates whether it is possible to transfer the success oftask-agnostic web-scale pre-training in NLP to another domain (CV).</p> <p><span id="more"></span>&lt;blockquote&gt;&lt;p&gt;This line of work represents the current pragmatic middle groundbetween learning from a limited amount of supervised “gold-labels” andlearning from practically unlimited amounts of raw text.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="approach"&gt;2 Approach&lt;/h2&gt;&lt;h3 id="advantage-of-natural-language-supervision"&gt;2.1 Advantage ofNatural Language Supervision&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;easy to scale: natural language data amount is huge, much easier toobtain than crowd-sourced labeling&lt;/li&gt;&lt;li&gt;flexible zero-shot transfer: connects image representation tolanguage; different from unsupervised or self-supervised model that islimited to image domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="constructing-dataset"&gt;2.2 Constructing Dataset&lt;/h3&gt;&lt;p&gt;To explore effects of web-scale pre-training, we first build aweb-scale dataset.&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Construct a query list of size 500,000 that contains words occurred&gt;= 100 times in Wikipedia&lt;/li&gt;&lt;li&gt;Search for images of these queries, construct a dataset of 400M(image, text) pair&lt;/li&gt;&lt;li&gt;<strong>Class balance</strong> (yeah that’s the word describing“make each class have the same number of samples so it’s fair”) byincluding 20,000 pairs per query&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="what-to-predict-what-is-the-loss"&gt;2.3 What to Predict? What isthe Loss?&lt;/h3&gt;&lt;p&gt;Previous methods with natural language supervision attempt is aboutpredicting a bag of words (BoW) / phrase n-gram representation oflabels. The authors explore different approaches. This work is all aboutlarge scale pre-training and <strong>scaling</strong>. <strong>Trainingefficiency</strong> is the key to scaling natural language supervision.Authors selected final pre-training method based on efficiency. Theycompared three approaches:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;<strong>Transformer language model (captioning model)</strong>:train a transformer to predict the caption of an image. So this is agenerative task and uses transformer’s loss function. It learns 3 timesslower than the baseline - approach 2.&lt;/li&gt;&lt;li&gt;<strong>A model predicts BoW encoding of the caption</strong>: thiswas used as a simple baseline and authors found approach 1 couldn’t evenbeat this baseline. This approach still tries to <strong>predict theexact words</strong> of the text label, but the order of how wordsappear no longer matters. This is not much easier due to the widevariety of descriptions, comments, and related text that co-occur withimages.&lt;/li&gt;&lt;li&gt;<strong>A contrastive model predicts which text <em>as a whole</em>is paired with which image</strong>: In this way, we decrease the outputspace to only the number of classes we have. We learn 4 times fasterthan the baseline - approach 2.&lt;/li&gt;&lt;/ol&gt;<figure><img src="/images/CLIP_2.png" alt="Accuracy vs #(images processed)"/>&lt;figcaption aria-hidden="true"&gt;Accuracy vs #(imagesprocessed)&lt;/figcaption&gt;</figure>&lt;p&gt;See Figure 2 for a detailed comparison on <em>accuracy vs. #(imagesfed)</em> of these three models. This illustrates how fast / slow atraining method learns.&lt;/p&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col style="width: 18%" /&gt;&lt;col style="width: 42%" /&gt;&lt;col style="width: 39%" /&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Approach&lt;/th&gt;&lt;th&gt;Output Space&lt;/th&gt;&lt;th&gt;Answer Space: In ideal scenario, what do we choose from?&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;Transformer Language Model&lt;/td&gt;&lt;td&gt;All English sentences (permutation of all English words)&lt;/td&gt;&lt;td&gt;500K queries&lt;/td&gt;&lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;BoW prediction model&lt;/td&gt;&lt;td&gt;Word count bucket of all English sentences (combination of allEnglish words)&lt;/td&gt;&lt;td&gt;500K queries&lt;/td&gt;&lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;Contrastive pairing model&lt;/td&gt;&lt;td&gt;Sentences describing class and labels&lt;/td&gt;&lt;td&gt;<code>batch_size</code> pre-selected queries (32768 in CLIP)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;It’s worth noting that CLIP uses a very large minibatch size of &lt;spanclass=”math inline”&gt;2<sup>15</sup> = 32768&lt;/span&gt;&lt;/p&gt;&lt;h3 id="model-architecture-and-scaling"&gt;2.4 Model Architecture andScaling&lt;/h3&gt;<figure><img src="/images/CLIP_1.png" alt="Summary of CLIP"/>&lt;figcaption aria-hidden="true"&gt;Summary of CLIP&lt;/figcaption&gt;</figure>&lt;p&gt;Image encoder has two architectures: ResNet-50 and ViT&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br/><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br/><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br/><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br/><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br/><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br/><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br/><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br/><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br/><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br/><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br/><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br/><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br/><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br/><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br/><span class="line"><span class="comment"># symmetric loss function</span></span><br/><span class="line">labels = np.arange(n)</span><br/><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br/><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br/><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;Note:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;<code>d_e</code> represents multi-modal embedding space.&lt;/li&gt;&lt;li&gt;the temperature parameter &lt;spanclass=”math inline”&gt;<em>τ</em>&lt;/span&gt; is directly optimized as alog-parameterized multiplicative scalar to avoid turning as ahyper-parameter. &lt;ahref=”https://github.com/openai/CLIP/blob/a1d071733d7111c9c014f024669f959182114e33/clip/model.py#L367-L368”&gt;implementationin original release&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The authors train CLIP from scratch without initializing the imageencoder with ImageNet weights or the text encoder with pre-trainedweights.&lt;/p&gt;&lt;p&gt;This section also describes how to scale the text encoder and how toscale both kinds of image encoder.&lt;/p&gt;&lt;h2 id="experiments"&gt;3 Experiments&lt;/h2&gt;&lt;p&gt;Authors conducted experiments on 36 different datasets.&lt;/p&gt;&lt;h3 id="zero-shot-transfer"&gt;3.1 Zero-Shot Transfer&lt;/h3&gt;&lt;p&gt;Authors wanted to experiment on zero-shot transfer ability because ofthe ability demonstrated in language models. The following is the mostexciting sentence to me in this paper. I think it explains a lot oflarge-scale design choices by OpenAI team. Did this paper inspire Ilyato go all the way down the path of scaling?&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Our focus on studying zero-shot transfer as an evaluation of tasklearning is inspired by work demonstrating task learning in the field ofNLP. To our knowledge Liu et al. (2018) first identified task learningas an “unexpected side-effect” when a language model trained to generateWikipedia articles learned to reliably transliterate names betweenlanguages.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Authors explain in detail how we do zero-shot classification and givean interpretation to the pipeline. I wrote the previous “output space”and “answer space” thing based on this interpretation.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The cosine similarity of these embeddings is then calculated, scaledby a temperature parameter <span class="math inline"><em>τ</em></span> ,and normalized into a probability distribution via a softmax. Note thatthis prediction layer is a multinomial logistic regression classifierwith L2-normalized inputs, L2-normalized weights, no bias, andtemperature scaling. When interpreted this way, the image encoder is thecomputer vision backbone which computes a feature representation for theimage and the text encoder is a hypernetwork which generates the weightsof a linear classifier based on the text specifying the visual conceptsthat the classes represent. Continuing with this interpretation, everystep of CLIP pre-training can be viewed as optimizing the performance ofa randomly created proxy to a computer vision dataset which contains 1example per class and has 32,768 total classes defined via naturallanguage descriptions.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;<strong>prompt engineering and ensembling </strong>&lt;/p&gt;&lt;p&gt;Text in our training data is usually a sentence, but text in testdata is just a one word label. To bridge this gap, we use some prompttemplate.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;default: <code>A photo of a &#123;label&#125;</code>&lt;/li&gt;&lt;li&gt;on several fine-grained image classification datasets, it’s helpfulto specify the category:<code>A photo of a &#123;label&#125;, a type of pet</code> or<code>a satellite photo of a &#123;label&#125;</code>&lt;/li&gt;&lt;li&gt;ensembling several different prompts improve performance: usedifferent context prompts such as <code>A photo of a big &#123;label&#125;</code>and <code>A photo of a small &#123;label&#125;</code>. Authors construct theensemble over the embedding space instead of probability space. In thisway, they cache a single set of averaged text embedding so compute costdoesn’t increase in amortized time.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;<strong>scaling law</strong>&lt;/p&gt;<figure>&lt;img src=”/images/CLIP_9.png”alt=”Zero-shot CLIP scales wrt model compute” /&gt;&lt;figcaption aria-hidden="true"&gt;Zero-shot CLIP scales wrt modelcompute&lt;/figcaption&gt;</figure>&lt;p&gt;Scaling law is the law that empirically shows that performance ispredictable as a function of important quantities such as trainingcompute and dataset size.&lt;/p&gt;&lt;p&gt;On 36 different datasets, ResNet CLIP’s average zero-shot error iswell modeled by a log-log linear scaling trend. However, performance onindividual evaluations is much more varied despite the smooth overalltrend. Authors did not report ViT CLIP scaling results.&lt;/p&gt;&lt;h3 id="representation-learning"&gt;3.2 Representation Learning&lt;/h3&gt;&lt;p&gt;To use CLIP as a representation of the image, there are two commonapproaches:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Fitting a linear classifier on a representation extracted from themodel&lt;/li&gt;&lt;li&gt;End-to-end fine-tuning of the model.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Fine-tuning increases flexibility, and prior work has convincinglydemonstrated that fine-tuning outperforms linear classification on mostimage classification datasets. However, OpenAI chooses to use linearclassifier to measure CLIP performance for the following reasons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;the more official reason: we chose it because it’s weak andtherefore better shows how dataset-agnostic CLIP is&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Our work is focused on developing a high-performing task anddataset-agnostic pre-training approach. Fine-tuning, because it adaptsrepresentations to each dataset during the fine-tuning phase, cancompensate for and potentially mask failures to learn general and robustrepresentations during the pre-training phase. Linear classifiers,because of their limited flexibility, instead highlight these failuresand provide clear feedback during development&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;the more practical reason:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Fine-tuning opens up a much larger design and hyper-parameter space,which makes it difficult to fairly evaluate and computationallyexpensive. By comparison, linear classifiers require minimalhyper-parameter tuning and have standardized implementations andevaluation procedures.&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;bonus reason:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Linear classifier has the added benefit of being very similar to theapproach used for its zero-shot classifiers which enables extensivecomparisons and analysis&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;<strong>approach</strong>: Appendix A.3 provides a full guideline oftraining such a linear classifier, including details on hyper-parametersearch, solver method, and train-valid-test split. Notably, the input tothe Logistic Regression is the image embedding (output of the imageencoder <code>I_f</code>), not the multi-modal embedding (imageembedding that went through the multi-modal linear projection)&lt;/p&gt;&lt;p&gt;<strong>results</strong>: when comparing to other models of similarcompute requirement, small CLIP have wins and loses. However, CLIPscales very well and the largest model achieves both SOTA score andcompute efficiency.&lt;/p&gt;&lt;p&gt;<strong>ViT vs ResNet</strong>: The authors found CLIP ViT is about3x more compute efficient than CLIP ResNet. This is aligned with ViTpaper’s finding&lt;/p&gt;&lt;p&gt;<strong>Out-of-Domain Performance and Natural DistributionShift</strong>: Researchers often find models exceeding human onImageNet test set can still make simple mistakes on other test data andscore much lower than human. A common explanation is these models areadept at finding patterns within dataset, so improve in-distributionperformance. However many of these patterns are spurious and do not holdfor other distributions and result in large drops in performance onother datasets.&lt;/p&gt;&lt;p&gt;Most of the studies that reach the above explanation limited theirevaluation model to those trained on ImageNet. Therefore, the authorswant to know to what degree are these failures attributable to deeplearning, ImageNet, or some combination of the two? They explore this byevaluating ImageNet models on natural distribution shifted dataset.&lt;/p&gt;&lt;p&gt;Natural distribution shift means testing trained models on data thatis different in e.g. image style, image blurriness, geographic location,and camera operation (<em>Hendrycks et al. The many faces ofrobustness</em>). “Natural” is used to make a distinction from syntheticdistribution shift made through style-transferred or adversariallygenerated.&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Authors found CLIP perform much better on these natural distributionshifted dataset.&lt;/li&gt;&lt;li&gt;However, this doesn’t necessarily mean supervised learning onImageNet causes a robustness gap. Other details of CLIP, such as itslarge and diverse pre-training dataset or use of natural languagesupervision could also produce robust models.&lt;/li&gt;&lt;li&gt;Therefore, OpenAI measured how the performance of CLIP models changeafter adapting to the ImageNet distribution via an L2 regularizedlogistic regression classifier fit to CLIP features on the ImageNettraining set. This improved accuracy on ImageNet by 9.2% to 85.4%, butaverage accuracy under distribution shift slightly decreases.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To me this doesn’t say much. If you fine-tune (or fit a linearclassifier) to a specific dataset, of course you’d expect its behaviorto be bad on some other dataset. But on the contrary, thesenatural-distribution-shifted dataset is not that different fromImageNet. Yes, there are some animations / sketches, but most are justsome more pictures of that class. And CLIP with an ImageNet linear headcannot get them right. I guess what the authors want to say is thatImageNet is not just A arbitrary dataset, but has almost become amachine learning benchmark dataset. It is supposed to be general becauseall models train on it and these models will be deployed to all sorts ofscenario.&lt;/p&gt;&lt;p&gt;The authors didn’t go far to attack the generality of ImageNet oreven draw any conclusion on why fitting an ImageNet classification headhurts natural distribution shift performance. The authors just prompt tocaution that though prior work has also pre-trained models ondistributions other than ImageNet, it is common to study and releasemodels only after they have been fine-tuned to ImageNet. And it would bewise to also study the models pre-trained on distributions other thanImageNet.&lt;/p&gt;&lt;p&gt;<strong>Results:</strong> Taken together, these results suggest thatthe recent shift towards large-scale task and dataset agnosticpre-training combined with a reorientation towards zero-shot andfew-shot benchmarking on broad evaluation suites promotes thedevelopment of more robust systems and provides a more accurateassessment of performance.&lt;/p&gt;&lt;h2 id="data-overlap-analysis"&gt;5 Data Overlap Analysis&lt;/h2&gt;&lt;p&gt;A concern with pre-training on a very large internet dataset isunintentional overlap with downstream evals. One option to prevent thisis to identify and remove all duplicates before training a model. Whilethis guarantees reporting true hold-out performance, it requires knowingall possible data which a model might be evaluated on ahead of time.This has the downside of limiting the scope of benchmarking andanalysis.&lt;/p&gt;&lt;p&gt;Therefore, OpenAI instead built a duplicate detector, document howmuch overlap occurs, and run experiments on dataset with and withoutthese overlaps to measure how performance changes due to these overlaps.So instead of simply removing them, they record performance of beforeand after removing them.&lt;/p&gt;&lt;p&gt;They found that there is a median overlap of 2.2% and an averageoverlap of 3.2%. Due to this small amount of overlap, overall accuracyis rarely shifted by more than 0.1% with only 7 datasets above thisthreshold.&lt;/p&gt;&lt;p&gt;It would be useful if OpenAI also releases their duplicate detectormodel. Appendix C discusses it in more details but it doesn’t seem likeOpenAI ever released it.&lt;/p&gt;&lt;h2 id="limitations"&gt;6 Limitations&lt;/h2&gt;&lt;p&gt;<strong>Performance</strong>:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;CLIP cannot beat dataset-specific trained &amp; designed models:CLIP zero-shot performs better than a pre-trained ResNet-50 feature + alinear classifier, but on most datasets, CLIP is well below the SOTA forthat specific dataset.&lt;/li&gt;&lt;li&gt;zero-shot CLIP still generalizes poorly to data that is trulyout-of-distribution for it: CLIP simply has a super large domain, notreally a general model. For example, MNIST digits are not at all in itsweb-scraped huge dataset, so CLIP does surprisingly bad on this supersimple dataset.&lt;/li&gt;&lt;li&gt;CLIP is limited to “choosing”: CLIP cannot just take in a pictureand spit out its class. You need to give CLIP a range to choose from.CLIP is based on “choosing”, not “generating” (image captioningmodel)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;<strong>Training Methodology</strong>:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;In training time, CLIP repeatedly queried performance on fullvalidation sets to guide optimization. These validation sets often havethousands of examples, which is unrealistic for true zero-shotscenarios. On the contrary, LLM in training time doesn’t do this(?)&lt;/li&gt;&lt;li&gt;Training dataset comes from Internet. Its image-text pairs areunfiltered and uncurated and result in CLIP models learning many socialbiases.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;<strong>Supervision with Natural Language</strong>:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Many complex tasks and visual concepts can be difficult to specifyjust through text.&lt;/li&gt;&lt;li&gt;Actual training examples are undeniably useful but CLIP does notoptimize for few-shot performance directly. In our work, we fall back tofitting linear classifiers on top of CLIP’s features. This results in acounter-intuitive drop in performance when transitioning from azero-shot to a few-shot setting.&lt;/li&gt;&lt;/ol&gt;&lt;h2 id="broader-impacts"&gt;7 Broader Impacts&lt;/h2&gt;&lt;p&gt;In this section, the authors mainly introduces the bias exists inCLIP and what kind of surveillance it can be used for.&lt;/p&gt;&lt;p&gt;Nothing too interesting, but they discussed how tweaking the categorysystem can improve model’s performance. This reminds me of what I did inXiaomi’s oversea app store tagging project, where I added new categoryand modified existing category’s definition to improve thecos-similarity based zero-shot classification model performance.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Given that we observed that people under 20 were the most likely tobe classified in both the crime-related and non-human animal categories,we carried out classification for the images with the same classes butwith an additional category ‘child’ added to the categories. We foundthat this drastically reduced the number of images of people under 20classified in either crime-related categories or non-human animalcategories (Table 7). This points to how class design has the potentialto be a key factor determining both the model performance and theunwanted biases or behavior the model may exhibit&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The authors then go on to conclude that&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Decisions about things like class design are a key determiner notonly of model performance, but also of how and in what contexts modelbiases manifest&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="takeaways"&gt;Takeaways&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Data is still the king in ML. It is possible to transfer thesuccess of task-agnostic web-scale pre-training in NLP to CV.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The key to scaling &amp; training efficiency is how compact youroutput space is (word permutation - &gt; word combination -&gt;<code>batch_size</code>)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;We can use prompt ensembling to improve CLIP’sperformance.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;To use CLIP as the feature extractor and put a linear classifieron top of it, we use the image embedding (image encoder’s output), nothe multi-modal embedding (image embedding went through the multi-modallinear projection);&lt;/p&gt;&lt;p&gt;On the other hand, for zero-shot classification, you use multi-modalembedding, the same as the training process except now you only have oneimage and calculate the cos similarity with all class names.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Decisions about things like class design are a key determiner notonly of model performance, but also of how and in what contexts modelbiases manifest&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[CLIP investigates whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain (CV).]]></summary></entry><entry><title type="html">Gradient Scaling</title><link href="https://yao-lirong.github.io/2024/04/08/gradient-scaling.html" rel="alternate" type="text/html" title="Gradient Scaling"/><published>2024-04-08T04:00:00+00:00</published><updated>2024-04-08T04:00:00+00:00</updated><id>https://yao-lirong.github.io/2024/04/08/gradient-scaling</id><content type="html" xml:base="https://yao-lirong.github.io/2024/04/08/gradient-scaling.html"><![CDATA[<p>Loss Scaling / Gradient Scaling was mentioned in &lt;ahref="#2024/03/01-Mixed-Precision-Training"&gt;Mixed-Precision Training&lt;/a&gt;as one of the 3 techniques, but there are many points to be careful withwhen in practice.</p> <p><span id="more"></span>&lt;h2 id="overview-typical-use-case"&gt;Overview: Typical Use Case&lt;/h2&gt;&lt;p&gt;Here’s an overview of how to use <code>amp.GradScaler</code> adaptedfrom &lt;ahref=”https://pytorch.org/docs/stable/amp.html#gradient-scaling”&gt;PyTorchofficial doc&lt;/a&gt;.&lt;/p&gt;&lt;h3 id="background"&gt;Background&lt;/h3&gt;&lt;p&gt;If the forward pass for a particular op has <code>float16</code>inputs, under &lt;ahref=”https://pytorch.org/docs/stable/amp.html”&gt;Automatic MixedPrecision package - torch.amp&lt;/a&gt;, the backward pass for that op willproduce gradients of the same data type - <code>float16</code> .Gradient values with small magnitudes may not be representable in<code>float16</code>. These values will flush to zero (“underflow”), sothe update for the corresponding parameters will be lost.&lt;/p&gt;&lt;h3 id="code"&gt;Code&lt;/h3&gt;&lt;ol type="1"&gt;&lt;li&gt;<code>scaler.scale(loss).backward()</code>: To prevent underflow,“gradient scaling’ multiplies the network’s loss(es) by a scale factorand invokes a backward pass on the scaled loss(es). In this way, thegradients on all parameters are scaled by this same factor and we don’thave to worry about them flush to zero. <code>scaler.scale(loss)</code>multiplies a given loss by <code>scaler</code>’s current scale factor.We then call backward on this scaled loss.&lt;/li&gt;&lt;li&gt;<code>scaler.step(optimizer)</code>: After back-propagation, alllearnable parameters get their gradients, which are scaled to preventunderflow. Before applying whatever learning algorithm (Adam, SGD, …) onthem, we have to unscale them so the amount to be updated is correct.<code>scaler.step(optimizer)</code> 1. unscales gradients, 2. calls<code>optimizer.step()</code>, and does the previous two points safely:&lt;ol type="1"&gt;&lt;li&gt;Internally invokes <code>unscale_(optimizer)</code> (unless<code>unscale_()</code> was explicitly called for <code>optimizer</code>earlier in the iteration). As part of the <code>unscale_()</code>,gradients are checked for infs/NaNs to prevent overflow/underflow (Forwhy overflow can happen, check point 3 <code>scaler.update</code>)&lt;/li&gt;&lt;li&gt;If no inf/NaN gradients are found, invokes<code>optimizer.step()</code> using the unscaled gradients. Otherwise,<code>optimizer.step()</code> is skipped to avoid corrupting theparams.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;<code>scaler.update()</code>: It would be great if we could justmultiply all gradients by a super big number so absolutely no underflowhappens, but doing so can cause overflow. The scaler estimates a goodscaling factor for each iteration, so neither underflow nor overflowhappens. <code>scaler.update()</code> updates <code>scaler</code>’sscale factor for next iteration.&lt;/li&gt;&lt;/ol&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">scaler = torch.cuda.amp.GradScaler()</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br/><span class="line"> <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br/><span class="line"> optimizer.zero_grad()</span><br/><span class="line"> <span class="keyword">with</span> autocast(device_type=<span class="string">'cuda'</span>, dtype=torch.float16):</span><br/><span class="line"> output = model(<span class="built_in">input</span>)</span><br/><span class="line"> loss = loss_fn(output, target)</span><br/><span class="line"> scaler.scale(loss).backward()</span><br/><span class="line"> scaler.step(optimizer)</span><br/><span class="line"> scaler.update()</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="working-with-unscaled-gradients---gradient-clipping"&gt;&lt;ahref=”https://pytorch.org/docs/stable/notes/amp_examples.html#id4”&gt;Workingwith Unscaled Gradients - Gradient clipping&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;gradient clipping manipulates a set of gradients such that theirglobal norm &lt;ahref=”https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_”&gt;<code>torch.nn.utils.clip_grad_norm_()</code>&lt;/a&gt;or maximum magnitude &lt;ahref=”https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_”&gt;<code>torch.nn.utils.clip_grad_value_()</code>&lt;/a&gt;is &lt;= some user-imposed threshold.&lt;/p&gt;&lt;p&gt;The “gradients” here of course refer to the original, unscaledgradients. Therefore, you need to call<code>scaler.unscale_(optimizer)</code> before clipping.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">scaler = GradScaler()</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br/><span class="line"> <span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br/><span class="line"> optimizer.zero_grad()</span><br/><span class="line"> <span class="keyword">with</span> autocast(device_type=<span class="string">'cuda'</span>, dtype=torch.float16):</span><br/><span class="line"> output = model(<span class="built_in">input</span>)</span><br/><span class="line"> loss = loss_fn(output, target)</span><br/><span class="line"> scaler.scale(loss).backward()</span><br/><span class="line"></span><br/><span class="line"> <span class="comment"># Unscales the gradients of optimizer's assigned params in-place</span></span><br/><span class="line"> scaler.unscale_(optimizer)</span><br/><span class="line"></span><br/><span class="line"> <span class="comment"># Since the gradients of optimizer's assigned params are unscaled, clips as usual:</span></span><br/><span class="line"> torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)</span><br/><span class="line"></span><br/><span class="line"> <span class="comment"># optimizer's gradients are already unscaled, so scaler.step does not unscale them,</span></span><br/><span class="line"> <span class="comment"># although it still skips optimizer.step() if the gradients contain infs or NaNs.</span></span><br/><span class="line"> scaler.step(optimizer)</span><br/><span class="line"></span><br/><span class="line"> scaler.update()</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="working-with-scaled-gradients---gradient-accumulation"&gt;&lt;ahref=”https://pytorch.org/docs/stable/notes/amp_examples.html#id6”&gt;Workingwith Scaled Gradients - Gradient accumulation&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Gradient accumulation adds gradients over an effective batch of size<code>batch_per_step * gradient_accumulation_steps</code>(<code>* num_procs</code> if distributed). Operations related to scaledgradients should occur at effective batch granularity. The followinghappens at the end of each effective batch:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;inf/NaN checking&lt;/li&gt;&lt;li&gt;step skipping if inf/NaN grads are found&lt;/li&gt;&lt;li&gt;parameter update&lt;/li&gt;&lt;li&gt;scale update&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Within an effective batch, all grads you accumulate should all bescaled and the scale factor should remain unchanged.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">scaler = GradScaler()</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br/><span class="line"> <span class="keyword">for</span> micro_step <span class="keyword">in</span> <span class="built_in">range</span>(gradient_accumulation_steps):</span><br/><span class="line"> <span class="built_in">input</span>, target = get_data(epoch, micro_step)</span><br/><span class="line"> <span class="keyword">with</span> autocast(device_type=<span class="string">'cuda'</span>, dtype=torch.float16):</span><br/><span class="line"> output = model(<span class="built_in">input</span>)</span><br/><span class="line"> loss = loss_fn(output, target)</span><br/><span class="line"> loss = loss / gradient_accumulation_steps</span><br/><span class="line"> <span class="comment"># Accumulates scaled gradients.</span></span><br/><span class="line"> scaler.scale(loss).backward()</span><br/><span class="line"> </span><br/><span class="line"> <span class="comment"># If you need to work with unscaled gradients, </span></span><br/><span class="line"> <span class="comment"># after all (scaled) grads for the upcoming step have been accumulated</span></span><br/><span class="line"> <span class="comment"># may unscale_ here if desired (e.g., to allow clipping unscaled gradients)</span></span><br/><span class="line"> scaler.step(optimizer)</span><br/><span class="line"> scaler.update()</span><br/><span class="line"> optimizer.zero_grad()</span><br/>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;<strong>These examples may seem too vanilla, check out &lt;ahref=”https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/train.py#L290-L314”&gt;nanoGPT’smixed precision training loop&lt;/a&gt; for a lively combination of gradientaccumulation and gradient clipping.</strong>&lt;/p&gt;&lt;h2 id="working-with-scaled-gradients---gradient-penalty"&gt;&lt;ahref=”https://pytorch.org/docs/stable/notes/amp_examples.html#id7”&gt;Workingwith Scaled Gradients - Gradient penalty&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;What? Why?&lt;/p&gt;&lt;p&gt;https://discuss.pytorch.org/t/whats-the-use-of-scaled-grad-params-in-this-example-of-gradient-penalty-with-scaled-gradients/199741/3&lt;/p&gt;&lt;h2 id="epilogue"&gt;Epilogue&lt;/h2&gt;&lt;p&gt;This <a href="https://deepgram.com/ai-glossary/gradient-scaling">wikipage from Deepgram</a> provides a detailed view of what gradient scalingis about, but I don’t know why it just reads like AI-generated content.Maybe because it gives too many unnecessary details.&lt;/p&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Loss Scaling / Gradient Scaling was mentioned in Mixed-Precision Training as one of the 3 techniques, but there are many points to be careful with when in practice.]]></summary></entry></feed>