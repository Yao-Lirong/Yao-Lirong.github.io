<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="k-bit-inference-scaling-laws">&lt;ahref="https://arxiv.org/abs/2212.09720"&gt;K-bit Inference ScalingLaws&lt;/a&gt;</h2> <p>This paper and its Appendix serves as a good summary of SOTAquantization techniques and their results.</p> <p><strong>Why should we quantize?</strong> The overall computationlatency – the time it takes from start to finish of a computation – ismainly determined by two factors: (1) how long does it take to load thedata from main memory into caches and registers, (2) how long does ittake to perform the computation. Therefore, reducing the time spentloading data from main memory is often the best way to accelerateoverall computation latency. Such reductions can be achieved mainlythrough caching and lower precision numbers.</p> <p>Note though, <strong>to do computation</strong>, we dequantize theweight in the cache and perform a 16-bit floating point multiplicationwith the 16-bit input. That’s because no CPU / GPU supports these weirddata type computation.</p> <p>In this work, the author used blocking, a zero-shot quantizationmethod, for 3, 4, 5, 6, 7, and 8 bits. By plotting out perplexity vstotal bits of the model, they found that lowering the bit precisiongenerally improves scaling. However, this trend stops across all modelsat 3-bit precision, where performance degrades. Therefore, <strong>4-bitprecision is optimal</strong> for almost all models at all scales, withfew exceptions.</p> <p>BLOOM and BLOOMZ show almost the same quantization behavior,indicating that <strong>fine-tuning</strong> an existing model does notchange its quantization properties.</p> <p>Data types: The quantile quantization and float data types providebetter scaling than integer and dynamic exponent quantization. Theauthor concluded that <strong>quantile quantization is thebest</strong>.</p> <ul> <li> <strong>zero-shot quantization</strong>: directly quantize a modelwithout any additional information. Can be used immediately, which makesthem easy to use, but zero-shot quantization methods often fail at lowerprecisions.</li> <li> <strong>one-shot quantizationL</strong> need a mini-batch of datafor quantization. more accurate, such as GPTQ, which optimizes therounding during quantization via a mini-batch of data. But they are alsomore complex and may require hours of optimization before a model can beused.</li> </ul> <h2 id="llm.int8">&lt;ahref="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"&gt;LLM.int8()&lt;/a&gt;</h2> <p>In this quantization paper, the author discovered an emergent outlierfeature in tranformers that totally wreck quantization. They alsoplotted a scaling law for this emergent outlier feature. By doing so, heproposed the <code>LLM.int8()</code> no-performance-degradationquantization method. I didn’t read the paper, but looked at the author’s&lt;ahref="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"&gt;blogpost&lt;/a&gt; instead.</p> <h2 id="how-quantization-works">How Quantization works</h2> <p>First, how do we quantize a number? Imagine the following example:you have a data type I5 with values [0, 1, 2, 3, 4, 5] and a data typeI3 with values [0, 2, 4]. We want to quantize I5 vector [3, 1, 2, 3] toI3:</p> <ol type="1"> <li> <p>map from original domain to unit domain [-1, 1]</p> <p>find absolute maximum <code>3 = max(abs([3, 1, 2, 3]))</code>, dividethe vector by 3 to get <code>[1.0, 0.33, 0.66, 1.0]</code></p> </li> <li> <p>map from unit domain [-1, 1] to quantized domain</p> <p>Multiply by the range of the target data type I3, which is 4:<code>[1.0, 0.33, 0.66, 1.0] -&gt; [4.0, 1.33, 2.66, 4.0]</code></p> </li> <li> <p>round to the nearest representable number in this quantizeddomain</p> <p><code>[4.0, 1.33, 2.66, 4.0] -&gt; [4, 0, 2, 4]</code></p> </li> </ol> <p>To dequantize, we reverse this process:</p> <ol type="1"> <li> <p>map from quantized domain to unit domain [-1, 1]</p> <p>Divide the vector by range 4 to get<code>[1.0, 0.0, 0.5, 1.0]</code></p> </li> <li> <p>map from unit domain [-1, 1] to original domain</p> <p>Multiply by the stored absolute maximum 3:<code>[1.0, 0.0, 0.5, 1.0] -&gt; [3.0, 0.0, 1.5, 3.0]</code></p> </li> <li> <p>round to the nearest representable number in the originaldomain</p> <p><code>[3.0, 0.0, 1.5, 3.0] -&gt; [3, 0, 2, 3]</code></p> </li> </ol> <p>We see that our dequantization and quantization led to one error atthe second element.</p> <h2 id="emergent-outlier-features">Emergent Outlier Features</h2> <p>Since we are using the absolute, it is obvious that if we have anoutlier, there will be more errors. So the authors go on to discover thedistribution of outliers. They call such outliers <strong>emergentoutlier features</strong>.</p> <p>The authors explain such outlier features as to select only a singlefeature. At the same time, the other small value part brings theunimportant values down.</p> <p>The authors also found this very interesting emergent phenomenom,which I directly quote below:</p> <blockquote> <p>However, this full “coordination” through a single dimension onlyhappens after the phase shift. Before the phase shift, in transformerswith less than 6.7B parameters some layers disagree which dimension touse for these large features (no prominent outliers).</p> <p>…</p> <p>The phase shift happens around 6.7B, where 100% of layers use thesame dimension for outliers. At this point, a couple of things happenrapidly:</p> <ol start="4" type="1"><li>Transformers become more stable. If you treat the outlier featuresseparately, I believe you can probably run and even train transformersin less than 8-bit precision without degradation in performance.</li></ol> </blockquote> <p>Note the <strong>model perplexity rather than mere model sizedetermines the phase shift</strong>.</p> <h2 id="ggml">&lt;ahref="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml"&gt;GGML&lt;/a&gt;</h2> <p>&lt;ahref="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml"&gt;MLabonne&lt;/a&gt;did a great explanation to how GGML did quantization.</p> <p>GGML quantizes weights in a rather naive way. Basically, it groupsblocks of values and rounds them to a lower precision. Some techniques,like Q4_K_M and Q5_K_M, implement a <strong>higher precision forcritical layers</strong>. In this case, every weight is stored in 4-bitprecision, with the exception of half of the attention.wv andfeed_forward.w2 tensors. Experimentally, this mixed precision proves tobe a good tradeoff between accuracy and resource usage.</p> <p>If we look into the &lt;ahref="https://github.com/ggerganov/ggml/blob/63d8fce8b57c5e97dd1d42b0d7b8c734df1f263c/src/ggml-quants.h"&gt;<code>ggml-quants.h</code>file&lt;/a&gt;, we can see how the blocks are defined. For example,the <code>block_q4_0</code> structure is defined as:</p> <figure class="highlight cpp"><table><tr> <td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td> <td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> QK4_0 32</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> {</span><br><span class="line">    <span class="type">ggml_fp16_t</span> d;          <span class="comment">// delta</span></span><br><span class="line">    <span class="type">uint8_t</span> qs[QK4_0 / <span class="number">2</span>];  <span class="comment">// nibbles / quants</span></span><br><span class="line">} block_q4_0;</span><br></pre></td> </tr></table></figure> <p>In GGML, weights are processed in blocks, each consisting of 32values. For each block, a scale factor (delta) is derived from thelargest weight value. All weights in the block are then scaled,quantized, and packed efficiently for storage (nibbles).</p> <p>Oobabooga, the author of &lt;ahref="https://github.com/oobabooga/text-generation-webui"&gt;textgeneration webui&lt;/a&gt;, did an &lt;ahref="https://oobabooga.github.io/blog/posts/perplexities/"&gt;in-depthsurvey&lt;/a&gt; of inference time, model size, vRAM usage on different typesof quantization formats (GPTQ, GGUF, EXL2, …)</p> </body></html>