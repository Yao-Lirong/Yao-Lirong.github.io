<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>This article &lt;ahref="https://www.leewayhertz.com/parameter-efficient-fine-tuning/"&gt;AGuid to Parameter-efficient Fine-tuning (PEFT)&lt;/a&gt; made a very goodsummary with nice drawings. There are some differences between itsexplanation with the original paper but the basic architecture is allgood.</p> <p><span id="more"></span>&lt;h2 id="prompt-tuning"&gt;Prompt Tuning&lt;/h2&gt;&lt;p&gt;Prefix tuning, prompt tuning, and p-tuning all prepend some vectorsas prefixes / soft prompts to the vector inputs to transformers. Theirgoal is to find a context that steers the language model towardgenerating text that solves a particular task.&lt;/p&gt;&lt;h2 id="adapter"&gt;&lt;ahref=”https://arxiv.org/pdf/1902.00751.pdf”&gt;Adapter&lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Before Adapter, when performing vanilla fine-tuning, a modificationis made to the top layer of the network because the label spaces andlosses for the upstream and downstream tasks differ. Now, Adaptermodules perform more general architectural modifications to re-purpose apretrained network for a downstream task: it injects new layers into theoriginal network.&lt;/p&gt;&lt;p&gt;In standard fine-tuning, the new top-layer and the original weightsare co-trained. In contrast, in Adapter tuning, the parameters of theoriginal network are frozen and therefore may be shared by manytasks.&lt;/p&gt;&lt;/blockquote&gt;<figure><img src="/images/Adapter.png" alt="Adapter Architecture">&lt;figcaption aria-hidden="true"&gt;Adapter Architecture&lt;/figcaption&gt;</figure>&lt;blockquote&gt;&lt;p&gt;Left: We add the adapter module twice to each Transformer layer:after the projection following multiheaded attention and after the twofeed-forward layers.&lt;/p&gt;&lt;p&gt;Right: The adapter consists of a bottleneck which contains fewparameters relative to the attention and feedforward layers in theoriginal model. The adapter also contains a skip-connection. Duringadapter tuning, the green layers are trained on the downstream data,this includes the adapter, the layer normalization parameters, and thefinal classification layer&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Therefore, we can denote the Adapter layer as:&lt;/p&gt;&lt;p&gt;&lt;spanclass=”math display”&gt;<em>y</em> = <em>B</em>(<em>σ</em>(<em>A</em><em>x</em>)) + <em>x</em>&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Define bottleneck dimension &lt;spanclass=”math inline”&gt;<em>r</em>&lt;/span&gt; (to be consistent with LoRA, inthe original Adapter paper, this was &lt;spanclass=”math inline”&gt;<em>m</em>&lt;/span&gt;), so <span class="math inline">$A\in \R^{r \times d}, B \in \R^{d \times r}$</span>. Including biases, weadd a total &lt;spanclass=”math inline”&gt;2<em>d</em><em>r</em> + <em>d</em> + <em>r</em>&lt;/span&gt;parameters with &lt;spanclass=”math inline”&gt;<em>r</em> ≪ <em>d</em>&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;In initialization, we initialize the adapters to a near-identityfunction, so original network is unaffected when training starts.&lt;/p&gt;&lt;p&gt;Adapter achieves similar results with only 1% needed parameters ascompared to full fine-tuning.&lt;/p&gt;&lt;h2 id="lora-low-rank-adaptation-of-llm"&gt;&lt;ahref=”https://arxiv.org/abs/2106.09685”&gt;LoRA: Low-Rank Adaptation ofLLM&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Before LoRA, SOTA techniques have some drawbacks:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Adapter Layers Introduce Inference Latency: Adapter layers have fewparameters, but “large neural networks rely on <strong>hardwareparallelism</strong> to keep the latency low, and adapter layers<strong>have to be processed sequentially</strong>. This makes adifference in the online inference setting where the batch size istypically as small as one.” I actually don’t understand how toparallelize an LLM inference even if without Adapter&lt;/li&gt;&lt;li&gt;Directly Optimizing the Prompt is Hard: Prompt tuning and prefixtuning both require adding a prefix (to either the input vector or tothe hidden vector in middle). In this way, it “<strong>reduces thesequence length available</strong> to process a downstream task, whichwe suspect makes tuning the prompt less performant compared to othermethods.” Its <strong>performance changes non-monotonically in trainableparameters</strong>, too. So it’s hard to optimize.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;LoRA’s architecture is simply a matrix multiplication - Adapterwithout non-linearity or skip connection. So instead of &lt;spanclass=”math inline”&gt;<em>y</em> = <em>B</em>(<em>σ</em>(<em>A</em><em>x</em>)) + <em>x</em>&lt;/span&gt;,we do &lt;spanclass=”math inline”&gt;<em>Δ</em><em>y</em> = <em>B</em><em>A</em><em>x</em>&lt;/span&gt;.There is one fundamental difference though: Adapter is an extra layer<strong>added into the original network</strong>, but LoRA is a layer<strong>added along side with the original network</strong>. That’s whyI have a delta in LoRA’s formula.&lt;/p&gt;&lt;p&gt;In fact, LoRA was specifically designed for low-rank matrixmultiplication decomposition, note for any pretrained weight &lt;spanclass=”math inline”&gt;$W_0 \in \R^{d \times k}$&lt;/span&gt; with &lt;spanclass=”math inline”&gt;<em>y</em> = <em>W</em><sub>0</sub><em>x</em>&lt;/span&gt;and we want to update this weight matrix in fine-tuning: &lt;spanclass=”math inline”&gt;<em>W</em> = <em>W</em><sub>0</sub> + <em>Δ</em><em>W</em>&lt;/span&gt;,we define &lt;spanclass=”math inline”&gt;<em>Δ</em><em>W</em> = <em>B</em><em>A</em>&lt;/span&gt;,so &lt;spanclass=”math inline”&gt;<em>W</em> = <em>W</em><sub>0</sub> + <em>B</em><em>A</em>&lt;/span&gt;.This simple design yields a unimaginable good result: note matrix &lt;spanclass=”math inline”&gt;<em>B</em><em>A</em>&lt;/span&gt; is also of dimension<span class="math inline">$\R^{d \times k}$</span>. Therefore, we candirectly add this result matrix to the original matrix and inferencingwith LoRA, when we add this matrix in, gives us <strong>no additionalinference latency</strong>.&lt;/p&gt;&lt;p&gt;Similar to Adapter, LoRA uses a random Gaussian initialization for Aand zero for B, so &lt;spanclass=”math inline”&gt;<em>Δ</em><em>W</em><em>x</em> = <em>B</em><em>A</em><em>x</em>&lt;/span&gt;is zero at the beginning of training and &lt;spanclass=”math inline”&gt;<em>W</em> = <em>W</em><sub>0</sub> + <em>Δ</em><em>W</em>&lt;/span&gt;yields the same result (identity) as before.&lt;/p&gt;&lt;p&gt;Thanks to the matrix decomposition nature, we can apply LoRA to anymatrix multiplication in principle. However, we found that intransformers, imagine if we have 8 ranks to distribute, when &lt;spanclass=”math inline”&gt;<em>r</em>(<em>W</em><sub><em>q</em></sub>) = <em>r</em>(<em>W</em><sub><em>v</em></sub>) = 4&lt;/span&gt;or &lt;spanclass=”math inline”&gt;<em>r</em>(<em>W</em><sub><em>q</em></sub>) = <em>r</em>(<em>W</em><sub><em>k</em></sub>) = <em>r</em>(<em>W</em><sub><em>v</em></sub>) = <em>r</em>(<em>W</em><sub><em>o</em></sub>) = 2&lt;/span&gt;gives best result. The author also found that the fine-tune matrixactually has a very low rank, so in practice even if we set &lt;spanclass=”math inline”&gt;<em>r</em> = 1&lt;/span&gt; can give good enough results.Go to Section 7 for more interesting experiments they conducted.&lt;/p&gt;&lt;p&gt;In diffusion models, we use LoRA on the stable diffusion’s crossattention layer.&lt;/p&gt;</p> </body></html>