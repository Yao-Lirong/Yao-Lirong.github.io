<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>&lt;ahref="https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers"&gt;PyTorchofficial documentation&lt;/a&gt; explains this concept very briefly and we gointo more detail here.</p> <p><span id="more"></span>&lt;h2 id="what-is-memory-pinning-and-why-we-use-it"&gt;What is memory pinningand why we use it&lt;/h2&gt;&lt;p&gt;First, let’s go back to our OS class and remind what “paged memory”means. Process always wants contiguous memory. The OS uses memory pagingto enable logically contiguous memory that is not physically contiguous.When a process requests memory, OS allocates page frames to the process.These page frames look contiguous to the process, but are actually notso in physical memory. The OS then maps the process’s logical pages tothe physical page frames.&lt;/p&gt;&lt;p&gt;This &lt;ahref=”https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned-host-memory”&gt;Nvidiablog on data transfer&lt;/a&gt; explains what this has to do with GPU: The GPUcannot access data directly from pageable host memory (logicallycontiguous), so when a data transfer from pageable host memory to devicememory is invoked, the CUDA driver must first allocate a temporarypage-locked, or “pinned”, physically contiguous host array, copy thehost data to the pinned array, and then transfer the data from thepinned array to device memory.&lt;/p&gt;<figure>&lt;imgsrc=”https://developer-blogs.nvidia.com/wp-content/uploads/2012/12/pinned-1024x541.jpg”alt=”pinned memory” /&gt;&lt;figcaption aria-hidden="true"&gt;pinned memory&lt;/figcaption&gt;</figure>&lt;p&gt;Therefore, we can avoid the cost of the transfer between pageable andpinned host arrays by directly allocating our host arrays in pinnedmemory.&lt;/p&gt;&lt;p&gt;To my understanding, “directly allocating in pinned memory”corresponds to what’s described in &lt;ahref=”https://pytorch.org/docs/stable/data.html#memory-pinning”&gt;<code>DataLoader</code>’sdocumentation&lt;/a&gt; as:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">loader = DataLoader(dataset, pin_memory=<span class="literal">True</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="pin_memory-and-non_blockingtrue"&gt;<code>pin_memory()</code> and<code>non_blocking=True</code>&lt;/h2&gt;&lt;p&gt;On the other hand, while reading &lt;ahref=”https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/train.py#L126-L128”&gt;nanoGPT’scode&lt;/a&gt;, I saw the following code:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> device_type == <span class="string">'cuda'</span>:</span><br><span class="line"> <span class="comment"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span></span><br><span class="line"> x, y = x.pin_memory().to(device, non_blocking=<span class="literal">True</span>), y.pin_memory().to(device, non_blocking=<span class="literal">True</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;<code>pin_memory</code> is familiar to us while<code>non_blocking</code> is something new. It tells the program that itcan perform other operations on this data while it being trasferred fromhost to device. (so don’t block till transfer is done to start theoperation) This async copy usually speeds things up. This &lt;ahref=”https://stackoverflow.com/a/55564072”&gt;Stack Overflow answer&lt;/a&gt;gives a detaild example of the async part.&lt;/p&gt;&lt;p&gt;Here in the code, we are explicitly calling <code>pin_memory()</code>on something already initialized, which really confused me. Sinceaccording to the above quoted Nvidia blog, “when a data transfer frompageable host memory to cuda device memory is invoked, the CUDA drivermust first allocate a pinned host array, copy the host data to thepinned array, and then transfer the data from the pinned array to devicememory.” That is to say: even without such an explicit<code>pin_memory()</code> call, CUDA will do it for us.&lt;/p&gt;&lt;p&gt;I found &lt;ahref=”https://discuss.pytorch.org/t/when-is-pinning-memory-useful-for-tensors-beyond-dataloaders/103710”&gt;thisexchange on PyTorch’s forum&lt;/a&gt; and &lt;ahref=”https://discuss.pytorch.org/t/how-is-explicit-pin-memory-different-from-just-calling-to-and-let-cuda-handle-it/197422”&gt;alsoasked this question myself&lt;/a&gt;, but didn’t receive a super clear answer.But inferring from what &lt;ahref=”https://discuss.pytorch.org/u/ptrblck/summary”&gt;&lt;spanclass=”citation” data-cites=”ptrblck”&gt;@ptrblck&lt;/span&gt;&lt;/a&gt; said, I thinkit is correct to say that the following two commands are equal in speed(with the first pinning memory implicitly and the second does itexplicitly)&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;<code>t.to("cuda", non_blocking=False)</code>&lt;/li&gt;&lt;li&gt;<code>t.pin_memory().to("cuda", non_blocking=False)</code>&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;and explicit memory pinning call is only useful when used togetherwith <code>to(device, non_blocking=True)</code>&lt;/p&gt;&lt;p&gt;Someone in <a href="https://zhuanlan.zhihu.com/p/477870660" rel="external nofollow noopener" target="_blank">thisZhihu discussion</a> also argues paged memory can be exchanged into diskswap when physical memory is not enough. Explicitly pinning memoryavoids this problem and saves the time of finding these pages in diskfor every query (pinning brings them all out into physical memory). Theposter did not give a reference though.&lt;/p&gt;</p> </body></html>