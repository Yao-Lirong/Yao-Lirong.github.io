<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Mixed-precision training was introduced in &lt;ahref="https://arxiv.org/abs/1710.03740"&gt;Nvidia and Baidu’s research&lt;/a&gt;.&lt;ahref="https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/"&gt;Theblogpost from Nvidia&lt;/a&gt; gave a nice summary of how it’s done and why itworks. Nvidia also gave a more in-depth coverage of the same points intheir tutorial on &lt;ahref="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html"&gt;trainingwith mixed precision&lt;/a&gt;.</p> <p><span id="more"></span>&lt;p&gt;I decided to learn this as I was reading &lt;ahref=”https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/sample.py#L28-L29”&gt;nanoGPT’scode&lt;/a&gt;:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span> <span class="comment"># allow tf32 on matmul</span></span><br><span class="line">torch.backends.cudnn.allow_tf32 = <span class="literal">True</span> <span class="comment"># allow tf32 on cudnn</span></span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="benefits"&gt;Benefits&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;<strong>Decrease the required amount of memory</strong>: FP32 -&gt;FP16&lt;/li&gt;&lt;li&gt;<strong>Shorten the training or inference time</strong>:&lt;ul&gt;&lt;li&gt;<strong>memory bandwith</strong>: half-precision halves the numberof bytes need to be accessed, thus reducing time-spent in memory-limitedoperations&lt;/li&gt;&lt;li&gt;<strong>arithmetic bandwidth</strong>: half-precision arithmatic isinherintely faster than single-precision&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="techniques-in-original-paper"&gt;3 Techniques in OriginalPaper&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;<strong>Accumulation into FP32</strong>: then convert to FP16 forstorage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>Loss Scaling (Gradient Scaling)</strong>: There are fourtypes of tensors encountered when training DNNs: activations, activationgradients, weights, and weight gradients. In experience activations,weights, and weight gradients can be represented with half precision.However, for some networks activation gradients are too small to berepresented in half-precision range (underflow)&lt;/p&gt;&lt;p&gt;Therefore, we need to scale up the activation gradients. This can bedone by simply multiply the training loss with the scale factor. Thisadds just a single multiplication and by the chain rule it ensures thatall the gradients are scaled up at no additional cost.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>FP32 Master Copy of Weights</strong>: Weight gradientmagnitudes are smaller than corresponding weights, especially aftermultiplication with the learning rate. So sometimes no update takesplace.&lt;/p&gt;&lt;p&gt;The remedy is to store the weights in single precision, but docomputation in half precision. Update this master copy of weights aftereach computation.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="more-recent-update"&gt;More Recent Update&lt;/h2&gt;&lt;p&gt;In NVIDIA Ampere GPU architecture, Nvidia introduced &lt;ahref=”https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/”&gt;TensorFloat32(TF32)&lt;/a&gt; with FP32 range (8bit) and FP16 precision (10bit). With theadditional sign bit, it is a novel 19 bit representation of floats.&lt;/p&gt;&lt;p&gt;On an A100, it brings 8x speed up compared to FP32, while FP16/BF16brings 16x speedup. Therefore, mixed-precision training with a native16-bit format (FP16/BF16) is still the fastest option.&lt;/p&gt;&lt;p&gt;TF32 is only exposed as a Tensor Core operation mode, not a type.Internally, all storage in memory and other operations remain completelyin FP32, only convolutions and matrix-multiplications convert theirinputs to TF32 right before multiplication. Therefore, it does notprovide the memory benifits or the native arithmetic speed up brought by16-bit format. Its benefit is that it brings Tensor Core acceleration tosingle-precision DL workloads, without needing any changes to modelscripts.&lt;/p&gt;&lt;p&gt;It needs to be noted that TF32 gives less accurate computationresults. Therefore, PyTorch decided to set<code>toggle torch.backends.matmul.allow_tf32 = False</code> by defaultstarting from &lt;ahref=”https://github.com/huggingface/transformers/issues/16588”&gt;1.12&lt;/a&gt;.Read more about &lt;ahref=”https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere”&gt;PyTorch’sofficial comparison&lt;/a&gt; of speed and numerical stability.&lt;/p&gt;&lt;h2 id="best-practices"&gt;Best Practices&lt;/h2&gt;&lt;p&gt;PyTorch gave some &lt;ahref=”https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/#best-practices”&gt;officialbest practices tips to developers&lt;/a&gt;. Please do check them outhere.&lt;/p&gt;</p> </body></html>