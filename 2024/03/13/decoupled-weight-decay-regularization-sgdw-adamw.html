<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p><a href="https://arxiv.org/abs/1711.05101" rel="external nofollow noopener" target="_blank">The paper Decoupled WeightDecay Regularization</a> mainly introduces AdamW, which is the SOTAoptimizer since then. It investigates why Adam with L2 regularizationsometimes performs worse than SGD with L2 regularization. Itdemonstrates weight decay and L2 regularization, two things peopleusually draw an equal sign, are not the same. And it shows weight decayis the ultimate go-to choice.</p> <p><span id="more"></span>&lt;p&gt;Weight decay and L2 regularization are equivalent in SGD when set L2regularizer <span class="math inline">$\lambda’ = \frac \lambda\alpha$</span>, which is our common practice. The situation is morecomplicated with adaptive gradient algorithms like Adam. Adam performsmuch better with weight decay and the authors propose the new SOTAoptimizer AdamW (Adam with decoupled weight decay). All the conclusionsand main finding can be found in the first 2 pages of the paper andmostly in the Introduction section. I did not read the math.&lt;/p&gt;&lt;p&gt;&lt;ahref=”https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html”&gt;Thisblogpost from Fast.ai&lt;/a&gt; demonstrates how the two methods are differentin code, a bit easier to understand than the paper which doesn’t providea comparision.&lt;/p&gt;&lt;h2 id="weight-decay-in-transformers"&gt;Weight Decay in Transformers&lt;/h2&gt;&lt;p&gt;AdamW is the go-to optimizer for LLM these days. Researchers chose itbecause LLMs are hard to train and rarely overfit, and Adam is the bestchoice when convergence speed is considered (&lt;ahref=”https://www.zhihu.com/question/519307910/answer/2384626354”&gt;reference&lt;/a&gt;).People have also found AdamW usually performs best with big weight decaycoefficient like 0.05 or 0.1 (&lt;ahref=”https://www.zhihu.com/question/536185388”&gt;zhihu question&lt;/a&gt;, &lt;ahref=”https://arxiv.org/abs/2010.11929”&gt;ViT paper: Training &amp;Fine-tuning section&lt;/a&gt;)&lt;/p&gt;&lt;p&gt;When we apply weight decay in transformers, we apply it to all layersexcept LayerNorm and bias layers.&lt;/p&gt;&lt;p&gt;In &lt;ahref=”https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/model.py#L268-L271”&gt;nanoGPT&lt;/a&gt;,Karpathy filtered them out using: <figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span></span><br><span class="line"><span class="comment"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.</span></span><br><span class="line">decay_params = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.dim() &gt;= <span class="number">2</span>]</span><br><span class="line">nodecay_params = [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_dict.items() <span class="keyword">if</span> p.dim() &lt; <span class="number">2</span>]</span><br><span class="line">optim_groups = [</span><br><span class="line"> {<span class="string">'params'</span>: decay_params, <span class="string">'weight_decay'</span>: weight_decay},</span><br><span class="line"> {<span class="string">'params'</span>: nodecay_params, <span class="string">'weight_decay'</span>: <span class="number">0.0</span>}</span><br><span class="line">]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/p&gt;&lt;p&gt;One caveat is that, &lt;ahref=”https://github.com/karpathy/nanoGPT/commit/7fe4a099ad2a4654f96a51c0736ecf347149c34c#diff-fada037ad086638e65c7ae77e3d223963e9afaa26326aab0ea718f4013176e43L282”&gt;inearlier versions&lt;/a&gt;, Karpathy did NOT weight decay embeddings:<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)</span><br><span class="line">…</span><br><span class="line"><span class="keyword">elif</span> pn.endswith(<span class="string">'weight'</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(m, blacklist_weight_modules):</span><br><span class="line"> <span class="comment"># weights of blacklist modules will NOT be weight decayed</span></span><br><span class="line"> no_decay.add(fpn)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/p&gt;&lt;p&gt;I couldn’t find any instruction on whether you should decayembeddings or not when training a transformer, but &lt;ahref=”https://github.com/huggingface/transformers/blob/66ce9593fdb8e340df546ddd0774eb444f17a12c/src/transformers/trainer.py#L979-L988”&gt;HuggingFace’s transformer implementation&lt;/a&gt; also decays embeddings, in linewith Karpathy’s latest implementation. <figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># get_parameter_names(model, name) excludes layers with <code class="language-plaintext highlighter-rouge">name</code></span></span><br><span class="line">decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)</span><br><span class="line">decay_parameters = [name <span class="keyword">for</span> name <span class="keyword">in</span> decay_parameters <span class="keyword">if</span> <span class="string">"bias"</span> <span class="keyword">not</span> <span class="keyword">in</span> name]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/p&gt;</p> </body></html>