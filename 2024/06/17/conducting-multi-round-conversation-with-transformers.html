<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>I was using LLaVA to query in an image how many characters there are.For higher accuracy, I decided to employ Chain of Thought, but struggledto implement it. CoT is conducted through a multiple round conversation.It is easily done in a graphical chat interface but how is it doneinternally with code?</p> <p><span id="more"></span>&lt;h2 id="token-level"&gt;Token Level&lt;/h2&gt;&lt;p&gt;Before diving into instruct / chat model, let’s go to the lowestlevel and think how transformers do generation. Transformer is anautoregressive model: it uses its own output as input for the nextround. Looking at &lt;ahref=”https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L328”&gt;nanoGPT’s<code>generate</code> function&lt;/a&gt;:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span><br><span class="line"><span class="string"> the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line"> idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= <span class="variable language_">self</span>.config.block_size <span class="keyword">else</span> idx[:, -<span class="variable language_">self</span>.config.block_size:]</span><br><span class="line"> logits, _ = <span class="variable language_">self</span>(idx_cond)</span><br><span class="line"> logits = logits[:, -<span class="number">1</span>, :] / temperature</span><br><span class="line"> probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line"> idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line"> idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"> <span class="keyword">return</span> idx</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;If we ignore the details, this for loop is effectively doing:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">token0 = tokenizer(text)</span><br><span class="line">output1 = model(token0)</span><br><span class="line"></span><br><span class="line">token1 = get_resposne(output1) </span><br><span class="line">output2 = model(token0 + token1)</span><br><span class="line"></span><br><span class="line">token2 = get_resposne(output2)</span><br><span class="line">output3 = model(token0 + token1 + token2)</span><br><span class="line">…</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;By writing it out like this, it’s clear that each turn of generation,we feed the previous step input into the model as something new, thoughexactly the same. Therefore, when we call<code>model(token0 + token1)</code>, <strong>we forgot about all theattention we calculated in <code>model(token0)</code></strong> eventhough the attention for <code>token0</code> part is actually completelythe same. This is why people complain transformer inference is slow andthis is where the inference speed-up techniques like KV-cache comesin.&lt;/p&gt;&lt;p&gt;This also reveals that the very popular graph demonstrating thetheory behind transformer’s inference lied (at least to me). Whencalculate &lt;spanclass=”math inline”&gt;<em>y</em><sub><em>i</em> + 1</sub>&lt;/span&gt;, we donot re-use &lt;spanclass=”math inline”&gt;<em>y</em><sub>0</sub>…<em>y</em><sub><em>i</em></sub>&lt;/span&gt;or the attention or the activations in the middle. We just re-feed themback into the model as something completely new.&lt;/p&gt;<figure>&lt;imgsrc=”https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png”alt=”Autoregressive Decoder” /&gt;&lt;figcaption aria-hidden="true"&gt;Autoregressive Decoder&lt;/figcaption&gt;</figure>&lt;h2 id="conversation-level"&gt;Conversation Level&lt;/h2&gt;&lt;p&gt;Chat model is also just a text continuation model except it follows achat template distinguishing which texts are inputted by the user andwhich are generated by the assistant. In the lowest abstraction level -the token level, for each turn, the model outputs one token and usesthat as part of the input in next turn’s generation. One abstractionlevel higher to this conversation level, to do multiple-roundconversation, a chat model similarly outputs one response to one user’sinput and uses that response as a part of the input for next turn’sgeneration. Therefore, to conduct conversation with a chat model, wejust append the model’s response at each turn to its correspondinginput.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">input1 = tokenizer(text1)</span><br><span class="line">output1 = model(input1)</span><br><span class="line"><span class="comment"># output1 contains input1 and model's response 1</span></span><br><span class="line">response1 = get_resposne(output1) </span><br><span class="line">input2 = tokenizer(text2)</span><br><span class="line">output2 = model(input1 + response1 + input2)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;And yes, this means to get <code>output2</code>, we feed<code>input1 + response1</code> both as new to the model, but thisshouldn’t be a concern anymore since we feed each token as newanyway.&lt;/p&gt;&lt;h2 id="get_response"&gt;<code>get_response</code>&lt;/h2&gt;&lt;p&gt;The question now comes to how we should implement<code>get_response</code> to extract the assistant’s response from thetext-continuation model’s output.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Find the indicator (prefix) of the start of assistant’s message:Note when the model doesn’t follow the instruction and failed togenerate such a prefix, this method fails.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">prefix = <span class="string">"[/INST]"</span> <span class="comment"># escape special characters for regex</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line"> output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">answer_idx = [m.end() <span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(prefix, detoked_output)][-<span class="number">1</span>]</span><br><span class="line">answer = detoked_output[answer_idx:]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;<strong>recommended</strong> - Get the substring that is afterthe input (prompt): Hugging Face uses this approach in their &lt;ahref=”https://github.com/huggingface/transformers/blob/1c1aec2ef1d6822fae3ffbb973b4c941f65f4ddf/src/transformers/pipelines/text_generation.py#L369-L387”&gt;TextGenerationPipeline&lt;/a&gt;.There’s a <code>clean_up_tokenization_spaces</code> variable in<code>decode</code> function which defaults to <code>False</code>. (Forwhat it does, see &lt;ahref=”https://discuss.huggingface.co/t/what-does-the-parameter-clean-up-tokenization-spaces-do-in-the-tokenizer-decode-function/17399”&gt;thisdiscussion&lt;/a&gt;) Hugging Face set it to <code>True</code> in both call,but I tried set both to <code>False</code> or one to <code>True</code>the other to <code>False</code>, either can give correct results. Thatsaid, it’s still best to follow what Hugging Face wrote. After all theyknow their codes best.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line"> output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>, </span><br><span class="line"> clean_up_tokenization_spaces=<span class="literal">True</span>)</span><br><span class="line">cutoff = <span class="built_in">len</span>(text_processor.decode(</span><br><span class="line"> inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>],</span><br><span class="line"> skip_special_tokens=<span class="literal">True</span>,</span><br><span class="line"> clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br><span class="line"> ))</span><br><span class="line">answer = detoked_output[cutoff:]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="detours-when-taking-the-recommended-approach"&gt;Detours whenTaking the Recommended Approach&lt;/h2&gt;&lt;p&gt;I had some trouble with this recommended approach at first:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">chat = [</span><br><span class="line"> {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: <span class="string">"&lt;image&gt;\nHow many animated characters are there in this image?"</span>}</span><br><span class="line">]</span><br><span class="line">prompt = text_processor.apply_chat_template(chat, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">inputs = processor(prompt, image, return_tensors=<span class="string">"pt"</span>).to(device)</span><br><span class="line">…</span><br><span class="line">detoked_output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">cutoff = <span class="built_in">len</span>(prompt)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;And <code>cutoff</code> is actually many indexes after the realstarting point of assistant’s response. That is because when we<code>apply_chat_template</code>, we added some special tokens<code>&lt;s&gt; &lt;\s&gt;</code> to indicate the start and end of oneturn of conversation with the assistant, but when we detokenize theoutput, we <code>skip_special_tokens</code> to get the response only andcaused this discrepancy.&lt;/p&gt;&lt;p&gt;I thought at first that this discrepancy comes from LLaVA replaced<code>&lt;image&gt;</code> token with the image embeddings (or<code>pixel_values</code> as Hugging Face calls it) because<code>&lt;image&gt;</code> also disappeared in the<code>detoked_output</code>. However, after reading LLaVA’s paper: &lt;ahref=”https://arxiv.org/abs/2304.08485”&gt;Visual Instruction Tuning&lt;/a&gt;Figure 1: LLaVA network architecture, I realized LLaVA actually puts theimage in front of the text input instead of inserting it in themiddle.&lt;/p&gt;<figure>&lt;img src=”https://arxiv.org/html/2304.08485v2/x1.png”alt=”LLaVA architecture” /&gt;&lt;figcaption aria-hidden="true"&gt;LLaVA architecture&lt;/figcaption&gt;</figure>&lt;p&gt;And <code>&lt;image&gt;</code> disappeared because it’s also aspecial token. However it was not inside the<code>tokenizer.all_special_tokens</code>. Reading the source code oftokenizer, I’m actually not sure how it was added as a special token sowas not able to debug why it’s not in <code>all_special_tokens</code>.For this specific behavior, I submitted &lt;ahref=”https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192”&gt;anissue on Hugging Face forum&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;You can find chat template definition in<code>tokenizer_config.json -&gt; "chat_template"</code>. Also in thisfile, <code>"added_tokens_decoder"</code> attribute defines<code>&lt;image&gt;</code> as a special token.&lt;/p&gt;&lt;h2 id="the-complete-code"&gt;The Complete Code&lt;/h2&gt;&lt;p&gt;I referenced Hugging Face conversation pipeline for &lt;ahref=”https://huggingface.co/docs/transformers/main/conversations#what-happens-inside-the-pipeline”&gt;thegeneral structure&lt;/a&gt; and &lt;ahref=”https://github.com/huggingface/transformers/blob/1c1aec2ef1d6822fae3ffbb973b4c941f65f4ddf/src/transformers/pipelines/text_generation.py#L369-L387”&gt;theresponse extractor&lt;/a&gt;&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">queries = [</span><br><span class="line"> <span class="string">"&lt;image&gt;\nHow many animated characters are there in this image?"</span>,</span><br><span class="line"> <span class="string">"Answer with a single number in decimal format. Give no explanations."</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_response</span>(<span class="params">image</span>):</span><br><span class="line"> chat = []</span><br><span class="line"> <span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line"> chat.append({<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: query})</span><br><span class="line"> prompt = text_processor.apply_chat_template(chat, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line"> inputs = processor(prompt, image, return_tensors=<span class="string">"pt"</span>).to(device)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">with</span> torch.no_grad():</span><br><span class="line"> output = model.generate(**inputs, max_new_tokens = <span class="number">300</span>)</span><br><span class="line"> output = processor.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"> input_ids = inputs[<span class="string">"input_ids"</span>]</span><br><span class="line"> cutoff = <span class="built_in">len</span>(text_processor.decode(</span><br><span class="line"> input_ids[<span class="number">0</span>],</span><br><span class="line"> skip_special_tokens=<span class="literal">True</span>,</span><br><span class="line"> clean_up_tokenization_spaces=<span class="literal">True</span>,</span><br><span class="line"> ))</span><br><span class="line"> answer = output[cutoff:]</span><br><span class="line"> chat.append({<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: answer})</span><br><span class="line"> <span class="keyword">return</span> answer</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="ps"&gt;PS&lt;/h2&gt;&lt;p&gt;As written at the start of this blogpost, it all began from me tryingto do multi-round conversation with a transformer. A web search took meto these discussions (&lt;ahref=”https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/19”&gt;link1&lt;/a&gt;, <a href="https://github.com/salesforce/LAVIS/issues/357" rel="external nofollow noopener" target="_blank">link2</a>). It’s obvious <a href="#Conversation-Level">this acceptedapproach</a> of appending output to previous message causes great wasteof computing resources, which made me realize how transform works &lt;ahref=”#Token-Level”&gt;internally at the lowest level&lt;/a&gt; is itself a wasteof resources.&lt;/p&gt;</p> </body></html>