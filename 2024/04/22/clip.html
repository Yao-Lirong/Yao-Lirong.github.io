<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>CLIP investigates whether it is possible to transfer the success oftask-agnostic web-scale pre-training in NLP to another domain (CV).</p> <p><span id="more"></span>&lt;blockquote&gt;&lt;p&gt;This line of work represents the current pragmatic middle groundbetween learning from a limited amount of supervised “gold-labels” andlearning from practically unlimited amounts of raw text.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="approach"&gt;2 Approach&lt;/h2&gt;&lt;h3 id="advantage-of-natural-language-supervision"&gt;2.1 Advantage ofNatural Language Supervision&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;easy to scale: natural language data amount is huge, much easier toobtain than crowd-sourced labeling&lt;/li&gt;&lt;li&gt;flexible zero-shot transfer: connects image representation tolanguage; different from unsupervised or self-supervised model that islimited to image domain.&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="constructing-dataset"&gt;2.2 Constructing Dataset&lt;/h3&gt;&lt;p&gt;To explore effects of web-scale pre-training, we first build aweb-scale dataset.&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Construct a query list of size 500,000 that contains words occurred&gt;= 100 times in Wikipedia&lt;/li&gt;&lt;li&gt;Search for images of these queries, construct a dataset of 400M(image, text) pair&lt;/li&gt;&lt;li&gt;<strong>Class balance</strong> (yeah that’s the word describing“make each class have the same number of samples so it’s fair”) byincluding 20,000 pairs per query&lt;/li&gt;&lt;/ol&gt;&lt;h3 id="what-to-predict-what-is-the-loss"&gt;2.3 What to Predict? What isthe Loss?&lt;/h3&gt;&lt;p&gt;Previous methods with natural language supervision attempt is aboutpredicting a bag of words (BoW) / phrase n-gram representation oflabels. The authors explore different approaches. This work is all aboutlarge scale pre-training and <strong>scaling</strong>. <strong>Trainingefficiency</strong> is the key to scaling natural language supervision.Authors selected final pre-training method based on efficiency. Theycompared three approaches:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;<strong>Transformer language model (captioning model)</strong>:train a transformer to predict the caption of an image. So this is agenerative task and uses transformer’s loss function. It learns 3 timesslower than the baseline - approach 2.&lt;/li&gt;&lt;li&gt;<strong>A model predicts BoW encoding of the caption</strong>: thiswas used as a simple baseline and authors found approach 1 couldn’t evenbeat this baseline. This approach still tries to <strong>predict theexact words</strong> of the text label, but the order of how wordsappear no longer matters. This is not much easier due to the widevariety of descriptions, comments, and related text that co-occur withimages.&lt;/li&gt;&lt;li&gt;<strong>A contrastive model predicts which text <em>as a whole</em>is paired with which image</strong>: In this way, we decrease the outputspace to only the number of classes we have. We learn 4 times fasterthan the baseline - approach 2.&lt;/li&gt;&lt;/ol&gt;<figure><img src="/images/CLIP_2.png" alt="Accuracy vs #(images processed)">&lt;figcaption aria-hidden="true"&gt;Accuracy vs #(imagesprocessed)&lt;/figcaption&gt;</figure>&lt;p&gt;See Figure 2 for a detailed comparison on <em>accuracy vs. #(imagesfed)</em> of these three models. This illustrates how fast / slow atraining method learns.&lt;/p&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col style="width: 18%" /&gt;&lt;col style="width: 42%" /&gt;&lt;col style="width: 39%" /&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Approach&lt;/th&gt;&lt;th&gt;Output Space&lt;/th&gt;&lt;th&gt;Answer Space: In ideal scenario, what do we choose from?&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;Transformer Language Model&lt;/td&gt;&lt;td&gt;All English sentences (permutation of all English words)&lt;/td&gt;&lt;td&gt;500K queries&lt;/td&gt;&lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;BoW prediction model&lt;/td&gt;&lt;td&gt;Word count bucket of all English sentences (combination of allEnglish words)&lt;/td&gt;&lt;td&gt;500K queries&lt;/td&gt;&lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;Contrastive pairing model&lt;/td&gt;&lt;td&gt;Sentences describing class and labels&lt;/td&gt;&lt;td&gt;<code>batch_size</code> pre-selected queries (32768 in CLIP)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;It’s worth noting that CLIP uses a very large minibatch size of &lt;spanclass=”math inline”&gt;2<sup>15</sup> = 32768&lt;/span&gt;&lt;/p&gt;&lt;h3 id="model-architecture-and-scaling"&gt;2.4 Model Architecture andScaling&lt;/h3&gt;<figure><img src="/images/CLIP_1.png" alt="Summary of CLIP">&lt;figcaption aria-hidden="true"&gt;Summary of CLIP&lt;/figcaption&gt;</figure>&lt;p&gt;Image encoder has two architectures: ResNet-50 and ViT&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;Note:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;<code>d_e</code> represents multi-modal embedding space.&lt;/li&gt;&lt;li&gt;the temperature parameter &lt;spanclass=”math inline”&gt;<em>τ</em>&lt;/span&gt; is directly optimized as alog-parameterized multiplicative scalar to avoid turning as ahyper-parameter. &lt;ahref=”https://github.com/openai/CLIP/blob/a1d071733d7111c9c014f024669f959182114e33/clip/model.py#L367-L368”&gt;implementationin original release&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The authors train CLIP from scratch without initializing the imageencoder with ImageNet weights or the text encoder with pre-trainedweights.&lt;/p&gt;&lt;p&gt;This section also describes how to scale the text encoder and how toscale both kinds of image encoder.&lt;/p&gt;&lt;h2 id="experiments"&gt;3 Experiments&lt;/h2&gt;&lt;p&gt;Authors conducted experiments on 36 different datasets.&lt;/p&gt;&lt;h3 id="zero-shot-transfer"&gt;3.1 Zero-Shot Transfer&lt;/h3&gt;&lt;p&gt;Authors wanted to experiment on zero-shot transfer ability because ofthe ability demonstrated in language models. The following is the mostexciting sentence to me in this paper. I think it explains a lot oflarge-scale design choices by OpenAI team. Did this paper inspire Ilyato go all the way down the path of scaling?&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Our focus on studying zero-shot transfer as an evaluation of tasklearning is inspired by work demonstrating task learning in the field ofNLP. To our knowledge Liu et al. (2018) first identified task learningas an “unexpected side-effect” when a language model trained to generateWikipedia articles learned to reliably transliterate names betweenlanguages.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Authors explain in detail how we do zero-shot classification and givean interpretation to the pipeline. I wrote the previous “output space”and “answer space” thing based on this interpretation.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The cosine similarity of these embeddings is then calculated, scaledby a temperature parameter <span class="math inline"><em>τ</em></span> ,and normalized into a probability distribution via a softmax. Note thatthis prediction layer is a multinomial logistic regression classifierwith L2-normalized inputs, L2-normalized weights, no bias, andtemperature scaling. When interpreted this way, the image encoder is thecomputer vision backbone which computes a feature representation for theimage and the text encoder is a hypernetwork which generates the weightsof a linear classifier based on the text specifying the visual conceptsthat the classes represent. Continuing with this interpretation, everystep of CLIP pre-training can be viewed as optimizing the performance ofa randomly created proxy to a computer vision dataset which contains 1example per class and has 32,768 total classes defined via naturallanguage descriptions.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;<strong>prompt engineering and ensembling </strong>&lt;/p&gt;&lt;p&gt;Text in our training data is usually a sentence, but text in testdata is just a one word label. To bridge this gap, we use some prompttemplate.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;default: <code>A photo of a {label}</code>&lt;/li&gt;&lt;li&gt;on several fine-grained image classification datasets, it’s helpfulto specify the category:<code>A photo of a {label}, a type of pet</code> or<code>a satellite photo of a {label}</code>&lt;/li&gt;&lt;li&gt;ensembling several different prompts improve performance: usedifferent context prompts such as <code>A photo of a big {label}</code>and <code>A photo of a small {label}</code>. Authors construct theensemble over the embedding space instead of probability space. In thisway, they cache a single set of averaged text embedding so compute costdoesn’t increase in amortized time.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;<strong>scaling law</strong>&lt;/p&gt;<figure>&lt;img src=”/images/CLIP_9.png”alt=”Zero-shot CLIP scales wrt model compute” /&gt;&lt;figcaption aria-hidden="true"&gt;Zero-shot CLIP scales wrt modelcompute&lt;/figcaption&gt;</figure>&lt;p&gt;Scaling law is the law that empirically shows that performance ispredictable as a function of important quantities such as trainingcompute and dataset size.&lt;/p&gt;&lt;p&gt;On 36 different datasets, ResNet CLIP’s average zero-shot error iswell modeled by a log-log linear scaling trend. However, performance onindividual evaluations is much more varied despite the smooth overalltrend. Authors did not report ViT CLIP scaling results.&lt;/p&gt;&lt;h3 id="representation-learning"&gt;3.2 Representation Learning&lt;/h3&gt;&lt;p&gt;To use CLIP as a representation of the image, there are two commonapproaches:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Fitting a linear classifier on a representation extracted from themodel&lt;/li&gt;&lt;li&gt;End-to-end fine-tuning of the model.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Fine-tuning increases flexibility, and prior work has convincinglydemonstrated that fine-tuning outperforms linear classification on mostimage classification datasets. However, OpenAI chooses to use linearclassifier to measure CLIP performance for the following reasons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;the more official reason: we chose it because it’s weak andtherefore better shows how dataset-agnostic CLIP is&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Our work is focused on developing a high-performing task anddataset-agnostic pre-training approach. Fine-tuning, because it adaptsrepresentations to each dataset during the fine-tuning phase, cancompensate for and potentially mask failures to learn general and robustrepresentations during the pre-training phase. Linear classifiers,because of their limited flexibility, instead highlight these failuresand provide clear feedback during development&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;the more practical reason:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Fine-tuning opens up a much larger design and hyper-parameter space,which makes it difficult to fairly evaluate and computationallyexpensive. By comparison, linear classifiers require minimalhyper-parameter tuning and have standardized implementations andevaluation procedures.&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;bonus reason:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Linear classifier has the added benefit of being very similar to theapproach used for its zero-shot classifiers which enables extensivecomparisons and analysis&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;<strong>approach</strong>: Appendix A.3 provides a full guideline oftraining such a linear classifier, including details on hyper-parametersearch, solver method, and train-valid-test split. Notably, the input tothe Logistic Regression is the image embedding (output of the imageencoder <code>I_f</code>), not the multi-modal embedding (imageembedding that went through the multi-modal linear projection)&lt;/p&gt;&lt;p&gt;<strong>results</strong>: when comparing to other models of similarcompute requirement, small CLIP have wins and loses. However, CLIPscales very well and the largest model achieves both SOTA score andcompute efficiency.&lt;/p&gt;&lt;p&gt;<strong>ViT vs ResNet</strong>: The authors found CLIP ViT is about3x more compute efficient than CLIP ResNet. This is aligned with ViTpaper’s finding&lt;/p&gt;&lt;p&gt;<strong>Out-of-Domain Performance and Natural DistributionShift</strong>: Researchers often find models exceeding human onImageNet test set can still make simple mistakes on other test data andscore much lower than human. A common explanation is these models areadept at finding patterns within dataset, so improve in-distributionperformance. However many of these patterns are spurious and do not holdfor other distributions and result in large drops in performance onother datasets.&lt;/p&gt;&lt;p&gt;Most of the studies that reach the above explanation limited theirevaluation model to those trained on ImageNet. Therefore, the authorswant to know to what degree are these failures attributable to deeplearning, ImageNet, or some combination of the two? They explore this byevaluating ImageNet models on natural distribution shifted dataset.&lt;/p&gt;&lt;p&gt;Natural distribution shift means testing trained models on data thatis different in e.g. image style, image blurriness, geographic location,and camera operation (<em>Hendrycks et al. The many faces ofrobustness</em>). “Natural” is used to make a distinction from syntheticdistribution shift made through style-transferred or adversariallygenerated.&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Authors found CLIP perform much better on these natural distributionshifted dataset.&lt;/li&gt;&lt;li&gt;However, this doesn’t necessarily mean supervised learning onImageNet causes a robustness gap. Other details of CLIP, such as itslarge and diverse pre-training dataset or use of natural languagesupervision could also produce robust models.&lt;/li&gt;&lt;li&gt;Therefore, OpenAI measured how the performance of CLIP models changeafter adapting to the ImageNet distribution via an L2 regularizedlogistic regression classifier fit to CLIP features on the ImageNettraining set. This improved accuracy on ImageNet by 9.2% to 85.4%, butaverage accuracy under distribution shift slightly decreases.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To me this doesn’t say much. If you fine-tune (or fit a linearclassifier) to a specific dataset, of course you’d expect its behaviorto be bad on some other dataset. But on the contrary, thesenatural-distribution-shifted dataset is not that different fromImageNet. Yes, there are some animations / sketches, but most are justsome more pictures of that class. And CLIP with an ImageNet linear headcannot get them right. I guess what the authors want to say is thatImageNet is not just A arbitrary dataset, but has almost become amachine learning benchmark dataset. It is supposed to be general becauseall models train on it and these models will be deployed to all sorts ofscenario.&lt;/p&gt;&lt;p&gt;The authors didn’t go far to attack the generality of ImageNet oreven draw any conclusion on why fitting an ImageNet classification headhurts natural distribution shift performance. The authors just prompt tocaution that though prior work has also pre-trained models ondistributions other than ImageNet, it is common to study and releasemodels only after they have been fine-tuned to ImageNet. And it would bewise to also study the models pre-trained on distributions other thanImageNet.&lt;/p&gt;&lt;p&gt;<strong>Results:</strong> Taken together, these results suggest thatthe recent shift towards large-scale task and dataset agnosticpre-training combined with a reorientation towards zero-shot andfew-shot benchmarking on broad evaluation suites promotes thedevelopment of more robust systems and provides a more accurateassessment of performance.&lt;/p&gt;&lt;h2 id="data-overlap-analysis"&gt;5 Data Overlap Analysis&lt;/h2&gt;&lt;p&gt;A concern with pre-training on a very large internet dataset isunintentional overlap with downstream evals. One option to prevent thisis to identify and remove all duplicates before training a model. Whilethis guarantees reporting true hold-out performance, it requires knowingall possible data which a model might be evaluated on ahead of time.This has the downside of limiting the scope of benchmarking andanalysis.&lt;/p&gt;&lt;p&gt;Therefore, OpenAI instead built a duplicate detector, document howmuch overlap occurs, and run experiments on dataset with and withoutthese overlaps to measure how performance changes due to these overlaps.So instead of simply removing them, they record performance of beforeand after removing them.&lt;/p&gt;&lt;p&gt;They found that there is a median overlap of 2.2% and an averageoverlap of 3.2%. Due to this small amount of overlap, overall accuracyis rarely shifted by more than 0.1% with only 7 datasets above thisthreshold.&lt;/p&gt;&lt;p&gt;It would be useful if OpenAI also releases their duplicate detectormodel. Appendix C discusses it in more details but it doesn’t seem likeOpenAI ever released it.&lt;/p&gt;&lt;h2 id="limitations"&gt;6 Limitations&lt;/h2&gt;&lt;p&gt;<strong>Performance</strong>:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;CLIP cannot beat dataset-specific trained &amp; designed models:CLIP zero-shot performs better than a pre-trained ResNet-50 feature + alinear classifier, but on most datasets, CLIP is well below the SOTA forthat specific dataset.&lt;/li&gt;&lt;li&gt;zero-shot CLIP still generalizes poorly to data that is trulyout-of-distribution for it: CLIP simply has a super large domain, notreally a general model. For example, MNIST digits are not at all in itsweb-scraped huge dataset, so CLIP does surprisingly bad on this supersimple dataset.&lt;/li&gt;&lt;li&gt;CLIP is limited to “choosing”: CLIP cannot just take in a pictureand spit out its class. You need to give CLIP a range to choose from.CLIP is based on “choosing”, not “generating” (image captioningmodel)&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;<strong>Training Methodology</strong>:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;In training time, CLIP repeatedly queried performance on fullvalidation sets to guide optimization. These validation sets often havethousands of examples, which is unrealistic for true zero-shotscenarios. On the contrary, LLM in training time doesn’t do this(?)&lt;/li&gt;&lt;li&gt;Training dataset comes from Internet. Its image-text pairs areunfiltered and uncurated and result in CLIP models learning many socialbiases.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;<strong>Supervision with Natural Language</strong>:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Many complex tasks and visual concepts can be difficult to specifyjust through text.&lt;/li&gt;&lt;li&gt;Actual training examples are undeniably useful but CLIP does notoptimize for few-shot performance directly. In our work, we fall back tofitting linear classifiers on top of CLIP’s features. This results in acounter-intuitive drop in performance when transitioning from azero-shot to a few-shot setting.&lt;/li&gt;&lt;/ol&gt;&lt;h2 id="broader-impacts"&gt;7 Broader Impacts&lt;/h2&gt;&lt;p&gt;In this section, the authors mainly introduces the bias exists inCLIP and what kind of surveillance it can be used for.&lt;/p&gt;&lt;p&gt;Nothing too interesting, but they discussed how tweaking the categorysystem can improve model’s performance. This reminds me of what I did inXiaomi’s oversea app store tagging project, where I added new categoryand modified existing category’s definition to improve thecos-similarity based zero-shot classification model performance.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Given that we observed that people under 20 were the most likely tobe classified in both the crime-related and non-human animal categories,we carried out classification for the images with the same classes butwith an additional category ‘child’ added to the categories. We foundthat this drastically reduced the number of images of people under 20classified in either crime-related categories or non-human animalcategories (Table 7). This points to how class design has the potentialto be a key factor determining both the model performance and theunwanted biases or behavior the model may exhibit&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The authors then go on to conclude that&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Decisions about things like class design are a key determiner notonly of model performance, but also of how and in what contexts modelbiases manifest&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="takeaways"&gt;Takeaways&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Data is still the king in ML. It is possible to transfer thesuccess of task-agnostic web-scale pre-training in NLP to CV.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The key to scaling &amp; training efficiency is how compact youroutput space is (word permutation - &gt; word combination -&gt;<code>batch_size</code>)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;We can use prompt ensembling to improve CLIP’sperformance.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;To use CLIP as the feature extractor and put a linear classifieron top of it, we use the image embedding (image encoder’s output), nothe multi-modal embedding (image embedding went through the multi-modallinear projection);&lt;/p&gt;&lt;p&gt;On the other hand, for zero-shot classification, you use multi-modalembedding, the same as the training process except now you only have oneimage and calculate the cos similarity with all class names.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Decisions about things like class design are a key determiner notonly of model performance, but also of how and in what contexts modelbiases manifest&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</p> </body></html>