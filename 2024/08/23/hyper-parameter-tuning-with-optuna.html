<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>After self-implementing a grid-search but having a horrible timewriting pyplot visualizing the result, I finally decided to find anexisting tool to do the HP tuning for me.</p> <p><span id="more"></span>&lt;p&gt;There are two popular HP tuning framework&lt;/p&gt;&lt;ul&gt;&lt;li&gt;<a href="https://docs.ray.io/en/latest/tune/index.html" rel="external nofollow noopener" target="_blank">RayTune</a>:almost industry standard&lt;/li&gt;&lt;li&gt;<a href="https://optuna.org/" rel="external nofollow noopener" target="_blank">Optuna</a>: user friendly, requiresminimal modification to original code&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There’s also &lt;ahref=”https://github.com/skorch-dev/skorch”&gt;skorch&lt;/a&gt; integratingscikit-learn and pytorch, so you can use &lt;ahref=”https://skorch.readthedocs.io/en/v1.0.0/user/quickstart.html#grid-search”&gt;sklearn<code>GridSearchCV</code>&lt;/a&gt;. For our simple task, we will go with<code>Optuna</code>.&lt;/p&gt;&lt;h2 id="getting-started"&gt;Getting Started&lt;/h2&gt;&lt;p&gt;To get Optuna running, you just need to add 4 lines in your traininglogic and a few more lines to start its search. In training logic:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">image_datasets, lr, weight_decay, num_epochs, trial : optuna.trial.Trial=<span class="literal">None</span></span>):</span><br><span class="line"> optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)</span><br><span class="line"> <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line"> model.train()</span><br><span class="line"> <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">"train"</span>]:</span><br><span class="line"><span class="comment"># Training Logic</span></span><br><span class="line"> model.<span class="built_in">eval</span>()</span><br><span class="line"> <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[<span class="string">"val"</span>]:</span><br><span class="line"> running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line"> <span class="comment"># Eval Logic</span></span><br><span class="line"> epoch_loss = running_loss / dataset_sizes[<span class="string">"val"</span>]</span><br><span class="line"> <span class="keyword">if</span> epoch_acc &gt; best_acc <span class="keyword">or</span> (epoch_acc == best_acc <span class="keyword">and</span> epoch_loss &lt; best_loss):</span><br><span class="line"> best_acc, best_loss = epoch_acc, epoch_loss</span><br><span class="line"> <span class="string">""" OPTUNA CODE GOES HERE:</span></span><br><span class="line"><span class="string"> For each epoch, you should report value of a user-defined factor. </span></span><br><span class="line"><span class="string"> Optuna uses this factor alone to determine whether to prune out </span></span><br><span class="line"><span class="string"> this trial at current epoch step. Your objective value returned </span></span><br><span class="line"><span class="string"> has nothing to do with pruning.</span></span><br><span class="line"><span class="string"> Read for more at: https://optuna.readthedocs.io/en/v3.6.1/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.report</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="keyword">if</span> trial <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"> trial.report(epoch_loss, epoch)</span><br><span class="line"> <span class="keyword">if</span> trial.should_prune():</span><br><span class="line"> <span class="keyword">raise</span> optuna.exceptions.TrialPruned()</span><br><span class="line"> <span class="keyword">return</span> best_loss</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The following code shows how to set the search space and start thesearch.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">optuna_objective</span>(<span class="params">trial : optuna.trial.Trial</span>):</span><br><span class="line"> <span class="string">""" Define a custom objective function we want to optimize on. </span></span><br><span class="line"><span class="string"> This function returns value of the criteria you want to finally evaluate your model on. </span></span><br><span class="line"><span class="string"> i.e. how you compare different models. The best model should have the best value of this objective.</span></span><br><span class="line"><span class="string"> If you say the best model should have highest training accuracy at the last epoch, then return training accuracy at the last epoch here. In our example, we think the best model should have the best <code class="language-plaintext highlighter-rouge">best_loss</code>, where a model's <code class="language-plaintext highlighter-rouge">best_loss</code> is this model's lowest validation loss across all epochs.</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> image_datasets = prepare_data()</span><br><span class="line"> lr = trial.suggest_float(<span class="string">"lr"</span>, <span class="number">1e-6</span>, <span class="number">1e-1</span>, log=<span class="literal">True</span>)</span><br><span class="line"> weight_decay = trial.suggest_float(<span class="string">"weight_decay"</span>, <span class="number">1e-6</span>, <span class="number">1e-1</span>, log=<span class="literal">True</span>)</span><br><span class="line"> loss = train_model(image_datasets, lr, weight_decay, <span class="number">15</span>, trial)</span><br><span class="line"> <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <strong>name</strong> == <span class="string">"<strong>main</strong>"</span>:</span><br><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> Create a study called <code class="language-plaintext highlighter-rouge">plant_144</code> where we minimize the objective passed in.</span></span><br><span class="line"><span class="string"> Start the search. The search ends when we finish 10 trials or spend 3 hours. </span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> study = optuna.create_study(</span><br><span class="line"> direction=<span class="string">"minimize"</span>,</span><br><span class="line"> study_name=<span class="string">"plant_144"</span>)</span><br><span class="line"> study.optimize(optuna_objective, n_trials=<span class="number">10</span>, timeout=<span class="number">3</span><em><span class="number">60</span></em><span class="number">60</span>)</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">" Objective Value: "</span>, study.best_trial.value)</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">" Params: "</span>)</span><br><span class="line"> <span class="keyword">for</span> key, value <span class="keyword">in</span> study.best_trial.params.items():</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">f" <span class="subst">{key}</span>: <span class="subst">{value}</span>"</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The above example was adapted from &lt;ahref=”https://github.com/optuna/optuna-examples/blob/ecc3e4282161f3cece1dc26d95f4186e3905e497/pytorch/pytorch_simple.py”&gt;Optuna’sPyTorch starting example&lt;/a&gt;. For more reporting printout statements,check the original example.&lt;/p&gt;&lt;h2 id="saving-study-and-board-visualization"&gt;Saving Study and BoardVisualization&lt;/h2&gt;&lt;p&gt;In addition to printing out all the info to the console and losingthem from memory after this python script finishes, we can save them inthe form of an RDB (Relational Database, or just database as mostdatabases are RDB). To do this, we pass a <em>database URL</em> to the<code>storage</code> argument&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">study = optuna.create_study(</span><br><span class="line"> direction=<span class="string">"minimize"</span>,</span><br><span class="line"> study_name=<span class="string">"plant_144"</span>,</span><br><span class="line"> storage=<span class="string">"sqlite:///db.sqlite3"</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;You can now Ctrl+C stop this search at anytime and resume it byrunning the same code again.&lt;/p&gt;&lt;p&gt;Database exposes itself as a server in machines. Therefore, to accessit (even in local machine), we use Database URL. Just like to access awebpage online, we use an HTTPS url. In our example here, the historywill be stored in a file called <code>db.sqlite3</code> under currentdirectory.&lt;/p&gt;&lt;p&gt;This file is a general database and can store study other than theone called <code>plant_144</code>. You can store another study insideit.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">study = optuna.create_study(</span><br><span class="line"> direction=<span class="string">"maximize"</span>,</span><br><span class="line"> study_name=<span class="string">"plant_8"</span>,</span><br><span class="line"> storage=<span class="string">"sqlite:///db.sqlite3"</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;For me this code just worked without having to install SQLite DB.This is probably because it comes with my Ubuntu but I have no idea.Check official tutorial &lt;ahref=”https://optuna.readthedocs.io/en/v3.6.1/tutorial/20_recipes/001_rdb.html”&gt;Saving/ResumingStudy&lt;/a&gt; for more on saving and loading.&lt;/p&gt;&lt;p&gt;You can now visualize the search history, each parameter’simportance, etc. with &lt;ahref=”https://github.com/optuna/optuna-dashboard”&gt;optuna-dashboard&lt;/a&gt;&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">optuna-dashboard sqlite:///db.sqlite3</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure><figure><img src="/images/optuna-dashboard.png" alt="optuna-dashboard">&lt;figcaption aria-hidden="true"&gt;optuna-dashboard&lt;/figcaption&gt;</figure>&lt;h2 id="multi-gpu-parallelism-support"&gt;Multi-GPU ParallelismSupport&lt;/h2&gt;&lt;p&gt;<a href="https://stackoverflow.com/a/62564488" rel="external nofollow noopener" target="_blank">roman’s Stack Overflowanswer</a> provides a very simple way to do multi-GPU tuning byutilizing Optuna’s resume feature. To do so, create a study by followingthe previous code. Then modify your code now to resume instead ofstarting a new study.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> <strong>name</strong> == <span class="string">'<strong>main</strong>'</span>:</span><br><span class="line"> study = optuna.load_study(study_name=<span class="string">'plant_144'</span>, storage=<span class="string">'sqlite:///db.sqlite3'</span>)</span><br><span class="line"> study.optimize(objective, n_trials=<span class="number">100</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;and simply start “resume” this study on different available GPUs&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">CUDA_VISIBLE_DEVICES=3 <span class="built_in">nohup</span> python optuna.py &gt; log3.txt 2&gt;&amp;1 &amp;</span><br><span class="line">CUDA_VISIBLE_DEVICES=5 <span class="built_in">nohup</span> python optuna.py &gt; log5.txt 2&gt;&amp;1 &amp;</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The history from both processes will be stored under the study called<code>plant_144</code> in file <code>db.sqlite3</code>.&lt;/p&gt;&lt;p&gt;For more information on parallelizing on multiple gpu, check officialguide: &lt;ahref=”https://optuna.readthedocs.io/en/v3.6.1/tutorial/10_key_features/004_distributed.html”&gt;EasyParallelization&lt;/a&gt;&lt;/p&gt;&lt;h2 id="some-complaints"&gt;Some Complaints&lt;/h2&gt;&lt;p&gt;In its visualization, Optuna doesn’t provide an option to filter outthe “bad” trial runs, making the scale of all graph ridiculous andusually of no information.&lt;/p&gt;</p> </body></html>