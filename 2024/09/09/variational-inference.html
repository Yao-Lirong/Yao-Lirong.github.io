<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="probabilistic-latent-variable-models">Probabilistic LatentVariable Models</h2> <p>The two general forms of probabilistic models are:</p> <ul> <li> <span class="math inline"><em>p</em>(<em>x</em>)</span>: a typicalprobabilistic distribution. In this model, we call &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; the query.</li> <li>&lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em>‚ÄÖ‚à£‚ÄÖ<em>x</em>)&lt;/span&gt;: aconditional probabilistic distribution. In this model, we cal &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; the evidence and &lt;spanclass="math inline"&gt;<em>y</em>&lt;/span&gt; the query.</li> </ul> <p>Latent variable models are models that have variables other than thequery and the evidence.</p> <ul> <li> <p>&lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)‚ÄÑ=‚ÄÑ‚àë<sub><em>z</em></sub><em>p</em>(<em>x</em>‚ÄÖ‚à£‚ÄÖ<em>z</em>)‚ÄÖ<em>p</em>(<em>z</em>)&lt;/span&gt;‚Äã</p> <p>A classic latent variable model of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)&lt;/span&gt; is the mixture model,where <span class="math inline"><em>p</em>(<em>x</em>)</span> isactually a mixture of several different probabilistic model. Forexample, in the following graph, &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; is a discrete variablerepresenting which class a datapoint belongs to and is represented bydifferent colors here. &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>‚ÄÖ‚à£‚ÄÖ<em>z</em>)&lt;/span&gt; is eachclass‚Äôs probability distribution, where in this case can each be modeledby a Gaussian. And &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)&lt;/span&gt;‚Äã when we observe it, isjust a bunch of uncolored datapoints and is hard to fit a distributionon it. However, we can see it‚Äôs roughly spread in 3 clusters so weintroduce the latent variable representing class and we can now well fita Gaussian mixture model on it (a mixture of 3 Gaussians)</p> <figure>&lt;img src="/images/Mixture-Gaussian-Distribution.png"alt="Gaussian Mixture Model" /&gt;<figcaption aria-hidden="true">Gaussian Mixture Model</figcaption></figure> </li> <li> <p>&lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em>‚ÄÖ‚à£‚ÄÖ<em>x</em>)‚ÄÑ=‚ÄÑ‚àë<sub><em>z</em></sub><em>p</em>(<em>y</em>‚ÄÖ‚à£‚ÄÖ<em>x</em>,‚ÄÜ<em>z</em>)‚ÄÜ<em>p</em>(<em>z</em>)&lt;/span&gt;or &lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em>‚ÄÖ‚à£‚ÄÖ<em>x</em>)‚ÄÑ=‚ÄÑ‚àë<sub><em>z</em></sub><em>p</em>(<em>y</em>‚ÄÖ‚à£‚ÄÖ<em>z</em>)‚ÄÜ<em>p</em>(<em>z</em>‚ÄÖ‚à£‚ÄÖ<em>x</em>)&lt;/span&gt;:the conditional probability is a bit more free. You can decompose andmodel it using <span class="math inline"><em>z</em></span>‚Äã as youlike.</p> <p>An example of latent conditional model is the mixture densitynetwork, which we use in RL‚Äôs imitation learning to deal withmulti-modal situations each requiring a different distribution.</p> </li> </ul> <h3 id="latent-variable-models-in-general">Latent Variable Models inGeneral</h3> <p>When we use latent variable models, it means we want to<strong>decompose a complicated distribution into several simple / easydistributions</strong>. By <strong>complicated</strong>, we mean it‚Äôsnot possible to write it in a well-defined distribution. By<strong>simple / easy</strong>, we mean we can write it as awell-defined parametrized distribution, where the parameters can becomplex, but the distribution itself is easy to write (a Gaussian ofjust mean and sigma, or as a Bernoulli with just one variable, etc.)&lt;spanclass="math display"&gt;<em>p</em>(<em>x</em>)‚ÄÑ=‚ÄÑ‚à´<em>p</em>(<em>x</em>‚ÄÖ‚à£‚ÄÖ<em>z</em>)<em>p</em>(<em>z</em>)<em>d</em><em>z</em>&lt;/span&gt;</p> <ul> <li> <span class="math inline"><em>p</em>(<em>z</em>)</span> is an ‚Äúeasy‚Äùprior we choose. For example a Gaussian, a categorical distribution,etc.</li> <li> <span class="math inline"><em>p</em>(<em>x</em>‚ÄÖ‚à£‚ÄÖ<em>z</em>)</span>should also be an easy distribution, like a Gaussian: $ p(x z) =(<em>{nn}(z), </em>{nn}(z))$ even though the mapping from &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; to the actual parameters ofGaussian can be complex, where in this case we have to model the mappingthrough a neural network and this mapping is the learnable part.</li> <li> <span class="math inline"><em>p</em>(<em>x</em>)</span> iscomplicated, not possible to write out as any well-defined distribution.Therefore, we decompose it into the two parts above that are<strong>easy to parametrize as a probability distribution and learn theparameters inside the distribution</strong>.</li> </ul> <p>Generative models are not equal to latent variable models. We usuallymodel generative models as latent variable ones because generativemodels are usually complex probability distributions and we can make iteasier by introducing one or more latent variable.</p> <h3 id="how-to-train-a-latent-variable-model">How to Train a LatentVariable Model</h3> <p>Given dataset &lt;spanclass="math inline"&gt;ùíü‚ÄÑ=‚ÄÑ{<em>x</em><sub>1</sub>,‚ÄÜ<em>x</em><sub>2</sub>,‚ÄÜ‚Ä¶,‚ÄÜ<em>x</em><sub><em>N</em></sub>}&lt;/span&gt;,to fit a typical probabilistic model &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em>)&lt;/span&gt;,we use the maximum likelihood estimation: <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N \sum_i \logp_\theta(x_i)$$</span> In the latent variable model set up, we can substitute thedefinition in and an MLE would look like <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N\sum_i \log \left( \int p_\theta(x_i \mid z) p(z) dz \right)$$</span> &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em>‚ÄÖ‚à£‚ÄÖ<em>z</em>)&lt;/span&gt;and <span class="math inline"><em>p</em>(<em>z</em>)</span> aredistributions of our choices, but this integral is still intractablewhen <span class="math inline"><em>z</em></span> is continuous. So nowit‚Äôs time to do some math tricks.</p> <h2 id="variational-inference">Variational Inference</h2> <h3 id="variational-approximation">Variational Approximation</h3> <p>First, we construct an easy / simple probability distribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to approximate &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;- the posterior distribution specific to datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;. By easy weagain mean it is easy to parametrize (a Gaussian, a Bernoulli, etc.)</p> <p>We will show that by introducing this &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;,we can actually construct a lower bound of &lt;spanclass="math inline"&gt;log‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.What‚Äôs good with this lower bound? Later on, we will also prove thisbound is sufficiently tight, so as we push up the value of this lowerbound, we push up the value of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;which is exactly what we want.</p> <p><span class="math display">$$\begin{align}\log p(x_{i})&amp;= \log\int_{z}p(x_{i}|z)p(z)\\&amp;= \log\int_{z}p(x_{i}|z)p(z) \frac{q_i(z)}{q_i(z)} \\&amp;= \log \mathbb E_{z\sim q_{i}(z)} \left[\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] \\&amp;\geq \mathbb E_{z\sim q_{i}(z)}\left[\log\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] &amp;\text{\# Jensen'sInequality} \\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] - \mathbb E_{z\sim q_{i}(z)} \left[ \log {q_{i}(z)}\right]\\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)= \mathcal L_i(p, q_i)\end{align}$$</span> Recall <span class="math inline"><em>p</em>(<em>x</em>)</span>is a difficult probability distribution, so we decompose it into twoeasy distributions &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, and use an easydistribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to approximate the posterior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.Now the good thing is: everything here is tractable: for the first term,we can fix a &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;of our choice (recall &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; is adistribution we constructed), sample some &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;, and evaluate the expression. Forthe second term, we notice it is just the entropy of a distribution andfor simple distributions (we constructed &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to besimple), it has a closed form (even if it doesn‚Äôt, you can simply sampleand evaluate)</p> <p>We call the final lower bound we derived here the <strong>variancelower bound</strong> or <strong>evidence lower bound (ELBO)</strong>.<span class="math display">$$\begin{align}\log p(x_{i})&amp;\geq \mathcal L_i(p, q_i) \\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)\end{align}$$</span> ### Effect of Pushing Up ELBO (Intuitively)</p> <p>Assume our <span class="math inline"><em>p</em>(‚ãÖ)</span>‚Äã is a fixedvalue, what does pushing up ELBO mean? Here, we give out an intuitiveexplanation. First, we look at <strong>the first term</strong> with thetwo log combined. <span class="math display">$$\begin{align} &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] \\= &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i},z) \right]\end{align}$$</span> To maximize this value, we just have to find a distribution of<span class="math inline"><em>z</em></span>, inside which we have thelargest value of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)&lt;/span&gt;.Therefore, we want <span class="math inline"><em>z</em></span> todistribute mostly under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)&lt;/span&gt;,Since &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;is the distribution we currently have for z, we want &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to sit mostly under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)&lt;/span&gt;.In the following graph, the y-axis is &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)&lt;/span&gt;,the distribution we try to maximize, and the x-axis is our latentvariable z. There is also a hidden axis - the probability mass(distribution) of <span class="math inline"><em>z</em></span>. Weproject this hidden axis to the y-axis in this graph. To maximize thisfirst term, we spread <span class="math inline"><em>z</em></span>‚Äôs massas much under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)&lt;/span&gt;as possible, which makes the green part of this graph.</p> <figure><img src="/images/p(xz)-with-entropy.png" alt="maximize ELBO"><figcaption aria-hidden="true">maximize ELBO</figcaption></figure> <p>Now we take <strong>the second term entropy</strong> intoconsideration. &lt;spanclass="math display"&gt;‚Ñí<sub><em>i</em></sub>(<em>p</em>,‚ÄÜ<em>q</em><sub><em>i</em></sub>)‚ÄÑ=‚ÄÑùîº<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[log‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)]‚ÄÖ+‚ÄÖ‚Ñã<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>(<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;From our <a href="link!"><strong><em>entropy post</em></strong></a>, weknow entropy measures the expected code length of communicating theevent described by a random variable. So the more random this variableis, the more code words it‚Äôs required to communicate it. Therefore, themore spread out / uniform the distribution is, the higher the entropy.If we‚Äôre maxing the entropy, we don‚Äôt want the distribution to beskinny. See the following graph.</p> <figure><img src="/images/entropy-example.png" alt="entropy-example"><figcaption aria-hidden="true">entropy-example</figcaption></figure> <p>When we consider both entropy and the first term, we should achievethis probability distribution depicted in brown. If we don‚Äôt have theentropy, <span class="math inline"><em>z</em></span> will want to situnder the most likely point, but since we added entropy, &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; now tries to cover it. Inconclusion, (equal sign ‚Äú=‚Äù reads ‚Äúin effect‚Äù) maximize evidence lowerbound = cover most of the &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;distribution = maximize approximation between &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;.</p> <figure><img src="/images/p(xz)-with-entropy.png" alt="maximize ELBO"><figcaption aria-hidden="true">maximize ELBO</figcaption></figure> <h3 id="effect-of-pushing-up-elbo-analytically">Effect of Pushing UpELBO (Analytically)</h3> <p>Can we measure how good our approximation is? That is, can we measurethe distance between &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;and <span class="math inline"><em>q</em><sub><em>i</em></sub></span>? Infact, we have a nice, analytical way to look at it using <strong>KLdivergence</strong>. For two arbitrary distribution &lt;spanclass="math inline"&gt;<em>p</em>,‚ÄÜ<em>q</em>&lt;/span&gt; of &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt;, the KL divergence of &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; from &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt; (the distance from &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; to &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;, note KL divergence is notsymmetric) is</p> <p><span class="math display">$$\begin{align}D_{\mathrm{KL}}(q|p)&amp;=E_{x\sim q(x)}\left[\log{\frac{q(x)}{p(x)}}\right]\\&amp;=E_{x \sim q(x)}[\log q(x)]-E_{x \sim q(x)}[\log p(x)]\\&amp;=-E_{x \sim q(x)}[\log p(x)]-H(q)\end{align}$$</span> Doesn‚Äôt this look similar to our evidence lower bound?Borrowing that explanation, minimizing KL divergence = cover most of the<span class="math inline"><em>p</em>(<em>z</em>)</span> distribution =maximize approximation between &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;.</p> <figure><img src="/images/KL-divergence.png" alt="KL-divergence"><figcaption aria-hidden="true">KL-divergence</figcaption></figure> <p>Having understood the definition of KL divergence, let‚Äôs use it tomeasure the distance between &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;- the distribution we want &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; toapproximate: <span class="math display">$$\begin{align}D_{KL}(q_{i}(z)\|p(z \mid x_{i}))&amp;= E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)}{p(z|x_{i})}}\right]\\&amp;= E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)p(x_{i})}{p(x_{i},z)}}\right]\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +E_{z\sim q_{i}(z)}\log q_i(z) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +\mathcal H(q_i) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -\mathcal L(p, q_i) + \log p(x_i)\\\log p(x_i) &amp;= \mathcal L(p, q_i) + D_{KL}(q_{i}(x_{i})\|p(z \midx_{i}))\end{align}$$</span> Therefore, having a good approximation of &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;= driving KL divergence, which is always a non-negative number, to 0 =the evidence lower bound is a tight bound or even equal to &lt;spanclass="math inline"&gt;log‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;‚Äã- the ultimate thing we want to optimize.</p> <p>Looking at our optimization objective &lt;spanclass="math inline"&gt;‚Ñí&lt;/span&gt; here: &lt;spanclass="math display"&gt;‚Ñí(<em>p</em>,‚ÄÜ<em>q</em><sub><em>i</em></sub>)‚ÄÑ=‚ÄÑlog‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)‚ÄÖ‚àí‚ÄÖ<em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub>)‚à•<em>p</em>(<em>z</em>‚ÄÖ‚à£‚ÄÖ<em>x</em><sub><em>i</em></sub>))&lt;/span&gt;</p> <ul> <li><p>When we optimize w.r.t. &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt;: note the first term &lt;spanclass="math inline"&gt;log‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;is independent of <span class="math inline"><em>q</em></span>, so itsvalue stays the same. We are in effect optimizing against the KLdivergence only, making the distance between our approximation &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;smaller. The best / extreme case is we have &lt;spanclass="math inline"&gt;<em>D</em><sub><em>K</em><em>L</em></sub>‚ÄÑ=‚ÄÑ0&lt;/span&gt;,so &lt;spanclass="math inline"&gt;‚Ñí‚ÄÑ=‚ÄÑlog‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.</p></li> <li><p>When we optimize w.r.t. &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;: Recall our ultimate goal is tomake &lt;spanclass="math inline"&gt;log‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;bigger, so we make a better model in theory. Only in theory because wedon‚Äôt know whether the bound is tight or not.</p></li> </ul> <h3 id="the-learning-algorithm">The Learning Algorithm?</h3> <p>Therefore, when we optimize &lt;spanclass="math inline"&gt;‚Ñí(<em>p</em>,‚ÄÜ<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;‚Äãw.r.t. <span class="math inline"><em>q</em></span>‚Äã, we make the boundtighter (make <span class="math inline">‚Ñí</span>‚Äã a better approximationof &lt;spanclass="math inline"&gt;log‚ÄÜ<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;‚Äã); when we optimize &lt;spanclass="math inline"&gt;‚Ñí(<em>p</em>,‚ÄÜ<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;‚Äãw.r.t. <span class="math inline"><em>p</em></span>‚Äã, we make a bettermodel in general.</p> <p>By alternating these two steps, we have <strong>the actual learningalgorithm</strong>. Let‚Äôs review: which parts are learnable in these twodistributions?</p> <ul> <li><p>In our <a href="#Latent-Variable-Models-in-General">latentvariable model setup</a>, we decompose the complicated distribution<span class="math inline"><em>p</em>(<em>x</em>)</span> into two easydistributions &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, where the mappingfrom <span class="math inline"><em>z</em></span> to actual parameters ofthis <span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>distribution needs to be modeled by a complex network. Therefore, theonly distribution in the <span class="math inline"><em>p</em></span>part with learnable parameters is &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt;. We denoteit with &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em>|<em>z</em>)&lt;/span&gt;.</p></li> <li><p>In our <a href="#Variational-Approximation">ELBO setup</a>, wealso introduced a simple &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;for each datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt; toapproximate the posterior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.To optimize w.r.t. <span class="math inline"><em>q</em></span>, weoptimize the parameters of each distribution. If &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)‚ÄÑ=‚ÄÑùí©(<em>Œº</em><sub><em>i</em></sub>,‚ÄÜ<em>œÉ</em><sub><em>i</em></sub>)&lt;/span&gt;,we optimize each &lt;spanclass="math inline"&gt;<em>Œº</em><sub><em>i</em></sub>,‚ÄÜ<em>œÉ</em><sub><em>i</em></sub>&lt;/span&gt;.(<em>we can optimize the entropy value for sure, but I‚Äôm not entirelysure how you would take gradient of the expectation term &lt;spanclass="math inline"&gt;<em>E</em><sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[log‚ÄÜ<em>p</em>(<em>z</em>)]&lt;/span&gt;</em>)</p></li> </ul> <p>Therefore, we have our learning algorithm: &lt;spanclass="math display"&gt;$$\begin{align}&amp;\text{for each $x_i$ in $\{x_1, \dots, x_N\}$: }\\&amp;\hspace{4mm} \text{sample $z \sim q_i(z)$}\\&amp;\hspace{4mm} \text{optimize against $p$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_\theta \mathcal L (p_\theta, q_i)= \nabla_\theta \log p_\theta(x_i|z) \\&amp;\hspace{4mm} \hspace{4mm} \theta \leftarrow \theta + \alpha\nabla_\theta \mathcal L (p, q_i) \\&amp;\hspace{4mm} \text{optimize against $q$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_{\mu_i, \sigma_i} \mathcal L(p_\theta, q_i) = \nabla_{\mu_i, \sigma_i} \left[\mathbb E_{z\simq_{i}(z)} \left[\log p(x_{i}|z)+\log p(z) \right] + \mathcal H_{z\simq_{i}(z)} (q_i) \right] \\&amp;\hspace{4mm} \hspace{4mm} (\mu_i, \sigma_i) \leftarrow (\mu_i,\sigma_i) + \alpha \nabla_{\mu_i, \sigma_i} \mathcal L (p, q_i) \\\end{align}$$&lt;/span&gt;</p> <p>There‚Äôs a problem with optimizing &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; though. Notewe have a separate <span class="math inline"><em>q</em></span> for eachdata point <span class="math inline"><em>i</em></span>, which means ifwe have <span class="math inline"><em>N</em></span> data points, we willhave to store &lt;spanclass="math inline"&gt;<em>N</em>‚ÄÖ√ó‚ÄÖ(|<em>Œº</em><sub><em>i</em></sub>|‚ÄÖ+‚ÄÖ|<em>œÉ</em><sub><em>i</em></sub>|)&lt;/span&gt;parameters assuming we chose &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to beGaussian. In machine learning, the number of data points &lt;spanclass="math inline"&gt;<em>N</em>&lt;/span&gt; is usually in millions, makingthis model unwieldily big. It‚Äôs true in inference time we do not use<span class="math inline"><em>q</em></span> at all (we‚Äôll see why thisis true in the last chapter about VAE), but in training time, we stillneed them so it‚Äôs necessary to keep all these parameters.</p> <p>Therefore, instead of having a separate &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(‚ãÖ)&lt;/span&gt; toapproximate each data point‚Äôs &lt;spanclass="math inline"&gt;<em>P</em>(‚ãÖ|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;specifically, we use a learnable model &lt;spanclass="math inline"&gt;<em>q</em><sub><em>œï</em></sub>(‚ãÖ|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;to approximate &lt;spanclass="math inline"&gt;<em>p</em>(‚ãÖ|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;This learnable network will take in a datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;, predicts thecorresponding &lt;spanclass="math inline"&gt;<em>Œº</em><sub><em>i</em></sub>,‚ÄÜ<em>œÉ</em><sub><em>i</em></sub>&lt;/span&gt;.We can then sample <span class="math inline"><em>z</em></span>‚Äã from thispredicted network.</p> <h2 id="amortized">Amortized</h2> <p>By adapting <span class="math inline"><em>q</em></span> to be alearnable network &lt;spanclass="math inline"&gt;<em>q</em><sub><em>œï</em></sub>&lt;/span&gt;‚Äã instead,model size does not depend on the number of datapoints anymore.Therefore, it is <strong>amortized</strong>.</p> <p>The variational lower bound becomes: &lt;spanclass="math display"&gt;‚Ñí(<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>),‚ÄÜ<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>))‚ÄÑ=‚ÄÑùîº<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[log‚ÄÜ<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)‚ÄÖ+‚ÄÖlog‚ÄÜ<em>p</em>(<em>z</em>)]‚ÄÖ+‚ÄÖ‚Ñã(<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>))&lt;/span&gt;The learning algorithm naturally becomes: $$ $$</p> <h3 id="gradient-over-expectation-policy-gradient">Gradient OverExpectation (Policy Gradient)</h3> <p>The question now boils down to how do we calculate this gradient&lt;spanclass="math inline"&gt;‚àá<sub><em>œï</em></sub>‚Ñí(<em>p</em><sub><em>Œ∏</em></sub>,‚ÄÜ<em>q</em><sub><em>œï</em></sub>)&lt;/span&gt;.</p> <p>The second term entropy is easy. We purposefully chose &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; to be a simple distribution, sothere is usually a close form of its entropy and we just have to look itup.</p> <p>The meat is in the first part. How do we take gradient w.r.t.parameter <span class="math inline"><em>œï</em></span> in the expectationterm‚Äôs distribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>œï</em></sub>&lt;/span&gt; ? Note theterm inside expectation is independent of &lt;spanclass="math inline"&gt;<em>œï</em>&lt;/span&gt;, so we can rewrite it as &lt;spanclass="math inline"&gt;<em>R</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)‚ÄÑ=‚ÄÑlog‚ÄÜ<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)‚ÄÖ+‚ÄÖlog‚ÄÜ<em>p</em>(<em>z</em>)&lt;/span&gt;and call the whole thing &lt;spanclass="math inline"&gt;<em>J</em>&lt;/span&gt;.<br>&lt;spanclass="math display"&gt;<em>J</em>(<em>œï</em>)‚ÄÑ=‚ÄÑ‚àá<sub><em>œï</em></sub>ùîº<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)]&lt;/span&gt;We chose these namings purposefully because we encountered somethingsimilar back in the &lt;ahref="https://slides.com/sarahdean-2/sp24-cs-4789-lec-16?token=KNeurk-c#/11/0/4"&gt;policygradient part of reinforcement learning <strong>LINK???</strong>&lt;/a&gt;.Say we have a trajectory <span class="math inline"><em>œÑ</em></span>,sampled from the state transition function with learnable policy &lt;spanclass="math inline"&gt;<em>œÄ</em><sub><em>Œ∏</em></sub>&lt;/span&gt;, the finalexpected value we can get from starting state &lt;spanclass="math inline"&gt;<em>s</em><sub>0</sub>&lt;/span&gt; can be written as thefollowing, where <span class="math inline"><em>R</em>(<em>œÑ</em>)</span>is a reward function returning the reward of this trajectory. &lt;spanclass="math display"&gt;<em>J</em>(<em>Œ∏</em>)‚ÄÑ=‚ÄÑ<em>V</em><sup><em>œÄ</em><sub><em>Œ∏</em></sub></sup>(<em>s</em><sub>0</sub>)‚ÄÑ=‚ÄÑùîº<sub><em>œÑ</em>‚ÄÑ‚àº‚ÄÑ<em>P</em><sub><em>s</em><sub>0</sub></sub><sup><em>œÄ</em><sub><em>Œ∏</em></sub></sup></sub>[<em>R</em>(<em>œÑ</em>)]&lt;/span&gt;We can take the gradient of this value function &lt;spanclass="math inline"&gt;<em>V</em>&lt;/span&gt; w.r.t our policy &lt;spanclass="math inline"&gt;<em>œÄ</em><sub><em>Œ∏</em></sub>&lt;/span&gt;, so this iscalled policy gradient. If you‚Äôre unfamiliar with RL setup, you justhave to know we can derive the following gradient and we can approximateit by sampling <span class="math inline"><em>M</em></span> trajectories.$$ <span class="math display">$$Pugging in our $q$ and $\phi$,$$</span> $$</p> <h3 id="reparametrization-trick">Reparametrization Trick</h3> <p>We have our full learning algorithm and it‚Äôs ready to go now.However, there is a tiny improvement we can do.</p> <p>We defined our &lt;spanclass="math inline"&gt;<em>q</em><sub><em>œï</em></sub>&lt;/span&gt; to be anormal distribution &lt;spanclass="math inline"&gt;ùí©(<em>Œº</em><sub><em>œï</em></sub>,‚ÄÜ<em>œÉ</em><sub><em>œï</em></sub>)&lt;/span&gt;Observe that all normal distributions can be written as a function ofthe unit normal distribution. Therefore, a sample &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; is in effect: &lt;spanclass="math display"&gt;<em>z</em>‚ÄÑ‚àº‚ÄÑùí©(<em>Œº</em><sub><em>œï</em></sub>,‚ÄÜ<em>œÉ</em><sub><em>œï</em></sub>)‚ÄÑ‚áî‚ÄÑ<em>z</em>‚ÄÑ=‚ÄÑ<em>Œº</em><sub><em>œï</em></sub>‚ÄÖ+‚ÄÖ<em>œµ</em><em>œÉ</em><sub><em>œï</em></sub>,‚ÄÖ<em>œµ</em>‚ÄÑ‚àº‚ÄÑùí©(0,‚ÄÜ1)&lt;/span&gt;Let‚Äôs rewrite our expectation term to now sample an &lt;spanclass="math inline"&gt;<em>œµ</em>&lt;/span&gt; from the unit normal distributioninstead. By decomposing <span class="math inline"><em>z</em></span> intothese two parts, we separate the stochastic part and changed &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; from a sample of some stochasticdistribution into a deterministic function &lt;spanclass="math inline"&gt;<em>z</em>(<em>œï</em>,‚ÄÜ<em>œµ</em>)&lt;/span&gt;parametrized by <span class="math inline"><em>œï</em></span> and randomvariable <span class="math inline"><em>œµ</em></span> that is independentof <span class="math inline"><em>œï</em></span>. &lt;spanclass="math inline"&gt;<em>œµ</em>&lt;/span&gt; takes the stochastic part alone.Our learnable parameter <span class="math inline"><em>œï</em></span> nowonly parametrizes deterministic quantity. &lt;spanclass="math display"&gt;‚àá<sub><em>œï</em></sub><em>J</em>(<em>œï</em>)‚ÄÑ=‚ÄÑ‚àá<sub><em>œï</em></sub>ùîº<sub><em>œµ</em>‚ÄÑ‚àº‚ÄÑùí©(0,‚ÄÜ1)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>Œº</em><sub><em>œï</em></sub>‚ÄÖ+‚ÄÖ<em>œµ</em><em>œÉ</em><sub><em>œï</em></sub>)]&lt;/span&gt;Aside from these theoretical benefits, mathematically, we do not have totake gradient w.r.t an expectation of parametrized distribution anymore.Instead, the gradient can go straight into the expectation term now likehow we usually interchange gradient and expectation (think aboutdiscrete case, expectation is just a big sum so we can do it). &lt;spanclass="math display"&gt;‚àá<sub><em>œï</em></sub><em>J</em>(<em>œï</em>)‚ÄÑ=‚ÄÑùîº<sub><em>œµ</em>‚ÄÑ‚àº‚ÄÑùí©(0,‚ÄÜ1)</sub>[‚àá<sub><em>œï</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>Œº</em><sub><em>œï</em></sub>‚ÄÖ+‚ÄÖ<em>œµ</em><em>œÉ</em><sub><em>œï</em></sub>)]&lt;/span&gt;Further, to approximate this expectation, we just sample some &lt;spanclass="math inline"&gt;<em>œµ</em>&lt;/span&gt; from this normal distribution.<span class="math display">$$\nabla_\phi J(\phi)\approx \frac 1 M \sum_j^M \nabla_\phi R(x_i, \mu_\phi + \epsilon_j\sigma_\phi)$$</span></p> <p>With reparametrization, we achieve a lower variance than policygradient because we are using the derivative of R. (<em>Unfortunatelythe lecturer didn‚Äôt provide a quantitative analysis on this and I don‚Äôtknow how to prove it</em>) On the other hand, previously, we only tookderivative w.r.t. the probability distribution. Why didn‚Äôt we usederivative of R back in RL with policy gradient? It‚Äôs not we don‚Äôt wantto but we can‚Äôt: we can‚Äôt use reparametrization in RL because in RL weusually cannot take derivative w.r.t. reward R.</p> <table> <colgroup> <col style="width: 6%"> <col style="width: 23%"> <col style="width: 23%"> <col style="width: 23%"> <col style="width: 23%"> </colgroup> <thead><tr class="header"> <th>Method</th> <th>Formula</th> <th>Approximation</th> <th>Benefit</th> <th>Deficit</th> </tr></thead> <tbody> <tr class="odd"> <td>Policy Gradient</td> <td>&lt;spanclass="math inline"&gt;‚àá<sub><em>œï</em></sub>ùîº<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>œï</em></sub>(<em>z</em>‚ÄÖ‚à£‚ÄÖ<em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>z</em>)]&lt;/span&gt;</td> <td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi[\logq_\phi(z_j \mid x_i)] R(x_i,z_j)$</span></td> <td>works with both discrete and continuous latent variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;</td> <td>High variance, requires multiple samples &amp; small learningrates</td> </tr> <tr class="even"> <td>Reparametrization</td> <td>&lt;spanclass="math inline"&gt;ùîº<sub><em>œµ</em>‚ÄÑ‚àº‚ÄÑùí©(0,‚ÄÜ1)</sub>[‚àá<sub><em>œï</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>,‚ÄÜ<em>Œº</em><sub><em>œï</em></sub>‚ÄÖ+‚ÄÖ<em>œµ</em><em>œÉ</em><sub><em>œï</em></sub>)]&lt;/span&gt;</td> <td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi R(x_i,\mu_\phi + \epsilon_j \sigma_\phi)$</span></td> <td>low variance, simple to implement (we‚Äôll see soon)</td> <td>only works with continuous variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; and have to model it with aGaussian</td> </tr> </tbody> </table> <p>In fact, you can forget about the policy gradient method and simplytake it for granted that you cannot back propagate a sampled value &lt;spanclass="math inline"&gt;‚àá<sub><em>œï</em></sub>ùîº<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>&lt;/span&gt;,so you have to find some way to make our &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;‚Äã deterministic, which is what we‚Äôredoing here with our reparametrization trick.</p> <figure>&lt;img src="/images/reparametrization-trick.png"alt="reparametrization-trick" /&gt;<figcaption aria-hidden="true">reparametrization-trick</figcaption></figure> <p>Left is without the ‚Äúreparameterization trick‚Äù, and right is with it.Red shows sampling operations that are non-differentiable. Blue showsloss layers. We forward the network by going up and back propagate it bygoing down. The forward behavior of these networks is identical, butback propagation can be applied only to the right network. Figure copiedfrom <a href="https://arxiv.org/abs/1606.05908" rel="external nofollow noopener" target="_blank">Carl Doersch: Tutorialon Variational Autoencoders</a></p> <h3 id="looking-at-mathcal-l-directly">Looking at &lt;spanclass="math inline"&gt;‚Ñí&lt;/span&gt; Directly</h3> <p><span class="math display">$$\begin{align}\mathcal L_i = \mathcal L \left( p_\theta(x_i | z), q_\phi(z | x_i)\right)&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)+\log p(z) \right] + \mathcal H (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p_\theta(x_{i}|z)\right] + \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p(z) \right] + \mathcalH (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)\right] - D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;= \mathbb E_{\epsilon \sim \mathcal N(0,1)} \left[\logp_\theta(x_{i}| \mu_\phi + \epsilon \sigma_\phi)\right] -D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;\approx \frac 1 M \sum_j^M \log p_\theta(x_{i}| \mu_\phi +\epsilon_j \sigma_\phi) - D_{KL}(q_\phi(z | x_i)\|p(z)) \\\end{align}$$</span></p> <p>For the first term, we can just evaluate it. For the second KL term,since we chose both distributions to be easy (in this case Gaussian),there often is a nice analytical form for it.</p> <p>Therefore, we can go ahead to maximize the variational lower bound<span class="math inline">‚Ñí</span>‚Äã. We can also draw out the followingcomputational graph for the log term and conclude we can back propagatethis graph without any problem. On the other hand, if we didn‚Äôt do thereparametrization trick, we will get stuck at &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;: you cannot back propagate &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; - a sampled value instead of avariable. And we will have to seek help from policy gradient. Withreparametrization, we decompose &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; into two variables &lt;spanclass="math inline"&gt;<em>Œº</em><sub><em>œï</em></sub>,‚ÄÜ<em>œÉ</em><sub><em>œï</em></sub>&lt;/span&gt;we can back propagate through and one stochastic value &lt;spanclass="math inline"&gt;<em>œµ</em>&lt;/span&gt; we do not care about.</p> <figure><img src="/images/computational-graph.png" alt="computational-graph"><figcaption aria-hidden="true">computational-graph</figcaption></figure> <h2 id="variational-autoencoder">Variational Autoencoder</h2> <h3 id="setup-and-interpretation">Setup and Interpretation</h3> <p>What we have gone though constitutes the full pipeline of avariational autoencoder.</p> <p>In a variation autoencoder, we have observed variable &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; and latent variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;</p> <ul> <li>encoder &lt;spanclass="math inline"&gt;<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em>)‚ÄÑ=‚ÄÑùí©(<em>Œº</em><sub><em>œï</em></sub>(<em>x</em>),‚ÄÜ<em>œÉ</em><sub><em>œï</em></sub>(<em>x</em>))&lt;/span&gt;</li> <li>decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em>|<em>z</em>)‚ÄÑ=‚ÄÑùí©(<em>Œº</em><sub><em>Œ∏</em></sub>(<em>z</em>),‚ÄÜ<em>œÉ</em><sub><em>Œ∏</em></sub>(<em>z</em>))&lt;/span&gt;</li> </ul> <p>In training, given an observed sample &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;, we encode itto latent variable &lt;spanclass="math inline"&gt;<em>z</em><sub><em>i</em></sub>&lt;/span&gt; using &lt;spanclass="math inline"&gt;<em>q</em><sub><em>œï</em></sub>&lt;/span&gt;, then triesto decode it back with decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Œ∏</em></sub>&lt;/span&gt;. We maximizethe variational lower bound during the process. For all &lt;spanclass="math inline"&gt;<em>N</em>&lt;/span&gt; samples, the training objectivelooks like: (where the <span class="math inline"><em>œµ</em></span> is asampled value) <span class="math display">$$\max_{\phi,\theta} \frac 1 N \sum_i^N \log p_\theta\left(x_{i}|\mu_\phi(x_i) + \epsilon \sigma_\phi(x_i)\right) - D_{KL}(q_\phi(z |x_i)\|p(z)) \\$$</span> In inference (generation), we sample a &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; from our prior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, then decode it using<span class="math inline"><em>p</em><sub><em>Œ∏</em></sub></span>: &lt;spanclass="math inline"&gt;<em>z</em>‚ÄÑ‚àº‚ÄÑ<em>p</em>(<em>z</em>),‚ÄÜ<em>x</em>‚ÄÑ‚àº‚ÄÑ<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em>|<em>z</em>)&lt;/span&gt;</p> <p>Why does the variational autoencoder work? We talked about manybenefits of maximizing this variational lower bound in &lt;ahref="#Effect-of-Pushing-Up-ELBO-(Analytically)"&gt;previous chapter&lt;/a&gt;.Let‚Äôs look at it again in this decoder-encoder setup,. &lt;spanclass="math display"&gt;‚Ñí<sub><em>i</em></sub>‚ÄÑ=‚ÄÑùîº<sub><em>z</em>‚ÄÑ‚àº‚ÄÑ<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[log‚ÄÜ<em>p</em><sub><em>Œ∏</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)]‚ÄÖ‚àí‚ÄÖ<em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>œï</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)‚à•<em>p</em>(<em>z</em>))&lt;/span&gt;</p> <ul> <li>The first &lt;spanclass="math inline"&gt;log‚ÄÜ<em>p</em><sub><em>Œ∏</em></sub>&lt;/span&gt; termmaximizes the probability of our observed image &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; given a sample &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;, so the model makes decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Œ∏</em></sub>&lt;/span&gt; toreconstruct image <span class="math inline"><em>x</em></span>‚Äã asaccurate as possible.</li> <li>The second KL term restricts the encoding of an image to be close tothe actual prior, which makes sure at inference / generate time, we candirectly sample from the prior.</li> </ul> <h3 id="comparison-with-auto-encoder">Comparison with Auto-Encoder</h3> <figure><img src="/images/vae-and-ae.png" alt="vae-and-ae"><figcaption aria-hidden="true">vae-and-ae</figcaption></figure> <p>The VAE‚Äôs decoder is trained to convert random points in theembedding space (generated by perturbing the input encodings) tosensible outputs. By contrast, the decoder for the deterministicautoencoder only ever gets as inputs the exact encodings of the trainingset, so it does not know what to do with random inputs that are outsidewhat it was trained on. So a standard autoencoder cannot create newsamples.</p> <p>The reason the VAE is better at sample is that it embeds images intoGaussians in latent space, whereas the AE embeds images into points,which are like delta functions. The advantage of using a latentdistribution is that it encourages local smoothness, since a given imagemay map to multiple nearby places, depending on the stochastic sampling.By contrast, in an AE, the latent space is typically not smooth, soimages from different classes often end up next to each other. Figurecopied from &lt;ahref="https://probml.github.io/pml-book/book1.html"&gt;ProbabilisticMachine Learning: An Introduction - Figure 20.26&lt;/a&gt;</p> <p>We can leverage the smoothness of the latent space to perform imageinterpolation in latent space.</p> <h2 id="reference">Reference</h2> <p>Most content of this blog post comes from &lt;ahref="https://www.youtube.com/watch?v=UTMpM4orS30"&gt;Berkeley CS 285(Sergey Levine): Lecture 18, Variational Inference&lt;/a&gt;, which I thinkorganized his lecture based on &lt;ahref="https://arxiv.org/abs/1906.02691"&gt;An Introduction to VariationalAutoencoders&lt;/a&gt; (2.1-2.7, and 2.9.1), or more in-depth on the author‚ÄôsPhD thesis <a href="http://dpkingma.com/#phdthesis" rel="external nofollow noopener" target="_blank">VariationalInference and Deep Learning: A New Synthesis</a> I found this wonderfultutorial in &lt;ahref="https://probml.github.io/pml-book/book2.html"&gt;ProbabilisticMachine Learning: Advanced Topics&lt;/a&gt;</p> <p>Some graph come from &lt;ahref="https://probml.github.io/pml-book/book1.html"&gt;ProbabilisticMachine Learning: An Introduction&lt;/a&gt; itself and &lt;ahref="https://arxiv.org/abs/1606.05908"&gt;Carl Doersch: Tutorial onVariational Autoencoders&lt;/a&gt;, which is referenced in the previousbook.</p> <p>Note though the <em>Probabilistic Machine Learning</em> book itselfis a horrible book with extremely confusing explanations.</p> </body></html>