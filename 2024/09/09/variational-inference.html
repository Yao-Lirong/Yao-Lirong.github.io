<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="probabilistic-latent-variable-models">Probabilistic LatentVariable Models</h2> <p>The two general forms of probabilistic models are:</p> <ul> <li> <span class="math inline"><em>p</em>(<em>x</em>)</span>: a typicalprobabilistic distribution. In this model, we call &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; the query.</li> <li>&lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em>â€…âˆ£â€…<em>x</em>)&lt;/span&gt;: aconditional probabilistic distribution. In this model, we cal &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; the evidence and &lt;spanclass="math inline"&gt;<em>y</em>&lt;/span&gt; the query.</li> </ul> <p>Latent variable models are models that have variables other than thequery and the evidence.</p> <ul> <li> <p>&lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)â€„=â€„âˆ‘<sub><em>z</em></sub><em>p</em>(<em>x</em>â€…âˆ£â€…<em>z</em>)â€…<em>p</em>(<em>z</em>)&lt;/span&gt;â€‹</p> <p>A classic latent variable model of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)&lt;/span&gt; is the mixture model,where <span class="math inline"><em>p</em>(<em>x</em>)</span> isactually a mixture of several different probabilistic model. Forexample, in the following graph, &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; is a discrete variablerepresenting which class a datapoint belongs to and is represented bydifferent colors here. &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>â€…âˆ£â€…<em>z</em>)&lt;/span&gt; is eachclassâ€™s probability distribution, where in this case can each be modeledby a Gaussian. And &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>)&lt;/span&gt;â€‹ when we observe it, isjust a bunch of uncolored datapoints and is hard to fit a distributionon it. However, we can see itâ€™s roughly spread in 3 clusters so weintroduce the latent variable representing class and we can now well fita Gaussian mixture model on it (a mixture of 3 Gaussians)</p> <figure>&lt;img src="/images/Mixture-Gaussian-Distribution.png"alt="Gaussian Mixture Model" /&gt;<figcaption aria-hidden="true">Gaussian Mixture Model</figcaption></figure> </li> <li> <p>&lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em>â€…âˆ£â€…<em>x</em>)â€„=â€„âˆ‘<sub><em>z</em></sub><em>p</em>(<em>y</em>â€…âˆ£â€…<em>x</em>,â€†<em>z</em>)â€†<em>p</em>(<em>z</em>)&lt;/span&gt;or &lt;spanclass="math inline"&gt;<em>p</em>(<em>y</em>â€…âˆ£â€…<em>x</em>)â€„=â€„âˆ‘<sub><em>z</em></sub><em>p</em>(<em>y</em>â€…âˆ£â€…<em>z</em>)â€†<em>p</em>(<em>z</em>â€…âˆ£â€…<em>x</em>)&lt;/span&gt;:the conditional probability is a bit more free. You can decompose andmodel it using <span class="math inline"><em>z</em></span>â€‹ as youlike.</p> <p>An example of latent conditional model is the mixture densitynetwork, which we use in RLâ€™s imitation learning to deal withmulti-modal situations each requiring a different distribution.</p> </li> </ul> <h3 id="latent-variable-models-in-general">Latent Variable Models inGeneral</h3> <p>When we use latent variable models, it means we want to<strong>decompose a complicated distribution into several simple / easydistributions</strong>. By <strong>complicated</strong>, we mean itâ€™snot possible to write it in a well-defined distribution. By<strong>simple / easy</strong>, we mean we can write it as awell-defined parametrized distribution, where the parameters can becomplex, but the distribution itself is easy to write (a Gaussian ofjust mean and sigma, or as a Bernoulli with just one variable, etc.)&lt;spanclass="math display"&gt;<em>p</em>(<em>x</em>)â€„=â€„âˆ«<em>p</em>(<em>x</em>â€…âˆ£â€…<em>z</em>)<em>p</em>(<em>z</em>)<em>d</em><em>z</em>&lt;/span&gt;</p> <ul> <li> <span class="math inline"><em>p</em>(<em>z</em>)</span> is an â€œeasyâ€prior we choose. For example a Gaussian, a categorical distribution,etc.</li> <li> <span class="math inline"><em>p</em>(<em>x</em>â€…âˆ£â€…<em>z</em>)</span>should also be an easy distribution, like a Gaussian: $ p(x z) =(<em>{nn}(z), </em>{nn}(z))$ even though the mapping from &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; to the actual parameters ofGaussian can be complex, where in this case we have to model the mappingthrough a neural network and this mapping is the learnable part.</li> <li> <span class="math inline"><em>p</em>(<em>x</em>)</span> iscomplicated, not possible to write out as any well-defined distribution.Therefore, we decompose it into the two parts above that are<strong>easy to parametrize as a probability distribution and learn theparameters inside the distribution</strong>.</li> </ul> <p>Generative models are not equal to latent variable models. We usuallymodel generative models as latent variable ones because generativemodels are usually complex probability distributions and we can make iteasier by introducing one or more latent variable.</p> <h3 id="how-to-train-a-latent-variable-model">How to Train a LatentVariable Model</h3> <p>Given dataset &lt;spanclass="math inline"&gt;ğ’Ÿâ€„=â€„{<em>x</em><sub>1</sub>,â€†<em>x</em><sub>2</sub>,â€†â€¦,â€†<em>x</em><sub><em>N</em></sub>}&lt;/span&gt;,to fit a typical probabilistic model &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Î¸</em></sub>(<em>x</em>)&lt;/span&gt;,we use the maximum likelihood estimation: <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N \sum_i \logp_\theta(x_i)$$</span> In the latent variable model set up, we can substitute thedefinition in and an MLE would look like <span class="math display">$$\theta \leftarrow \underset{\theta}{arg\!\max} \frac 1 N\sum_i \log \left( \int p_\theta(x_i \mid z) p(z) dz \right)$$</span> &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Î¸</em></sub>(<em>x</em>â€…âˆ£â€…<em>z</em>)&lt;/span&gt;and <span class="math inline"><em>p</em>(<em>z</em>)</span> aredistributions of our choices, but this integral is still intractablewhen <span class="math inline"><em>z</em></span> is continuous. So nowitâ€™s time to do some math tricks.</p> <h2 id="variational-inference">Variational Inference</h2> <h3 id="variational-approximation">Variational Approximation</h3> <p>First, we construct an easy / simple probability distribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to approximate &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;- the posterior distribution specific to datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;. By easy weagain mean it is easy to parametrize (a Gaussian, a Bernoulli, etc.)</p> <p>We will show that by introducing this &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;,we can actually construct a lower bound of &lt;spanclass="math inline"&gt;logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.Whatâ€™s good with this lower bound? Later on, we will also prove thisbound is sufficiently tight, so as we push up the value of this lowerbound, we push up the value of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;which is exactly what we want.</p> <p><span class="math display">$$\begin{align}\log p(x_{i})&amp;= \log\int_{z}p(x_{i}|z)p(z)\\&amp;= \log\int_{z}p(x_{i}|z)p(z) \frac{q_i(z)}{q_i(z)} \\&amp;= \log \mathbb E_{z\sim q_{i}(z)} \left[\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] \\&amp;\geq \mathbb E_{z\sim q_{i}(z)}\left[\log\frac{p(x_{i}|z)p(z)}{q_{i}(z)}\right] &amp;\text{\# Jensen'sInequality} \\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] - \mathbb E_{z\sim q_{i}(z)} \left[ \log {q_{i}(z)}\right]\\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)= \mathcal L_i(p, q_i)\end{align}$$</span> Recall <span class="math inline"><em>p</em>(<em>x</em>)</span>is a difficult probability distribution, so we decompose it into twoeasy distributions &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, and use an easydistribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to approximate the posterior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.Now the good thing is: everything here is tractable: for the first term,we can fix a &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;of our choice (recall &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; is adistribution we constructed), sample some &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;, and evaluate the expression. Forthe second term, we notice it is just the entropy of a distribution andfor simple distributions (we constructed &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to besimple), it has a closed form (even if it doesnâ€™t, you can simply sampleand evaluate)</p> <p>We call the final lower bound we derived here the <strong>variancelower bound</strong> or <strong>evidence lower bound (ELBO)</strong>.<span class="math display">$$\begin{align}\log p(x_{i})&amp;\geq \mathcal L_i(p, q_i) \\&amp;= \mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] + \mathcal H_{z\sim q_{i}(z)} (q_i)\end{align}$$</span> ### Effect of Pushing Up ELBO (Intuitively)</p> <p>Assume our <span class="math inline"><em>p</em>(â‹…)</span>â€‹ is a fixedvalue, what does pushing up ELBO mean? Here, we give out an intuitiveexplanation. First, we look at <strong>the first term</strong> with thetwo log combined. <span class="math display">$$\begin{align} &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i}|z)+\log p(z)\right] \\= &amp;\mathbb E_{z\sim q_{i}(z)} \left[\log p(x_{i},z) \right]\end{align}$$</span> To maximize this value, we just have to find a distribution of<span class="math inline"><em>z</em></span>, inside which we have thelargest value of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)&lt;/span&gt;.Therefore, we want <span class="math inline"><em>z</em></span> todistribute mostly under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)&lt;/span&gt;,Since &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;is the distribution we currently have for z, we want &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;to sit mostly under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)&lt;/span&gt;.In the following graph, the y-axis is &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)&lt;/span&gt;,the distribution we try to maximize, and the x-axis is our latentvariable z. There is also a hidden axis - the probability mass(distribution) of <span class="math inline"><em>z</em></span>. Weproject this hidden axis to the y-axis in this graph. To maximize thisfirst term, we spread <span class="math inline"><em>z</em></span>â€™s massas much under the peak of &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)&lt;/span&gt;as possible, which makes the green part of this graph.</p> <figure><img src="/images/p(xz)-with-entropy.png" alt="maximize ELBO"><figcaption aria-hidden="true">maximize ELBO</figcaption></figure> <p>Now we take <strong>the second term entropy</strong> intoconsideration. &lt;spanclass="math display"&gt;â„’<sub><em>i</em></sub>(<em>p</em>,â€†<em>q</em><sub><em>i</em></sub>)â€„=â€„ğ”¼<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)]â€…+â€…â„‹<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>(<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;From our <a href="link!"><strong><em>entropy post</em></strong></a>, weknow entropy measures the expected code length of communicating theevent described by a random variable. So the more random this variableis, the more code words itâ€™s required to communicate it. Therefore, themore spread out / uniform the distribution is, the higher the entropy.If weâ€™re maxing the entropy, we donâ€™t want the distribution to beskinny. See the following graph.</p> <figure><img src="/images/entropy-example.png" alt="entropy-example"><figcaption aria-hidden="true">entropy-example</figcaption></figure> <p>When we consider both entropy and the first term, we should achievethis probability distribution depicted in brown. If we donâ€™t have theentropy, <span class="math inline"><em>z</em></span> will want to situnder the most likely point, but since we added entropy, &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; now tries to cover it. Inconclusion, (equal sign â€œ=â€ reads â€œin effectâ€) maximize evidence lowerbound = cover most of the &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;distribution = maximize approximation between &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;.</p> <figure><img src="/images/p(xz)-with-entropy.png" alt="maximize ELBO"><figcaption aria-hidden="true">maximize ELBO</figcaption></figure> <h3 id="effect-of-pushing-up-elbo-analytically">Effect of Pushing UpELBO (Analytically)</h3> <p>Can we measure how good our approximation is? That is, can we measurethe distance between &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;and <span class="math inline"><em>q</em><sub><em>i</em></sub></span>? Infact, we have a nice, analytical way to look at it using <strong>KLdivergence</strong>. For two arbitrary distribution &lt;spanclass="math inline"&gt;<em>p</em>,â€†<em>q</em>&lt;/span&gt; of &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt;, the KL divergence of &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; from &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt; (the distance from &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; to &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;, note KL divergence is notsymmetric) is</p> <p><span class="math display">$$\begin{align}D_{\mathrm{KL}}(q|p)&amp;=E_{x\sim q(x)}\left[\log{\frac{q(x)}{p(x)}}\right]\\&amp;=E_{x \sim q(x)}[\log q(x)]-E_{x \sim q(x)}[\log p(x)]\\&amp;=-E_{x \sim q(x)}[\log p(x)]-H(q)\end{align}$$</span> Doesnâ€™t this look similar to our evidence lower bound?Borrowing that explanation, minimizing KL divergence = cover most of the<span class="math inline"><em>p</em>(<em>z</em>)</span> distribution =maximize approximation between &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;.</p> <figure><img src="/images/KL-divergence.png" alt="KL-divergence"><figcaption aria-hidden="true">KL-divergence</figcaption></figure> <p>Having understood the definition of KL divergence, letâ€™s use it tomeasure the distance between &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;- the distribution we want &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; toapproximate: <span class="math display">$$\begin{align}D_{KL}(q_{i}(z)\|p(z \mid x_{i}))&amp;= E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)}{p(z|x_{i})}}\right]\\&amp;= E_{z\simq_{i}(z)}\left[\log{\frac{q_{i}(z)p(x_{i})}{p(x_{i},z)}}\right]\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +E_{z\sim q_{i}(z)}\log q_i(z) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -E_{z\sim q_{i}(z)}\left[\log p(x_{i}|z)+\log p(z)\right] +\mathcal H(q_i) + E_{z\sim q_{i}(z)}\log p(x_{i})\\&amp;= -\mathcal L(p, q_i) + \log p(x_i)\\\log p(x_i) &amp;= \mathcal L(p, q_i) + D_{KL}(q_{i}(x_{i})\|p(z \midx_{i}))\end{align}$$</span> Therefore, having a good approximation of &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)&lt;/span&gt;= driving KL divergence, which is always a non-negative number, to 0 =the evidence lower bound is a tight bound or even equal to &lt;spanclass="math inline"&gt;logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;â€‹- the ultimate thing we want to optimize.</p> <p>Looking at our optimization objective &lt;spanclass="math inline"&gt;â„’&lt;/span&gt; here: &lt;spanclass="math display"&gt;â„’(<em>p</em>,â€†<em>q</em><sub><em>i</em></sub>)â€„=â€„logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)â€…âˆ’â€…<em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub>)âˆ¥<em>p</em>(<em>z</em>â€…âˆ£â€…<em>x</em><sub><em>i</em></sub>))&lt;/span&gt;</p> <ul> <li><p>When we optimize w.r.t. &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt;: note the first term &lt;spanclass="math inline"&gt;logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;is independent of <span class="math inline"><em>q</em></span>, so itsvalue stays the same. We are in effect optimizing against the KLdivergence only, making the distance between our approximation &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;smaller. The best / extreme case is we have &lt;spanclass="math inline"&gt;<em>D</em><sub><em>K</em><em>L</em></sub>â€„=â€„0&lt;/span&gt;,so &lt;spanclass="math inline"&gt;â„’â€„=â€„logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.</p></li> <li><p>When we optimize w.r.t. &lt;spanclass="math inline"&gt;<em>p</em>&lt;/span&gt;: Recall our ultimate goal is tomake &lt;spanclass="math inline"&gt;logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;bigger, so we make a better model in theory. Only in theory because wedonâ€™t know whether the bound is tight or not.</p></li> </ul> <h3 id="the-learning-algorithm">The Learning Algorithm?</h3> <p>Therefore, when we optimize &lt;spanclass="math inline"&gt;â„’(<em>p</em>,â€†<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;â€‹w.r.t. <span class="math inline"><em>q</em></span>â€‹, we make the boundtighter (make <span class="math inline">â„’</span>â€‹ a better approximationof &lt;spanclass="math inline"&gt;logâ€†<em>p</em>(<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;â€‹); when we optimize &lt;spanclass="math inline"&gt;â„’(<em>p</em>,â€†<em>q</em><sub><em>i</em></sub>)&lt;/span&gt;â€‹w.r.t. <span class="math inline"><em>p</em></span>â€‹, we make a bettermodel in general.</p> <p>By alternating these two steps, we have <strong>the actual learningalgorithm</strong>. Letâ€™s review: which parts are learnable in these twodistributions?</p> <ul> <li><p>In our <a href="#Latent-Variable-Models-in-General">latentvariable model setup</a>, we decompose the complicated distribution<span class="math inline"><em>p</em>(<em>x</em>)</span> into two easydistributions &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt; and &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, where the mappingfrom <span class="math inline"><em>z</em></span> to actual parameters ofthis <span class="math inline"><em>p</em>(<em>x</em>|<em>z</em>)</span>distribution needs to be modeled by a complex network. Therefore, theonly distribution in the <span class="math inline"><em>p</em></span>part with learnable parameters is &lt;spanclass="math inline"&gt;<em>p</em>(<em>x</em>|<em>z</em>)&lt;/span&gt;. We denoteit with &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Î¸</em></sub>(<em>x</em>|<em>z</em>)&lt;/span&gt;.</p></li> <li><p>In our <a href="#Variational-Approximation">ELBO setup</a>, wealso introduced a simple &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)&lt;/span&gt;for each datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt; toapproximate the posterior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;.To optimize w.r.t. <span class="math inline"><em>q</em></span>, weoptimize the parameters of each distribution. If &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(<em>z</em>)â€„=â€„ğ’©(<em>Î¼</em><sub><em>i</em></sub>,â€†<em>Ïƒ</em><sub><em>i</em></sub>)&lt;/span&gt;,we optimize each &lt;spanclass="math inline"&gt;<em>Î¼</em><sub><em>i</em></sub>,â€†<em>Ïƒ</em><sub><em>i</em></sub>&lt;/span&gt;.(<em>we can optimize the entropy value for sure, but Iâ€™m not entirelysure how you would take gradient of the expectation term &lt;spanclass="math inline"&gt;<em>E</em><sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>i</em></sub>(<em>z</em>)</sub>[logâ€†<em>p</em>(<em>z</em>)]&lt;/span&gt;</em>)</p></li> </ul> <p>Therefore, we have our learning algorithm: &lt;spanclass="math display"&gt;$$\begin{align}&amp;\text{for each $x_i$ in $\{x_1, \dots, x_N\}$: }\\&amp;\hspace{4mm} \text{sample $z \sim q_i(z)$}\\&amp;\hspace{4mm} \text{optimize against $p$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_\theta \mathcal L (p_\theta, q_i)= \nabla_\theta \log p_\theta(x_i|z) \\&amp;\hspace{4mm} \hspace{4mm} \theta \leftarrow \theta + \alpha\nabla_\theta \mathcal L (p, q_i) \\&amp;\hspace{4mm} \text{optimize against $q$:}\\&amp;\hspace{4mm} \hspace{4mm} \nabla_{\mu_i, \sigma_i} \mathcal L(p_\theta, q_i) = \nabla_{\mu_i, \sigma_i} \left[\mathbb E_{z\simq_{i}(z)} \left[\log p(x_{i}|z)+\log p(z) \right] + \mathcal H_{z\simq_{i}(z)} (q_i) \right] \\&amp;\hspace{4mm} \hspace{4mm} (\mu_i, \sigma_i) \leftarrow (\mu_i,\sigma_i) + \alpha \nabla_{\mu_i, \sigma_i} \mathcal L (p, q_i) \\\end{align}$$&lt;/span&gt;</p> <p>Thereâ€™s a problem with optimizing &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; though. Notewe have a separate <span class="math inline"><em>q</em></span> for eachdata point <span class="math inline"><em>i</em></span>, which means ifwe have <span class="math inline"><em>N</em></span> data points, we willhave to store &lt;spanclass="math inline"&gt;<em>N</em>â€…Ã—â€…(|<em>Î¼</em><sub><em>i</em></sub>|â€…+â€…|<em>Ïƒ</em><sub><em>i</em></sub>|)&lt;/span&gt;parameters assuming we chose &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>&lt;/span&gt; to beGaussian. In machine learning, the number of data points &lt;spanclass="math inline"&gt;<em>N</em>&lt;/span&gt; is usually in millions, makingthis model unwieldily big. Itâ€™s true in inference time we do not use<span class="math inline"><em>q</em></span> at all (weâ€™ll see why thisis true in the last chapter about VAE), but in training time, we stillneed them so itâ€™s necessary to keep all these parameters.</p> <p>Therefore, instead of having a separate &lt;spanclass="math inline"&gt;<em>q</em><sub><em>i</em></sub>(â‹…)&lt;/span&gt; toapproximate each data pointâ€™s &lt;spanclass="math inline"&gt;<em>P</em>(â‹…|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;specifically, we use a learnable model &lt;spanclass="math inline"&gt;<em>q</em><sub><em>Ï•</em></sub>(â‹…|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;to approximate &lt;spanclass="math inline"&gt;<em>p</em>(â‹…|<em>x</em><sub><em>i</em></sub>)&lt;/span&gt;This learnable network will take in a datapoint &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;, predicts thecorresponding &lt;spanclass="math inline"&gt;<em>Î¼</em><sub><em>i</em></sub>,â€†<em>Ïƒ</em><sub><em>i</em></sub>&lt;/span&gt;.We can then sample <span class="math inline"><em>z</em></span>â€‹ from thispredicted network.</p> <h2 id="amortized">Amortized</h2> <p>By adapting <span class="math inline"><em>q</em></span> to be alearnable network &lt;spanclass="math inline"&gt;<em>q</em><sub><em>Ï•</em></sub>&lt;/span&gt;â€‹ instead,model size does not depend on the number of datapoints anymore.Therefore, it is <strong>amortized</strong>.</p> <p>The variational lower bound becomes: &lt;spanclass="math display"&gt;â„’(<em>p</em><sub><em>Î¸</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>),â€†<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>))â€„=â€„ğ”¼<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[logâ€†<em>p</em><sub><em>Î¸</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)â€…+â€…logâ€†<em>p</em>(<em>z</em>)]â€…+â€…â„‹(<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>))&lt;/span&gt;The learning algorithm naturally becomes: $$ $$</p> <h3 id="gradient-over-expectation-policy-gradient">Gradient OverExpectation (Policy Gradient)</h3> <p>The question now boils down to how do we calculate this gradient&lt;spanclass="math inline"&gt;âˆ‡<sub><em>Ï•</em></sub>â„’(<em>p</em><sub><em>Î¸</em></sub>,â€†<em>q</em><sub><em>Ï•</em></sub>)&lt;/span&gt;.</p> <p>The second term entropy is easy. We purposefully chose &lt;spanclass="math inline"&gt;<em>q</em>&lt;/span&gt; to be a simple distribution, sothere is usually a close form of its entropy and we just have to look itup.</p> <p>The meat is in the first part. How do we take gradient w.r.t.parameter <span class="math inline"><em>Ï•</em></span> in the expectationtermâ€™s distribution &lt;spanclass="math inline"&gt;<em>q</em><sub><em>Ï•</em></sub>&lt;/span&gt; ? Note theterm inside expectation is independent of &lt;spanclass="math inline"&gt;<em>Ï•</em>&lt;/span&gt;, so we can rewrite it as &lt;spanclass="math inline"&gt;<em>R</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)â€„=â€„logâ€†<em>p</em><sub><em>Î¸</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)â€…+â€…logâ€†<em>p</em>(<em>z</em>)&lt;/span&gt;and call the whole thing &lt;spanclass="math inline"&gt;<em>J</em>&lt;/span&gt;.<br>&lt;spanclass="math display"&gt;<em>J</em>(<em>Ï•</em>)â€„=â€„âˆ‡<sub><em>Ï•</em></sub>ğ”¼<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)]&lt;/span&gt;We chose these namings purposefully because we encountered somethingsimilar back in the &lt;ahref="https://slides.com/sarahdean-2/sp24-cs-4789-lec-16?token=KNeurk-c#/11/0/4"&gt;policygradient part of reinforcement learning <strong>LINK???</strong>&lt;/a&gt;.Say we have a trajectory <span class="math inline"><em>Ï„</em></span>,sampled from the state transition function with learnable policy &lt;spanclass="math inline"&gt;<em>Ï€</em><sub><em>Î¸</em></sub>&lt;/span&gt;, the finalexpected value we can get from starting state &lt;spanclass="math inline"&gt;<em>s</em><sub>0</sub>&lt;/span&gt; can be written as thefollowing, where <span class="math inline"><em>R</em>(<em>Ï„</em>)</span>is a reward function returning the reward of this trajectory. &lt;spanclass="math display"&gt;<em>J</em>(<em>Î¸</em>)â€„=â€„<em>V</em><sup><em>Ï€</em><sub><em>Î¸</em></sub></sup>(<em>s</em><sub>0</sub>)â€„=â€„ğ”¼<sub><em>Ï„</em>â€„âˆ¼â€„<em>P</em><sub><em>s</em><sub>0</sub></sub><sup><em>Ï€</em><sub><em>Î¸</em></sub></sup></sub>[<em>R</em>(<em>Ï„</em>)]&lt;/span&gt;We can take the gradient of this value function &lt;spanclass="math inline"&gt;<em>V</em>&lt;/span&gt; w.r.t our policy &lt;spanclass="math inline"&gt;<em>Ï€</em><sub><em>Î¸</em></sub>&lt;/span&gt;, so this iscalled policy gradient. If youâ€™re unfamiliar with RL setup, you justhave to know we can derive the following gradient and we can approximateit by sampling <span class="math inline"><em>M</em></span> trajectories.$$ <span class="math display">$$Pugging in our $q$ and $\phi$,$$</span> $$</p> <h3 id="reparametrization-trick">Reparametrization Trick</h3> <p>We have our full learning algorithm and itâ€™s ready to go now.However, there is a tiny improvement we can do.</p> <p>We defined our &lt;spanclass="math inline"&gt;<em>q</em><sub><em>Ï•</em></sub>&lt;/span&gt; to be anormal distribution &lt;spanclass="math inline"&gt;ğ’©(<em>Î¼</em><sub><em>Ï•</em></sub>,â€†<em>Ïƒ</em><sub><em>Ï•</em></sub>)&lt;/span&gt;Observe that all normal distributions can be written as a function ofthe unit normal distribution. Therefore, a sample &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; is in effect: &lt;spanclass="math display"&gt;<em>z</em>â€„âˆ¼â€„ğ’©(<em>Î¼</em><sub><em>Ï•</em></sub>,â€†<em>Ïƒ</em><sub><em>Ï•</em></sub>)â€„â‡”â€„<em>z</em>â€„=â€„<em>Î¼</em><sub><em>Ï•</em></sub>â€…+â€…<em>Ïµ</em><em>Ïƒ</em><sub><em>Ï•</em></sub>,â€…<em>Ïµ</em>â€„âˆ¼â€„ğ’©(0,â€†1)&lt;/span&gt;Letâ€™s rewrite our expectation term to now sample an &lt;spanclass="math inline"&gt;<em>Ïµ</em>&lt;/span&gt; from the unit normal distributioninstead. By decomposing <span class="math inline"><em>z</em></span> intothese two parts, we separate the stochastic part and changed &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; from a sample of some stochasticdistribution into a deterministic function &lt;spanclass="math inline"&gt;<em>z</em>(<em>Ï•</em>,â€†<em>Ïµ</em>)&lt;/span&gt;parametrized by <span class="math inline"><em>Ï•</em></span> and randomvariable <span class="math inline"><em>Ïµ</em></span> that is independentof <span class="math inline"><em>Ï•</em></span>. &lt;spanclass="math inline"&gt;<em>Ïµ</em>&lt;/span&gt; takes the stochastic part alone.Our learnable parameter <span class="math inline"><em>Ï•</em></span> nowonly parametrizes deterministic quantity. &lt;spanclass="math display"&gt;âˆ‡<sub><em>Ï•</em></sub><em>J</em>(<em>Ï•</em>)â€„=â€„âˆ‡<sub><em>Ï•</em></sub>ğ”¼<sub><em>Ïµ</em>â€„âˆ¼â€„ğ’©(0,â€†1)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>Î¼</em><sub><em>Ï•</em></sub>â€…+â€…<em>Ïµ</em><em>Ïƒ</em><sub><em>Ï•</em></sub>)]&lt;/span&gt;Aside from these theoretical benefits, mathematically, we do not have totake gradient w.r.t an expectation of parametrized distribution anymore.Instead, the gradient can go straight into the expectation term now likehow we usually interchange gradient and expectation (think aboutdiscrete case, expectation is just a big sum so we can do it). &lt;spanclass="math display"&gt;âˆ‡<sub><em>Ï•</em></sub><em>J</em>(<em>Ï•</em>)â€„=â€„ğ”¼<sub><em>Ïµ</em>â€„âˆ¼â€„ğ’©(0,â€†1)</sub>[âˆ‡<sub><em>Ï•</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>Î¼</em><sub><em>Ï•</em></sub>â€…+â€…<em>Ïµ</em><em>Ïƒ</em><sub><em>Ï•</em></sub>)]&lt;/span&gt;Further, to approximate this expectation, we just sample some &lt;spanclass="math inline"&gt;<em>Ïµ</em>&lt;/span&gt; from this normal distribution.<span class="math display">$$\nabla_\phi J(\phi)\approx \frac 1 M \sum_j^M \nabla_\phi R(x_i, \mu_\phi + \epsilon_j\sigma_\phi)$$</span></p> <p>With reparametrization, we achieve a lower variance than policygradient because we are using the derivative of R. (<em>Unfortunatelythe lecturer didnâ€™t provide a quantitative analysis on this and I donâ€™tknow how to prove it</em>) On the other hand, previously, we only tookderivative w.r.t. the probability distribution. Why didnâ€™t we usederivative of R back in RL with policy gradient? Itâ€™s not we donâ€™t wantto but we canâ€™t: we canâ€™t use reparametrization in RL because in RL weusually cannot take derivative w.r.t. reward R.</p> <table> <colgroup> <col style="width: 6%"> <col style="width: 23%"> <col style="width: 23%"> <col style="width: 23%"> <col style="width: 23%"> </colgroup> <thead><tr class="header"> <th>Method</th> <th>Formula</th> <th>Approximation</th> <th>Benefit</th> <th>Deficit</th> </tr></thead> <tbody> <tr class="odd"> <td>Policy Gradient</td> <td>&lt;spanclass="math inline"&gt;âˆ‡<sub><em>Ï•</em></sub>ğ”¼<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>â€…âˆ£â€…<em>x</em><sub><em>i</em></sub>)</sub>[<em>R</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>z</em>)]&lt;/span&gt;</td> <td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi[\logq_\phi(z_j \mid x_i)] R(x_i,z_j)$</span></td> <td>works with both discrete and continuous latent variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;</td> <td>High variance, requires multiple samples &amp; small learningrates</td> </tr> <tr class="even"> <td>Reparametrization</td> <td>&lt;spanclass="math inline"&gt;ğ”¼<sub><em>Ïµ</em>â€„âˆ¼â€„ğ’©(0,â€†1)</sub>[âˆ‡<sub><em>Ï•</em></sub><em>R</em>(<em>x</em><sub><em>i</em></sub>,â€†<em>Î¼</em><sub><em>Ï•</em></sub>â€…+â€…<em>Ïµ</em><em>Ïƒ</em><sub><em>Ï•</em></sub>)]&lt;/span&gt;</td> <td><span class="math inline">$\frac 1 M \sum_j^M \nabla_\phi R(x_i,\mu_\phi + \epsilon_j \sigma_\phi)$</span></td> <td>low variance, simple to implement (weâ€™ll see soon)</td> <td>only works with continuous variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; and have to model it with aGaussian</td> </tr> </tbody> </table> <p>In fact, you can forget about the policy gradient method and simplytake it for granted that you cannot back propagate a sampled value &lt;spanclass="math inline"&gt;âˆ‡<sub><em>Ï•</em></sub>ğ”¼<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>&lt;/span&gt;,so you have to find some way to make our &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;â€‹ deterministic, which is what weâ€™redoing here with our reparametrization trick.</p> <figure>&lt;img src="/images/reparametrization-trick.png"alt="reparametrization-trick" /&gt;<figcaption aria-hidden="true">reparametrization-trick</figcaption></figure> <p>Left is without the â€œreparameterization trickâ€, and right is with it.Red shows sampling operations that are non-differentiable. Blue showsloss layers. We forward the network by going up and back propagate it bygoing down. The forward behavior of these networks is identical, butback propagation can be applied only to the right network. Figure copiedfrom <a href="https://arxiv.org/abs/1606.05908" rel="external nofollow noopener" target="_blank">Carl Doersch: Tutorialon Variational Autoencoders</a></p> <h3 id="looking-at-mathcal-l-directly">Looking at &lt;spanclass="math inline"&gt;â„’&lt;/span&gt; Directly</h3> <p><span class="math display">$$\begin{align}\mathcal L_i = \mathcal L \left( p_\theta(x_i | z), q_\phi(z | x_i)\right)&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)+\log p(z) \right] + \mathcal H (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p_\theta(x_{i}|z)\right] + \mathbb E_{z\sim q_\phi(z | x_i)} \left[\log p(z) \right] + \mathcalH (q_\phi(z|x_i))\\&amp;= \mathbb E_{z\sim q_\phi(z | x_i)} \left[\logp_\theta(x_{i}|z)\right] - D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;= \mathbb E_{\epsilon \sim \mathcal N(0,1)} \left[\logp_\theta(x_{i}| \mu_\phi + \epsilon \sigma_\phi)\right] -D_{KL}(q_\phi(z | x_i)\|p(z)) \\&amp;\approx \frac 1 M \sum_j^M \log p_\theta(x_{i}| \mu_\phi +\epsilon_j \sigma_\phi) - D_{KL}(q_\phi(z | x_i)\|p(z)) \\\end{align}$$</span></p> <p>For the first term, we can just evaluate it. For the second KL term,since we chose both distributions to be easy (in this case Gaussian),there often is a nice analytical form for it.</p> <p>Therefore, we can go ahead to maximize the variational lower bound<span class="math inline">â„’</span>â€‹. We can also draw out the followingcomputational graph for the log term and conclude we can back propagatethis graph without any problem. On the other hand, if we didnâ€™t do thereparametrization trick, we will get stuck at &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;: you cannot back propagate &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; - a sampled value instead of avariable. And we will have to seek help from policy gradient. Withreparametrization, we decompose &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; into two variables &lt;spanclass="math inline"&gt;<em>Î¼</em><sub><em>Ï•</em></sub>,â€†<em>Ïƒ</em><sub><em>Ï•</em></sub>&lt;/span&gt;we can back propagate through and one stochastic value &lt;spanclass="math inline"&gt;<em>Ïµ</em>&lt;/span&gt; we do not care about.</p> <figure><img src="/images/computational-graph.png" alt="computational-graph"><figcaption aria-hidden="true">computational-graph</figcaption></figure> <h2 id="variational-autoencoder">Variational Autoencoder</h2> <h3 id="setup-and-interpretation">Setup and Interpretation</h3> <p>What we have gone though constitutes the full pipeline of avariational autoencoder.</p> <p>In a variation autoencoder, we have observed variable &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; and latent variable &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;</p> <ul> <li>encoder &lt;spanclass="math inline"&gt;<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em>)â€„=â€„ğ’©(<em>Î¼</em><sub><em>Ï•</em></sub>(<em>x</em>),â€†<em>Ïƒ</em><sub><em>Ï•</em></sub>(<em>x</em>))&lt;/span&gt;</li> <li>decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Î¸</em></sub>(<em>x</em>|<em>z</em>)â€„=â€„ğ’©(<em>Î¼</em><sub><em>Î¸</em></sub>(<em>z</em>),â€†<em>Ïƒ</em><sub><em>Î¸</em></sub>(<em>z</em>))&lt;/span&gt;</li> </ul> <p>In training, given an observed sample &lt;spanclass="math inline"&gt;<em>x</em><sub><em>i</em></sub>&lt;/span&gt;, we encode itto latent variable &lt;spanclass="math inline"&gt;<em>z</em><sub><em>i</em></sub>&lt;/span&gt; using &lt;spanclass="math inline"&gt;<em>q</em><sub><em>Ï•</em></sub>&lt;/span&gt;, then triesto decode it back with decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Î¸</em></sub>&lt;/span&gt;. We maximizethe variational lower bound during the process. For all &lt;spanclass="math inline"&gt;<em>N</em>&lt;/span&gt; samples, the training objectivelooks like: (where the <span class="math inline"><em>Ïµ</em></span> is asampled value) <span class="math display">$$\max_{\phi,\theta} \frac 1 N \sum_i^N \log p_\theta\left(x_{i}|\mu_\phi(x_i) + \epsilon \sigma_\phi(x_i)\right) - D_{KL}(q_\phi(z |x_i)\|p(z)) \\$$</span> In inference (generation), we sample a &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt; from our prior &lt;spanclass="math inline"&gt;<em>p</em>(<em>z</em>)&lt;/span&gt;, then decode it using<span class="math inline"><em>p</em><sub><em>Î¸</em></sub></span>: &lt;spanclass="math inline"&gt;<em>z</em>â€„âˆ¼â€„<em>p</em>(<em>z</em>),â€†<em>x</em>â€„âˆ¼â€„<em>p</em><sub><em>Î¸</em></sub>(<em>x</em>|<em>z</em>)&lt;/span&gt;</p> <p>Why does the variational autoencoder work? We talked about manybenefits of maximizing this variational lower bound in &lt;ahref="#Effect-of-Pushing-Up-ELBO-(Analytically)"&gt;previous chapter&lt;/a&gt;.Letâ€™s look at it again in this decoder-encoder setup,. &lt;spanclass="math display"&gt;â„’<sub><em>i</em></sub>â€„=â€„ğ”¼<sub><em>z</em>â€„âˆ¼â€„<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)</sub>[logâ€†<em>p</em><sub><em>Î¸</em></sub>(<em>x</em><sub><em>i</em></sub>|<em>z</em>)]â€…âˆ’â€…<em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em><sub><em>Ï•</em></sub>(<em>z</em>|<em>x</em><sub><em>i</em></sub>)âˆ¥<em>p</em>(<em>z</em>))&lt;/span&gt;</p> <ul> <li>The first &lt;spanclass="math inline"&gt;logâ€†<em>p</em><sub><em>Î¸</em></sub>&lt;/span&gt; termmaximizes the probability of our observed image &lt;spanclass="math inline"&gt;<em>x</em>&lt;/span&gt; given a sample &lt;spanclass="math inline"&gt;<em>z</em>&lt;/span&gt;, so the model makes decoder &lt;spanclass="math inline"&gt;<em>p</em><sub><em>Î¸</em></sub>&lt;/span&gt; toreconstruct image <span class="math inline"><em>x</em></span>â€‹ asaccurate as possible.</li> <li>The second KL term restricts the encoding of an image to be close tothe actual prior, which makes sure at inference / generate time, we candirectly sample from the prior.</li> </ul> <h3 id="comparison-with-auto-encoder">Comparison with Auto-Encoder</h3> <figure><img src="/images/vae-and-ae.png" alt="vae-and-ae"><figcaption aria-hidden="true">vae-and-ae</figcaption></figure> <p>The VAEâ€™s decoder is trained to convert random points in theembedding space (generated by perturbing the input encodings) tosensible outputs. By contrast, the decoder for the deterministicautoencoder only ever gets as inputs the exact encodings of the trainingset, so it does not know what to do with random inputs that are outsidewhat it was trained on. So a standard autoencoder cannot create newsamples.</p> <p>The reason the VAE is better at sample is that it embeds images intoGaussians in latent space, whereas the AE embeds images into points,which are like delta functions. The advantage of using a latentdistribution is that it encourages local smoothness, since a given imagemay map to multiple nearby places, depending on the stochastic sampling.By contrast, in an AE, the latent space is typically not smooth, soimages from different classes often end up next to each other. Figurecopied from &lt;ahref="https://probml.github.io/pml-book/book1.html"&gt;ProbabilisticMachine Learning: An Introduction - Figure 20.26&lt;/a&gt;</p> <p>We can leverage the smoothness of the latent space to perform imageinterpolation in latent space.</p> <h2 id="reference">Reference</h2> <p>Most content of this blog post comes from &lt;ahref="https://www.youtube.com/watch?v=UTMpM4orS30"&gt;Berkeley CS 285(Sergey Levine): Lecture 18, Variational Inference&lt;/a&gt;, which I thinkorganized his lecture based on &lt;ahref="https://arxiv.org/abs/1906.02691"&gt;An Introduction to VariationalAutoencoders&lt;/a&gt; (2.1-2.7, and 2.9.1), or more in-depth on the authorâ€™sPhD thesis <a href="http://dpkingma.com/#phdthesis" rel="external nofollow noopener" target="_blank">VariationalInference and Deep Learning: A New Synthesis</a> I found this wonderfultutorial in &lt;ahref="https://probml.github.io/pml-book/book2.html"&gt;ProbabilisticMachine Learning: Advanced Topics&lt;/a&gt;</p> <p>Some graph come from &lt;ahref="https://probml.github.io/pml-book/book1.html"&gt;ProbabilisticMachine Learning: An Introduction&lt;/a&gt; itself and &lt;ahref="https://arxiv.org/abs/1606.05908"&gt;Carl Doersch: Tutorial onVariational Autoencoders&lt;/a&gt;, which is referenced in the previousbook.</p> <p>Note though the <em>Probabilistic Machine Learning</em> book itselfis a horrible book with extremely confusing explanations.</p> </body></html>