<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>So Google, fxxk you.</p> <p><span id="more"></span>&lt;h2 id="prerequsities"&gt;Prerequsities&lt;/h2&gt;&lt;p&gt;This picture very well explains how TFLite works and also whyTensorFlow 2 has both a <code>tf</code> and a <code>keras</code>.&lt;/p&gt;<figure>&lt;imgsrc=”https://web.archive.org/web/20220216170621if_/https://www.tensorflow.org/lite/images/convert/workflow.svg”alt=”TFLite Workflow” /&gt;&lt;figcaption aria-hidden="true"&gt;TFLite Workflow&lt;/figcaption&gt;</figure>&lt;h2 id="detours"&gt;Detours&lt;/h2&gt;&lt;p&gt;This section is mostly rant, but it is meaningful in preventing youfrom taking any of the wrong path. Skip to the next section for atutorial on what to do.&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;&lt;p&gt;We first found the Google’s official release &lt;ahref=”https://github.com/google-research/google-research/tree/master/mobilebert”&gt;http://google-research/mobilebert/&lt;/a&gt;,but&lt;/p&gt;&lt;ul&gt;&lt;li&gt;the tutorial was unclear: Why do I need <code>data_dir</code> and<code>output_dir</code> to export TFLite? How do I even read in thepre-trained weights?&lt;/li&gt;&lt;li&gt;the code itself was pretty messy: why did they have export functionand training function all at this same file <code>run_squad.py</code>and the only way to tell the program whether to train/export is checkingwhether <code>export_dir is None</code> rather than passing a flag?&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In figuring out what each part does in this code, I looked upTensorFlow 1’s doc and good lord they were broken. Google doesn’t evenhost it anywhere: you have to go to &lt;ahref=”https://github.com/tensorflow/docs/tree/master/site/en/r1”&gt;aGitHub repo&lt;/a&gt; to read them in <code>.md</code> format. At this momentI decided I will not touch anything written by TensorFlow 1’s API. (Iactually went through this pain back at my first ML intern in Haier, butnot again)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Sidenote before this: I didn’t know you can release model’s onKaggle (thought everyone releases on Hugging Face) and Google &lt;ahref=”https://www.kaggle.com/discussions/product-feedback/448425”&gt;movedtheir own TensorFlow Hub to Kaggle&lt;/a&gt;&lt;/p&gt;&lt;p&gt;So my supervisor found me &lt;ahref=”https://www.kaggle.com/models/google/mobilebert/tensorFlow1”&gt;amore readable Google release on Kaggle&lt;/a&gt; with some high-level API anddoesn’t require you to read the painful source code. The above link has<a href="https://www.kaggle.com/models/tensorflow/mobilebert" rel="external nofollow noopener" target="_blank">a redirectto TensorFlow 2 implementation</a> with an official TFLite release. Howneat.&lt;/p&gt;&lt;p&gt;However, the official TFLite release&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;doesn’t have &lt;ahref=”https://www.tensorflow.org/lite/guide/signatures”&gt;signature&lt;/a&gt; -TensorFlow’s specification of input and output (remember when you passinputs to a model you need to give name to theme.g. <code>token_ids = ..., mask = ...</code>) which is required forXiaomi Service Framework to run a TFLite. P.S. Yes signature is notrequired to specify when exporting, but for god’s sake all your tutorialteaches people to use it and your own released ditched it? WTFGoogle.&lt;/li&gt;&lt;li&gt;is broken (as expected?). &lt;ahref=”forgot%20where%20the%20guide%20was”&gt;When I tried to run it on myPC&lt;/a&gt;, I got the following error<code>indices_has_only_positive_elements was not true.gather index out of boundsNode number 2 (GATHER) failed to invoke.gather index out of boundsNode number 2 (GATHER) failed to invoke</code>.Someone encountered &lt;ahref=”https://github.com/tensorflow/tensorflow/issues/59730”&gt;a similarbug&lt;/a&gt; while running the example code provided by TensorFlow and theGoogle SWE found a bug in their example. At this moment I decided not totrust this TFLite file anymore and just convert it on my own.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;So let’s use this official TensorFlow 2 implementation and &lt;ahref=”forgot%20where%20the%20guide%20was”&gt;convert it to TFLite&lt;/a&gt;. Itwas all good and running on my PC, but&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Its output format was really weird&lt;ul&gt;&lt;li&gt;It output consists of<code>'mobile_bert_encoder', 'mobile_bert_encoder_1', 'mobile_bert_encoder_2', ..., 'mobile_bert_encoder_51'</code>&lt;/li&gt;&lt;li&gt;Each of these has shape <code>(1, 4, 128, 128)</code> for a<code>seq_length = 128, hidden_dim = 512</code> model. I figured 4 beingthe number of heads and the other 128 is <code>hidden_dim</code> foreach head.&lt;/li&gt;&lt;li&gt;They output attention scores, not the final encoded vector: my inputwas 5 tokens and they output is<code>output[0, 0, 0, :] = array([0.198, 0.138, 0.244, 0.148, 0.270, 0. , 0. , ...</code>.They sum to 1 and any other positions at <code>output</code> are 0 , soattention score was my best guess.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;It doesn’t run on Android phone:<code>tflite engine load failed due to java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Op builtin_code out of range: 153. Are you using old TFLite binary with newer model?</code>A &lt;ahref=”https://stackoverflow.com/questions/67883156/tflite-runtime-op-builtin-code-out-of-range-131-are-you-using-old-tflite-bi”&gt;StackOverflow answer&lt;/a&gt; suggests the TensorFlow used to export TFLiterunning on my PC doesn’t match the version of TFLite run time on thisAndroid phone. It can also be caused by me messing up with the wholeenvironment while installing &lt;ahref=”https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model”&gt;Optimum&lt;/a&gt;to export TFLite last night, but I didn’t bother to look because Ifinally found the solution&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;And comes the savior, the king, the go-to solution in MLOps -Huggingface. Reminded by a discussion I read by chance, I came torealize <code>TFMobileBertModel.from_pretrained</code> actually returnsthe Keras model (and the without <code>TF</code> version returns aPyTorch model). That means I can just use Hugging Face API to read itin, then use the native TensorFlow 2 API to export to TFLite. Andeverything works like a charm now. The final output signature is justHugging Face’s familiar<code>['last_hidden_state', 'pooler_output']</code>&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2 id="converting-tensorflow-model-to-tflite"&gt;Converting TensorFlowModel to TFLite&lt;/h2&gt;&lt;p&gt;Conversion is pretty straight forward. You can just follow thisofficial guide: &lt;ahref=”https://www.tensorflow.org/lite/models/convert/convert_models”&gt;ForMobile &amp; Edge: Convert TensorFlow models&lt;/a&gt;. Though I actuallyfollowed my predecessor’s note (which actually comes from &lt;ahref=”https://www.tensorflow.org/lite/guide/signatures”&gt;another TFtutorial&lt;/a&gt;). He also told me to caution that calling<code>tf.disable_eager_execution()</code> can lead to absence ofsignature, so do not call <code>tf.disable_eager_execution()</code> todisable eager mode.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertTokenizerFast, TFMobileBertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert Model</span></span><br><span class="line"><span class="keyword">if</span> be_sane:</span><br><span class="line"> bert_model = TFMobileBertModel.from_pretrained(kerasH5_model_path) <span class="keyword">if</span> keras_file <span class="keyword">else</span> &lt;/span&gt;<br><span class="line"> TFMobileBertModel.from_pretrained(pytorch_model_path, from_pt = <span class="literal">True</span>)</span><br><span class="line"> converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line"><span class="keyword">else</span>: <span class="comment"># be crazy or already knows the messy TensorFlow.SavedModel format</span></span><br><span class="line"> converter = tf.lite.TFLiteConverter.from_saved_model(model_path)</span><br><span class="line">tflite_model = converter.convert()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save Model</span></span><br><span class="line">tflite_output_path = <span class="string">'/model.tflite'</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(tflite_output_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line"> f.write(tflite_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Signature</span></span><br><span class="line"><span class="comment"># Empty signature means error in the export process and the file cannot be used by Xiaomi Service Framework</span></span><br><span class="line">interpreter = tf.lite.Interpreter(model_path=tflite_output_path)</span><br><span class="line">interpreter = tf.lite.Interpreter(model_content=tflite_model)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line">signatures = interpreter.get_signature_list()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"tflite model signatures:"</span>, signatures)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;blockquote&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">{<span class="string">'serving_default'</span>: {<span class="string">'inputs'</span>: [<span class="string">'attention_mask'</span>,</span><br><span class="line"><span class="string">'input_ids'</span>,</span><br><span class="line"><span class="string">'token_type_ids'</span>],</span><br><span class="line"><span class="string">'outputs'</span>: [<span class="string">'last_hidden_state'</span>, <span class="string">'pooler_output'</span>]}}</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/blockquote&gt;&lt;p&gt;In addition, summarizing from the detours I took,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Do not use Hugging Face’s Optimum for (at least vanilla) conversionbecause it just calls the above command (see &lt;ahref=”https://github.com/huggingface/optimum/blob/e0f58121140ce4baa01919ad70a6c13e936f7605/optimum/exporters/tflite/convert.py#L363-L371”&gt;code&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;Do not even bother to look at &lt;ahref=”https://github.com/google-research/google-research/tree/master/mobilebert#export-mobilebert-to-tf-lite-format”&gt;Google’soriginal code&lt;/a&gt; converting MobileBert to TFLite because nobody knowswhat they’re writing.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="running-tflite-on-pc"&gt;Running TFLite (on PC)&lt;/h2&gt;&lt;p&gt;Running TFLite on Android phone is the other department’s task. Ijust want to run the TFLite file on PC to test everything’s good. To dothat, I strictly followed TensorFlow’s official guide: &lt;ahref=”https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python”&gt;<strong>TensorFlowLite inference: Load and run a model in Python</strong>&lt;/a&gt;.Ourconverted models have the signatures, you can just follow the “with adefined SignatureDef” guide.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">tokenizer = MobileBertTokenizerFast(<span class="string">f"<span class="subst">{model_path}</span>/vocab.txt"</span>)</span><br><span class="line">t_output = tokenizer(<span class="string">"越过长城，走向世界"</span>, return_tensors=<span class="string">"tf"</span>)</span><br><span class="line">ii, tt, am = t_output[<span class="string">'input_ids'</span>], t_output[<span class="string">'token_type_ids'</span>], t_output[<span class="string">'attention_mask'</span>]</span><br><span class="line"><span class="comment"># <code class="language-plaintext highlighter-rouge">get_signature_runner()</code> with empty input gives the "serving_default" runner</span></span><br><span class="line"><span class="comment"># <code class="language-plaintext highlighter-rouge">runner</code> input parameter is specified by <code class="language-plaintext highlighter-rouge">serving_default[&amp;#x27;inputs&amp;#x27;]</code></span></span><br><span class="line">runner = interpreter.get_signature_runner() </span><br><span class="line">output = runner(input_ids = ii, token_type_ids = tt, attention_mask = am)</span><br><span class="line"><span class="keyword">assert</span> output.keys == [<span class="string">'last_hidden_state'</span>, <span class="string">'pooler_output'</span>]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;On the other hand, for a model without signatures, you need to usethe more primitive API <code>input_details</code> and<code>output_details</code>. They specify the following properties,where <code>index</code> is (probably) the index of this tensor in thecompute graph. To pass input values and get output values, you need toaccess them by this index.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">interpreter.allocate_tensors()</span><br><span class="line">input_details = interpreter.get_input_details()</span><br><span class="line">output_details = interpreter.get_output_details()</span><br><span class="line"></span><br><span class="line">interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">'index'</span>], input_data)</span><br><span class="line">interpreter.invoke()</span><br><span class="line">output_data = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">'index'</span>])</span><br><span class="line"><span class="built_in">print</span>(output_data)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;The following is the <code>input_details</code> of the non-signatureGoogle packed MobileBert.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">interpreter.get_input_details()</span><br><span class="line">[{<span class="string">'name'</span>: <span class="string">'model_attention_mask:0'</span>,</span><br><span class="line"> <span class="string">'index'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'shape'</span>: array([ <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br><span class="line"> <span class="string">'shape_signature'</span>: array([ <span class="number">1</span>, <span class="number">512</span>], dtype=int32),</span><br><span class="line"> <span class="string">'dtype'</span>: numpy.int64,</span><br><span class="line"> <span class="string">'quantization'</span>: (<span class="number">0.0</span>, <span class="number">0</span>),</span><br><span class="line"> <span class="string">'quantization_parameters'</span>: {<span class="string">'scales'</span>: array([], dtype=float32),</span><br><span class="line"> <span class="string">'zero_points'</span>: array([], dtype=int32),</span><br><span class="line"> <span class="string">'quantized_dimension'</span>: <span class="number">0</span>},</span><br><span class="line"> <span class="string">'sparsity_parameters'</span>: {}},</span><br><span class="line"> {…}]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;h2 id="numerical-accuracy"&gt;Numerical Accuracy&lt;/h2&gt;&lt;p&gt;Our original torch/TensorFlow encoder and the converted TFLiteencoder, when both running on PC using Python, has a 1.2% difference intheir output (<code>last_hidden_state</code> or<code>pooled_output</code>). We <strong>do not know</strong> where thisdiscrepancy comes from.&lt;/p&gt;&lt;h2 id="converting-tokenizer-to-tflite"&gt;Converting Tokenizer toTFLite&lt;/h2&gt;&lt;p&gt;We exported and ran the <em>encoder</em>, but that’s not enough. Wecan’t ask the user to type in <code>token_ids</code> every time.Therefore, we need to integrate the preprocessor (tokenizer) into ourTFLite file. To do that, we first tried integrating &lt;ahref=”https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/multi-cased-preprocess/3”&gt;Google’sofficial Keras tokenizer implementation&lt;/a&gt; into our BERT model andconvert them together into a TFLite (yeah I didn’t learn the lesson).This failed in the converting step for reasons that would become clearlater. And we switched gears to follow some other guide and first try toconvert a standalone tokenizer to TFLite.&lt;/p&gt;&lt;p&gt;Tokenizer is a part of the TensorFlow Text library. I followed the &lt;ahref=”https://www.tensorflow.org/text/guide/text_tf_lite”&gt;<strong>officialguide: Converting TensorFlow Text operators to TensorFlowLite</strong>&lt;/a&gt; with <code>text.FastBertTokenizer</code>. Note whenyou follow it, do it carefully and closely. I encountered a few problemsalong the way:&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;&lt;p&gt;When you change the <code>text.WhitespaceTokenizer</code> inguide to our <code>text.FastBertTokenizer</code>, remember to specify a<code>text.FastBertTokenizer(vocab=vocab_lst)</code>. We need not thepath to the vocab but the actual liste.g. <code>[ "[PAD]", "[unused0]", "[unused1]", ...]</code> describesthe vocab where <code>[PAD]</code> maps to token id 0,<code>[unused0]</code> to token id 1, and so on.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>text.FastBertTokenizer</code> (or the standard version)does not add <code>[CLS]</code> token for you. Google says this is tomake sure “you are able to manipulate the tokens and determine how toconstruct your segments separately” (&lt;ahref=”https://github.com/tensorflow/text/issues/146”&gt;GitHub issue&lt;/a&gt;).How considerate you are, dear Google. I spent one and a half dayfiguring out how to add these tokens when the model’s input length needsto be fixed, otherwise it triggers TensorFlow’s compute graph to throw“can’t get variable-length input” error. I finally found a solution in&lt;ahref=”https://github.com/google-ai-edge/mediapipe/blob/a91256a42bbe49f8ebdb9e2ec7643c5c69dbec6f/mediapipe/model_maker/python/text/text_classifier/bert_tokenizer.py#L58-L71”&gt;Google’smediapipe’s implementation&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>Could not translate MLIR to FlatBuffer</code> when running<code>tflite_model = converter.convert()</code>: as mentioned, you mustfollow the guide very carefully. The guide specifies a TensorFlow Textversion. If not this version, the conversion would fail&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">pip install -U <span class="string">"tensorflow-text==2.11.*"</span></span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;<code>Encountered unresolved custom op: FastBertNormalize</code>when running converted interpreter / signature: as stated in the &lt;ahref=”https://www.tensorflow.org/text/guide/text_tf_lite#inference”&gt;Inferencesection of the guide&lt;/a&gt;, tokenizers are custom operations and need tobe specified when running inference. (I can’t find doc for<code>InterpreterWithCustomOps</code> anywhere but it does have anargument <code>model_path</code>)&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">interp = interpreter.InterpreterWithCustomOps(</span><br><span class="line"> model_content=tflite_model,<span class="comment"># or model_path=TFLITE_FILE_PATH</span></span><br><span class="line"> custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;TensorFlow Text custom ops are not found on Android: the aboveinference guide writes&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;while the example below shows inference in Python, the steps aresimilar in other languages with some minor API translations&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;which is a total lie. Android does not support these operations asthe &lt;ahref=”https://www.tensorflow.org/lite/guide/op_select_allowlist#tensorflow_text_and_sentencepiece_operators”&gt;customtext op list&lt;/a&gt; only mentions python support.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;At the end, I did manage to 1 merge the above tokenizer andHuggingFace model, 2 export a TFLite model that reads in a text andoutputs the last hidden state. However, I seem to have lost that pieceof the code. Don’t worry though. Because thanks to Google’s shittyframework, it only works with very few tokenizer implementations anyway.The work-for-all solution is to build your own tokenizer in Java.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;P.S. While debugging the FlatBuffer error, I came across the &lt;ahref=”https://www.tensorflow.org/lite/guide/authoring”&gt;TensorFlowauthoring tool&lt;/a&gt; that can explicitly specify a function’s input outputformat and detect op unsupported by TFLite. However, the tools is prettybroken for me. Debugging this tool would probably take longer thanfinding the problem yourself online / ask on a forum.&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id="writing-your-own-tokenizer"&gt;Writing Your Own Tokenizer&lt;/h2&gt;&lt;p&gt;What’s weird is TensorFlow does have an official BERT on Androidexample. Reading it again, I found their tokenizer is actuallyimplemented by C++ (&lt;ahref=”https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier#key_features_of_the_bertnlclassifier_api”&gt;seethis example&lt;/a&gt;). The repo containing the tokenizer code is called &lt;ahref=”https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/text/tokenizers/bert_tokenizer.h”&gt;tflite-support&lt;/a&gt;.Finding &lt;ahref=”https://www.tensorflow.org/lite/inference_with_metadata/lite_support#current_use-case_coverage”&gt;thislibrary’s doc&lt;/a&gt;, it becomes clear that the text-related operations arecurrently not supported.&lt;/p&gt;<figure>&lt;img src=”/images/tflite-support.png”alt=”TFLite-Support Current use-case coverage” /&gt;&lt;figcaption aria-hidden="true"&gt;TFLite-Support Current use-casecoverage&lt;/figcaption&gt;</figure>&lt;p&gt;Google seems to have used JNI to call the C++ implementation oftokenizer (&lt;ahref=”https://github.com/tensorflow/tflite-support/blob/8ed4a7b70df385a253aad7ed7df782439f42da6c/tensorflow_lite_support/java/src/java/org/tensorflow/lite/task/text/nlclassifier/BertNLClassifier.java#L39-L53”&gt;seecode&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;Therefore, we’d better write our own tokenizer. Luckily Hugging Facealso has a Bert on Android example - &lt;ahref=”https://github.com/huggingface/tflite-android-transformers/tree/master/bert”&gt;tflite-android-transformers&lt;/a&gt;and writes more accessible code. We directly copied &lt;ahref=”https://github.com/huggingface/tflite-android-transformers/tree/master/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization”&gt;theirtokenizer implementation&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;However, when switching to Chinese vocabulary, the tokenizer goesglitchy. See the following example where we tokenize thesentence「越过长城 ，走向世界」&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="comment"># Our Java tokenizer gives the following tokens, which detokenizes to the following string</span></span><br><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">19871</span>, <span class="number">20327</span>, <span class="number">14871</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">14460</span>, <span class="number">13743</span>, <span class="number">17575</span>, <span class="number">102</span>])</span><br><span class="line"><span class="string">'[CLS] 越过长城 ， 走向世界 [SEP]'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># On the other hand, official Hugging Face python BertTokenizer gives</span></span><br><span class="line">tokenizer.decode([<span class="number">101</span>, <span class="number">6632</span>, <span class="number">6814</span>, <span class="number">7270</span>, <span class="number">1814</span>, <span class="number">8024</span>, <span class="number">6624</span>, <span class="number">1403</span>, <span class="number">686</span>, <span class="number">4518</span>, <span class="number">102</span>])</span><br><span class="line"><span class="string">'[CLS] 越 过 长 城 ， 走 向 世 界 [SEP]'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inspecting the first difference, our Java tokenizer seems to have used sentencepiece </span></span><br><span class="line">tokenizer.decode([<span class="number">19871</span>])</span><br><span class="line"><span class="string">'##过'</span></span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;It turns out &lt;ahref=”https://github.com/google-research/bert/blob/master/multilingual.md#tokenization”&gt;BERTin its original implementation&lt;/a&gt; (&lt;ahref=”https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/tokenization.py#L207”&gt;code&lt;/a&gt;)does not use sentence-piece tokenizer on Chinese characters. Instead, ituses character level tokenizer. Therefore, we need to first insert awhitespace to every character to ensure sentence-piece isn’t applied.Note Hugging Face tokenizer follows BERT original python code veryclosely so you can &lt;ahref=”https://github.com/huggingface/tflite-android-transformers/blob/dcd6da1bfb28e3cd6bc83b58a112cdcd3d6cc2fe/bert/src/main/java/co/huggingface/android_transformers/bertqa/tokenization/BasicTokenizer.java#L34”&gt;easilyfind where to insert&lt;/a&gt; that piece of code.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Bert original implementation in Python, with Chinese logic&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, text</span>):</span><br><span class="line"> <span class="string">"""Tokenizes a piece of text."""</span></span><br><span class="line"> text = convert_to_unicode(text)</span><br><span class="line"> text = <span class="variable language_">self</span>._clean_text(text)</span><br><span class="line"> <span class="comment"># Chinese Logic</span></span><br><span class="line"> text = <span class="variable language_">self</span>._tokenize_chinese_chars(text)</span><br><span class="line"> orig_tokens = whitespace_tokenize(text)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hugging Face tokenizer in Java, without Chinese logic&lt;/p&gt;<figure class="highlight java">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">BasicTokenizer</span> {</span><br><span class="line"> <span class="keyword">public</span> List&lt;String&gt; <span class="title function_">tokenize</span><span class="params">(String text)</span> {</span><br><span class="line"> <span class="type">String</span> <span class="variable">cleanedText</span> <span class="operator">=</span> cleanText(text);</span><br><span class="line"> <span class="comment">// Insert Here</span></span><br><span class="line"> List&lt;String&gt; origTokens = whitespaceTokenize(cleanedText);</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="building-a-classifier"&gt;Building a Classifier&lt;/h2&gt;&lt;p&gt;The final task is actually to build a classifier of 28 online storecommodity classes. As I mentioned in the <a href="#Detours">Detourssection</a>, I do not know and don’t wanna bother to know how to defineor change a signature. Therefore, I again turn to Hugging Face for its<code>MobileBertForSequenceClassification</code>.&lt;/p&gt;&lt;p&gt;The default classification head only has 1 layer, I changed itsstructure to give it more expressive power.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">model = MobileBertForSequenceClassification.from_pretrained(</span><br><span class="line"> model_path, num_labels=<span class="built_in">len</span>(labels), problem_type=<span class="string">"multi_label_classification"</span>,</span><br><span class="line"> id2label=id2label, label2id=label2id)</span><br><span class="line">model.classifier = nn.Sequential(OrderedDict([</span><br><span class="line"> (<span class="string">'fc1'</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br><span class="line"> (<span class="string">'relu1'</span>, nn.LeakyReLU()),</span><br><span class="line"> (<span class="string">'fc2'</span>, nn.Linear(<span class="number">1024</span>, num_labels))</span><br><span class="line">]))</span><br><span class="line"><span class="comment"># Fine-tune …</span></span><br><span class="line">torch.save(model.state_dict(), model_path)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;However, this throws error when you try to read such a fine-tunedmodel back in. <code>MobileBertForSequenceClassification</code> is setto have one-layer classification head, so it cannot read in yourself-defined classifier’s weights.&lt;/p&gt;<figure class="highlight bash">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line"> model_path, problem_type=<span class="string">"multi_label_classification"</span>,</span><br><span class="line"> num_labels=len(labels), id2label=id2label, label2id=label2id)</span><br><span class="line"></span><br><span class="line">&gt; Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at ./ckpts/ and are newly initialized: [<span class="string">'classifier.bias'</span>, <span class="string">'classifier.weight'</span>]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;To solve this, you can&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;Save encoder weight and classifier weight separately, then load themseparately&lt;/li&gt;&lt;li&gt;Create a custom class corresponding to your weights and initializean instance of that class instead&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2 is clearly &lt;ahref=”https://github.com/huggingface/transformers/issues/1001#issuecomment-520162877”&gt;themore sensible way&lt;/a&gt;. You should read the very clearly written<code>MobileBertForSequenceClassification</code> to understand whatexactly needs to be changed. It turns out all we have to do is to extendthe original class and change its <code>__init__</code> part, so it hasa 2-layer classification head.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> MobileBertForSequenceClassification, TFMobileBertForSequenceClassification</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomMobileBertForSequenceClassification</span>(<span class="title class_ inherited__">MobileBertForSequenceClassification</span>):</span><br><span class="line"> <span class="keyword">def</span> <span class="title function_"><strong>init</strong></span>(<span class="params">self, config</span>):</span><br><span class="line"> <span class="built_in">super</span>().<strong>init</strong>(config)</span><br><span class="line"> <span class="variable language_">self</span>.classifier = nn.Sequential(OrderedDict([</span><br><span class="line"> (<span class="string">'fc1'</span>, nn.Linear(<span class="number">768</span>, <span class="number">1024</span>)),</span><br><span class="line"> (<span class="string">'relu1'</span>, nn.LeakyReLU()),</span><br><span class="line"> (<span class="string">'fc2'</span>, nn.Linear(<span class="number">1024</span>, <span class="number">28</span>))</span><br><span class="line"> ]))</span><br><span class="line"> <span class="variable language_">self</span>.post_init()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TFCustomMobileBertForSequenceClassification</span>(<span class="title class_ inherited__">TFMobileBertForSequenceClassification</span>):</span><br><span class="line"> <span class="keyword">def</span> <span class="title function_"><strong>init</strong></span>(<span class="params">self, config, *inputs, **kwargs</span>):</span><br><span class="line"> <span class="built_in">super</span>().<strong>init</strong>(config, <em>inputs, **kwargs)&lt;/span&gt;<br><span class="line"> <span class="variable language_">self</span>.classifier = keras.Sequential([</span><br><span class="line"> keras.layers.Dense(<span class="number">1024</span>, input_dim=<span class="number">768</span>, name=<span class="string">'fc1'</span>),</span><br><span class="line"> keras.layers.LeakyReLU(alpha=<span class="number">0.01</span>, name = <span class="string">'relu1'</span>), <span class="comment"># Keras defaults alpha to 0.3</span></span><br><span class="line"> keras.layers.Dense(<span class="number">28</span>, name=<span class="string">'fc2'</span>)</span><br><span class="line"> ])</span><br><span class="line"></span><br><span class="line">torch_model = CustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line"> model_path, problem_type=<span class="string">"multi_label_classification"</span>,</span><br><span class="line"> num_labels=<span class="built_in">len</span>(labels), id2label=id2label, label2id=label2id)</span><br><span class="line">tf_model = TFCustomMobileBertForSequenceClassification.from_pretrained(</span><br><span class="line"> …, from_pt=<span class="literal">True</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;However, you may find these two models output different values on thesame input. A closer look at weights unveil that <strong>Hugging Facedidn’t convert classifier’s weights from our Torch model to TensorFlowmodel correctly</strong>. We have to set them manually instead.&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">tf_model.classifier.get_layer(<span class="string">"fc1"</span>).set_weights([torch_model.classifier.fc1.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc1.bias.detach()])</span><br><span class="line">tf_model.classifier.get_layer(<span class="string">"fc2"</span>).set_weights([torch_model.classifier.fc2.weight.transpose(<span class="number">1</span>, <span class="number">0</span>).detach(), torch_model.classifier.fc2.bias.detach()])</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;And now we are finally ready to go.&lt;/p&gt;&lt;h2 id="quantization"&gt;Quantization&lt;/h2&gt;&lt;p&gt;I followed this official doc: &lt;ahref=”https://ai.google.dev/edge/litert/models/post_training_quantization”&gt;Post-trainingquantization&lt;/a&gt;. Because of time limit, I didn’t try Quantization AwareTraining (QAT).&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line">vanilla_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">tflite_model = vanilla_converter.convert()</span><br><span class="line"></span><br><span class="line">quant8_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">quant8_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br><span class="line">tflite_quant8_model = quant8_converter.convert()</span><br><span class="line"></span><br><span class="line">quant16_converter = tf.lite.TFLiteConverter.from_keras_model(bert_model)</span><br><span class="line">quant16_converter.optimizations = [tf.lite.Optimize.DEFAULT]</span><br><span class="line">quant16_converter.target_spec.supported_types = [tf.float16]</span><br><span class="line">tflite_quant16_model = quant16_converter.convert()</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;p&gt;Below I report several key metrics for this Chinese-MobileBERT + a2-layer classification head of [768</em>1024, 1024*<code>class_num</code>].This was tested on a Xiaomi 12X with snapdragon 870. The baseline modelis my colleague’s BERT-Large implementation with accuracy 88.50% andsize 1230MB. My model’s accuracy was bad at first: 75.01% withhyper-parameter <code>weight_decay = 0.01, learning_rate = 1e-4</code>,but we searched out a good hyper-parameter of<code>weight_decay = 2e-4,learning_rate = 2e-5</code> giving 86.01%. Wehad 28 classes, 38000 training data in total, and trained for 5 epochswhere the validation accuracy roughly flattens.&lt;/p&gt;&lt;table&gt;&lt;colgroup&gt;&lt;col style="width: 12%" /&gt;&lt;col style="width: 11%" /&gt;&lt;col style="width: 5%" /&gt;&lt;col style="width: 24%" /&gt;&lt;col style="width: 10%" /&gt;&lt;col style="width: 12%" /&gt;&lt;col style="width: 10%" /&gt;&lt;col style="width: 4%" /&gt;&lt;col style="width: 7%" /&gt;&lt;/colgroup&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Quantization&lt;/th&gt;&lt;th&gt;Logit Difference&lt;/th&gt;&lt;th&gt;Accuracy&lt;/th&gt;&lt;th&gt;Accuracy (after hyper-param search)&lt;/th&gt;&lt;th&gt;Model Size (MB)&lt;/th&gt;&lt;th&gt;Inference Time(ms)&lt;/th&gt;&lt;th&gt;Power Usage(ma)&lt;/th&gt;&lt;th&gt;CPU(%)&lt;/th&gt;&lt;th&gt;Memory(MB)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;float32 (No quant)&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;75.01%&lt;/td&gt;&lt;td&gt;86.094%&lt;/td&gt;&lt;td&gt;101.4&lt;/td&gt;&lt;td&gt;1003.3&lt;/td&gt;&lt;td&gt;89.98&lt;/td&gt;&lt;td&gt;108.02&lt;/td&gt;&lt;td&gt;267.11&lt;/td&gt;&lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;float16&lt;/td&gt;&lt;td&gt;0.015%&lt;/td&gt;&lt;td&gt;75.01%&lt;/td&gt;&lt;td&gt;86.073%&lt;/td&gt;&lt;td&gt;51&lt;/td&gt;&lt;td&gt;838&lt;/td&gt;&lt;td&gt;64.15&lt;/td&gt;&lt;td&gt;108.77&lt;/td&gt;&lt;td&gt;377.11&lt;/td&gt;&lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;int8&lt;/td&gt;&lt;td&gt;4.251%&lt;/td&gt;&lt;td&gt;63.49%&lt;/td&gt;&lt;td&gt;85.947%&lt;/td&gt;&lt;td&gt;25.9&lt;/td&gt;&lt;td&gt;573.8&lt;/td&gt;&lt;td&gt;60.09&lt;/td&gt;&lt;td&gt;110.83&lt;/td&gt;&lt;td&gt;233.19&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;If look at the not fine-tuned, vanilla transformer encoder only, the<code>last_hidden_state</code> has a difference:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Quantization&lt;/th&gt;&lt;th&gt;Logit Difference&lt;/th&gt;&lt;th&gt;Model Size (MB)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;float32 (No quant)&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;97&lt;/td&gt;&lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;float16&lt;/td&gt;&lt;td&gt;0.1%&lt;/td&gt;&lt;td&gt;48.1&lt;/td&gt;&lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;int8&lt;/td&gt;&lt;td&gt;19.8%&lt;/td&gt;&lt;td&gt;24.9&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id="small-language-models"&gt;Small Language Models&lt;/h2&gt;&lt;p&gt;BERT is the go-to option for classification task. But when it comesto small BERT, we had several options:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;mobileBERT&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;distilledBERT&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;tinyBERT&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;As the post is about, we used mobileBERT at last because it’s byGoogle Brain and Google probably knows their thing best.&lt;/p&gt;&lt;p&gt;On the other hand, if you’re looking for small generative model,which people mostly call SLM (Small Language Model) as opposed to LLM, Ifound these options but didn’t try them myself.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;openELM: Apple, 1.1B&lt;/li&gt;&lt;li&gt;Phi-2: Microsoft, 2.7B&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="post-script"&gt;Post Script&lt;/h2&gt;&lt;p&gt;If you want to build an app utilizing edge transformer, I wouldrecommend to read the source code of &lt;ahref=”https://github.com/huggingface/tflite-android-transformers”&gt;HuggingFace’s toy app&lt;/a&gt;. It doesn’t have a README or tutorial, nor have Igone through it personally, but everything from TensorFlow sucks(including MediaPipe unfortunately)&lt;/p&gt;&lt;p&gt;When checking back on this tutorial at date 2024/12/28, I foundGoogle released &lt;ahref=”https://github.com/google-ai-edge/ai-edge-torch”&gt;AI EdgeTorch&lt;/a&gt;, the official tool converting PyTorch models into a .tfliteformat. So you may probably want to try this first, but again, don’ttrust anything from TensorFlow team.&lt;/p&gt;</span></figure></span></figure></p> </body></html>