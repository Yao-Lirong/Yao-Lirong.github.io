<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Before this, see &lt;ahref="#2024/06/17-Conducting-Multi-Round-Conversation-with-Transformers"&gt;2024/06/17Conducting Multi-Round Conversation with Transformers&lt;/a&gt; for why weneed cache. But we have query, key, value three matrices. Why do youonly cache past keys and values? How about past queries?</p> <p><span id="more"></span>&lt;h2 id="attention-mechanism-in-detail"&gt;Attention Mechanism inDetail&lt;/h2&gt;Recall the attention process in transformer can be written in thefollowing matrix form: <span class="math display">\(Z = \text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\)</span> If we look a particular output at position &lt;spanclass=”math inline”&gt;<em>i</em>&lt;/span&gt;, it can be written as: \(z_i =({}&lt;p&gt;)&lt;/p&gt;&lt;pre&gt;&lt;code&gt;\begin&amp;#123;bmatrix&amp;#125;v_1 \\v_2 \\\vdots \\v_n\end&amp;#123;bmatrix&amp;#125;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;\) A simple example can be found in the famous &lt;ahref=”https://jalammar.github.io/illustrated-transformer/”&gt;IllustratedTransformer&lt;/a&gt;&lt;/p&gt;<figure>&lt;img src=”https://jalammar.github.io/images/t/self-attention-output.png”alt=”self attention output” /&gt;&lt;figcaption aria-hidden="true"&gt;self attention output&lt;/figcaption&gt;</figure>&lt;p&gt;From the formula and the example, we can see that key and values arealways a pair in calculation. In fact, this is aligned with the veryconcept of soft dictionary behind attention: we get a query fromsomewhere and look at all the keys in the dictionaries to find, for eachkey, how much it relates to this query and output the weighted averageof each key’s value based on the relatedness.&lt;/p&gt;&lt;h2 id="generative-transformer-decoder-based"&gt;Generative Transformer(Decoder Based)&lt;/h2&gt;<figure>&lt;imgsrc=”https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png”alt=”Autoregressive Decoder” /&gt;&lt;figcaption aria-hidden="true"&gt;Autoregressive Decoder&lt;/figcaption&gt;</figure>&lt;p&gt;Let’s consider a causal language model, aka a transformer’sautoregressive generative decoder. At inference time, <strong>we onlycare about the output at the last position</strong> because the model isautoregressive and the outputs at all the previous positions are exactlythe same as our input. (See the above graph from blogpost &lt;ahref=”https://huggingface.co/blog/encoder-decoder”&gt;Transformers-basedEncoder-Decoder Models&lt;/a&gt;) Therefore, if the current sequence haslength <span class="math inline"><em>s</em></span>, we only care about<span class="math inline"><em>z</em><sub><em>s</em></sub></span>. Allthe other outputs &lt;spanclass=”math inline”&gt;<em>z</em><sub>1…<em>s</em> − 1</sub>&lt;/span&gt; areuseless.&lt;/p&gt;&lt;p&gt;&lt;ahref=”https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/model.py#L188-L191”&gt;Inferencecode in Karpathy’s nanoGPT&lt;/a&gt; corroborated this in its inference timeimplementation:&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"> <span class="comment"># inference-time mini-optimization: only forward the lm_head on the very last position</span></span><br><span class="line"> logits = <span class="variable language_">self</span>.lm_head(x[:, [-<span class="number">1</span>], :]) <span class="comment"># note: using list [-1] to preserve the time dim</span></span><br><span class="line"> loss = <span class="literal">None</span></span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>Now revisit the formula to calculate the output &lt;spanclass=”math inline”&gt;<em>z</em><sub><em>s</em></sub>&lt;/span&gt;: \(z_s =( {}&lt;p&gt;)&lt;/p&gt;&lt;pre&gt;&lt;code&gt;\begin&amp;#123;bmatrix&amp;#125;v_1 \\v_2 \\\vdots \\v_s\end&amp;#123;bmatrix&amp;#125;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;\) It should be clear that to save computation, we only need to cachethe <code>kv</code> values in the previous positions and it’s useless tocache the <code>q</code> value.&lt;/p&gt;&lt;p&gt;To give a more detailed example, let’s consider the whole process togenerate a sequence of tokens with length &lt;spanclass=”math inline”&gt;<em>n</em>&lt;/span&gt;: &lt;spanclass=”math inline”&gt;<em>t</em><sub>1</sub>, …, <em>t</em><sub><em>n</em></sub>&lt;/span&gt;.We can see the previous queries are never used in the computation.<br>\(\)&lt;/p&gt;&lt;h2 id="time-complexity-boost"&gt;Time Complexity Boost&lt;/h2&gt;&lt;p&gt;People complain about the slow inference time of generativetransformer model, where it has a quadratic sequence length term &lt;spanclass=”math inline”&gt;<em>O</em>(<em>s</em><sup>2</sup>)&lt;/span&gt;. Thisquadratic term is caused by &lt;spanclass=”math inline”&gt;<em>Q</em><em>K</em><sup><em>T</em></sup>&lt;/span&gt;matrix multiplication in attention where both matrices have shape &lt;spanclass=”math inline”&gt;<em>s</em> × <em>d</em>&lt;/span&gt;. Recall running timeof matmul <span class="math inline"><em>A</em><em>B</em></span> where<span class="math inline">$A \in \R^{m \times p}, B \in \R^{p \timesn}$</span> is &lt;spanclass=”math inline”&gt;<em>O</em>(<em>m</em><em>p</em><em>n</em>)&lt;/span&gt;,so this matmul of query and key matrix has time complexity &lt;spanclass=”math inline”&gt;<em>O</em>(<em>s</em><sup>2</sup><em>d</em>)&lt;/span&gt;.&lt;/p&gt;&lt;p&gt;However, by observing that we only need the output at the very lastposition in generative model and utilizing KV-cache, we reduce ourmatrix <span class="math inline">$Q \in \R^{s \times d}$</span> to asingle vector of <span class="math inline">$q \in \R^{1 \timesd}$</span> and effectively reduce the time complexity of this operationto <span class="math inline"><em>O</em>(<em>s</em><em>d</em>)</span>.Therefore, we can eliminate the quadratic term from our inference timeand only need linear time <span class="math inline"><em>s</em></span>instead.&lt;/p&gt;&lt;h2 id="what-about-encoder-based-transformer-model"&gt;What about EncoderBased Transformer Model?&lt;/h2&gt;&lt;p&gt;Encoder Based transformer models do not have the issue of repeatedlycomputing the same past tokens’ attention scores so do not need aKV-cache.&lt;/p&gt;&lt;h2 id="code-implementation"&gt;Code Implementation&lt;/h2&gt;&lt;p&gt;Facebook’s &lt;ahref=”https://github.com/facebookresearch/XLM”&gt;cross-lingual languagemodel (XLM)&lt;/a&gt; gives a fantastic example of how to implement KV-Cache(or transformers in general, it provides abundant comments of tensorshape at each step).&lt;/p&gt;&lt;ol type="1"&gt;&lt;li&gt;&lt;p&gt;At inference time, do not recompute elements (where<code>slen</code> or a more descriptive naming can be<code>cached_sequence_length</code> is how many previous positions havebeen cached): &lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L373-L380”&gt;link&lt;/a&gt;&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"> _slen = slen - cache[<span class="string">'slen'</span>]</span><br><span class="line"> x = x[:, -_slen:]</span><br><span class="line"> positions = positions[:, -_slen:]</span><br><span class="line"> <span class="keyword">if</span> langs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"> langs = langs[:, -_slen:]</span><br><span class="line"> mask = mask[:, -_slen:]</span><br><span class="line"> attn_mask = attn_mask[:, -_slen:]</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;li&gt;&lt;p&gt;Retrieve, use and update cache: &lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L199-L207”&gt;link1&lt;/a&gt;&lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L423”&gt;link2&lt;/a&gt;&lt;/p&gt;<figure class="highlight python">&lt;table&gt;&lt;tr&gt;&lt;td class="gutter"&gt;&lt;pre&gt;<span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre&gt;<span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.layer_id <span class="keyword">in</span> cache:</span><br><span class="line"> k_, v_ = cache[<span class="variable language_">self</span>.layer_id]</span><br><span class="line"> k = torch.cat([k_, k], dim=<span class="number">2</span>) <span class="comment"># (bs, n_heads, klen, dim_per_head)</span></span><br><span class="line"> v = torch.cat([v_, v], dim=<span class="number">2</span>) <span class="comment"># (bs, n_heads, klen, dim_per_head)</span></span><br><span class="line">cache[<span class="variable language_">self</span>.layer_id] = (k, v)</span><br><span class="line">…</span><br><span class="line">cache[<span class="string">'slen'</span>] += tensor.size(<span class="number">1</span>)</span><br>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</figure>&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;XLM can serve multiple purposes including as a generative causallanguage model, masked language model, or a translation language model.We use KV-Cache only with causal language model in<code>generate()</code> function, see &lt;ahref=”https://github.com/facebookresearch/XLM/blob/cd281d32612d145c6742b4d3f048f80df8669c30/xlm/model/transformer.py#L482-L498”&gt;code&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;XLM has a <code>Memory</code> module that implements &lt;ahref=”https://github.com/facebookresearch/XLM#v-product-key-memory-layers-pkm”&gt;Product-KeyMemory Layers&lt;/a&gt; whose mechanism rings very familiar to me but I can’trecall where I’ve encountered something similar before. Anyway, you canignore those <code>Memory</code> implementations and focus on theattention part if use it as a source to learn cache or the basics ofattention.&lt;/p&gt;&lt;h2 id="more-code-examples"&gt;More Code Examples&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;This Medium post &lt;ahref=”https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8”&gt;KVcaching explained&lt;/a&gt; leads way to where to find Hugging Face’simplementation in general, which can be too modular and abstractnowadays. It’s hidden in the <code>forward</code> function in<code>XXXForCausalLM</code>. Take &lt;ahref=”https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/llama2#transformers.LlamaForCausalLM”&gt;<code>LlamaForCausalLM</code>&lt;/a&gt;as an example, in its <code>forward</code> function, we still need to godown the abstraction to <code>LlamaModel</code> -&gt;<code>LlamaDecoderLayer</code> -&gt; &lt;ahref=”https://github.com/huggingface/transformers/blob/6c1d0b069de22d7ed8aa83f733c25045eea0585d/src/transformers/models/llama/modeling_llama.py#L337-L340”&gt;<code>LlamaAttention</code>&lt;/a&gt;and we can see the <code>past_key_value</code> there implementing the<code>Cache</code> class. I didn’t read into how Hugging Face didit.&lt;/li&gt;&lt;li&gt;<a href="https://zhuanlan.zhihu.com/p/630832593" rel="external nofollow noopener" target="_blank">This Zhihu postexplaining KV-Cache</a> leads way to &lt;ahref=”https://github.com/huggingface/transformers/blob/d1a1bcf56aeb8593b9cc613b21422e6311875599/src/transformers/models/gpt2/modeling_gpt2.py#L318-L321”&gt;HuggingFace’s GPT-2&lt;/a&gt;. The &lt;ahref=”https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L105-L108”&gt;originalGPT-2 code&lt;/a&gt; is in fact more straightforward, but you’d better justread XLM. It simply has more comments and the naming is moreself-explanatory.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="ps"&gt;PS&lt;/h2&gt;&lt;p&gt;I initially didn’t find where Hugging Face implemented KV-Cache incurrent version (<code>transformer 4.40</code>) but only this &lt;ahref=”https://github.com/huggingface/transformers/blob/aec1ca3a588bc6c65f7886e3d3eaa74901a6356f/src/transformers/cache_utils.py#L293”&gt;<code>Cache</code>class&lt;/a&gt; and failed to find where it’s used. So I followed therecommendation under &lt;ahref=”https://zhuanlan.zhihu.com/p/601044938”&gt;this Zhihu post&lt;/a&gt; to goto transformer 2.5.0 instead. A quick search like “kv” or “cache” led meto &lt;ahref=”https://github.com/huggingface/transformers/blob/v2.5.0/src/transformers/modeling_xlm.py”&gt;<code>modeling_xlm.py</code>&lt;/a&gt;.I was surprised to find early Hugging Face model code was more of arename of original implementation instead of a refactor they do now.&lt;/p&gt;&lt;p&gt;I then read this &lt;ahref=”https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8”&gt;KVcaching explained&lt;/a&gt; post. Its graph isn’t super straightforward but itintroduces how KV-cache reduces time complexity and where to findHugging Face’s implementation.&lt;/p&gt;</p> </body></html>