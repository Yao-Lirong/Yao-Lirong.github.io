<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="intro-to-matryoshka-representation-learning">Intro to MatryoshkaRepresentation Learning</h2> <p>In Matryoshka Representation Learning (MRL), we want to construct anencoding &lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em></sub>&lt;/span&gt; withdimension <span class="math inline"><em>d</em></span> such that itstruncations of different lengths (&lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em>/16</sub>, <em>e</em><sub><em>d</em>/8</sub>, <em>e</em><sub><em>d</em>/4</sub>, <em>e</em><sub><em>d</em>/2</sub>&lt;/span&gt;​)are each (somewhat) valid representations. Suppose you’re training on aclassification problem with the classic encoder + classifier headarchitecture. At train time:</p> <ul> <li>classic setting: you just use the vector &lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em></sub>&lt;/span&gt; as input tothe classifier head</li> <li>MRL: construct multiple classifier heads (in our case 5) and put oneon top of encoding of each length (&lt;spanclass="math inline"&gt;<em>e</em><sub><em>d</em>/16</sub>, …, <em>e</em><sub><em>d</em></sub>&lt;/span&gt;)and average the loss of each classifier head. So we build heads of size<code>[d, num_class], [d/2, num_class], ... [d/16, num_class]</code>Note these classifier heads share weights.</li> </ul> <h2 id="application-adaptive-retrieval">Application: AdaptiveRetrieval</h2> <p>Online retrieval is one of the tasks where latency matters the most.Given a user query <span class="math inline"><em>q</em></span>, it isslow to compute KNN from a dataset of size 1M (&lt;spanclass="math inline"&gt;10<sup>6</sup>&lt;/span&gt;) indexes if each index hasdimension 3072. With MRL, we can decompose the process into twostages:</p> <ol type="1"> <li>Shortlist: First retrieve 2K indexes where the distance is computedusing only 1024-d vector (the first 1024 elements of the 3072vector)</li> <li>Rerank: Find KNN among these 2K indexes where the distance iscomputed using the full length 3072 vector</li> </ol> <p>The FLOP is therefore reduced from &lt;spanclass="math inline"&gt;3072 × 10<sup>6</sup>&lt;/span&gt; to &lt;spanclass="math inline"&gt;1024 × 10<sup>6</sup> + 3072 × 2<em>K</em>&lt;/span&gt;.Ce Gao tested full length 3072-dim vector vs adaptive retrieval usingMatryoshka 1024-dim. The accuracy dropped from 99% to 89% with RequestsPer Second (RPS) raises from 300 to 1000.</p> <p>Find more details of Matryoshka Representation Learning and itsapplications in this wonderful &lt;ahref="https://aniketrege.github.io/blog/2024/mrl/#what-is-mrl-really-this-time"&gt;blogpost. Read from section <em>What is MRL? (Really this Time)</em>&lt;/a&gt;</p> <h2 id="binary-vector-search">Binary Vector Search</h2> <p>&lt;ahref="https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors"&gt;CeGao suggested&lt;/a&gt; another way to reduce memory and FLOP use. He proposesto turn the length <span class="math inline"><em>d</em></span> FP32vector into a length <span class="math inline"><em>d</em></span> binaryvector, where original positive value is set to 1 and original negativevalue is set to 0.</p> <p>Without using adaptive retrieval, the accuracy dropped from 99% to83%, but the latency (RPS = 3000) and memory has a significantimprovement because previously one single vector / encoding consists of<span class="math inline"><em>d</em></span> 32-bit number, whereas nowit only consists of <span class="math inline"><em>d</em></span> 1-bitnumber.</p> <p>If you adapt the Adaptive Retrieval setup mentioned earlier:</p> <ol type="1"> <li>Shortlist: retrieve 2K indexes using full-length but binaryvector</li> <li>Rerank: find KNN among 2K indexes using full-length, FP32vector</li> </ol> <p>you get a precision drop from 99% only to 96% with an RPS of1700.</p> <p>P.S. I discovered this method on &lt;ahref="https://simonwillison.net/2024/Mar/26/binary-vector-search/"&gt;SimonWillison’s blog&lt;/a&gt;.</p> </body></html>